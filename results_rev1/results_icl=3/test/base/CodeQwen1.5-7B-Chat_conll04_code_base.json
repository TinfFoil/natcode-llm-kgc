[{"Model": "Qwen/CodeQwen1.5-7B-Chat", "Precision": 0.08157524613220815, "Recall": 0.14250614250614252, "F1_Score": 0.1037567084078712, "rel_type_metrics": {"Located_in": {"precision": 0.10434782608695652, "recall": 0.13333333333333333, "f1": 0.11707317073170732}, "Live_in": {"precision": 0.12318840579710146, "recall": 0.17346938775510204, "f1": 0.1440677966101695}, "Work_for": {"precision": 0.1032258064516129, "recall": 0.21052631578947367, "f1": 0.13852813852813853}, "Organization_based_in": {"precision": 0.016666666666666666, "recall": 0.020833333333333332, "f1": 0.018518518518518517}, "Kill": {"precision": 0.10185185185185185, "recall": 0.23404255319148937, "f1": 0.14193548387096774}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-22-45-18", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "Qwen/CodeQwen1.5-7B-Chat", "Precision": 0.09405940594059406, "Recall": 0.18673218673218672, "F1_Score": 0.12510288065843622, "rel_type_metrics": {"Located_in": {"precision": 0.12244897959183673, "recall": 0.26666666666666666, "f1": 0.16783216783216784}, "Work_for": {"precision": 0.07692307692307693, "recall": 0.17105263157894737, "f1": 0.10612244897959185}, "Kill": {"precision": 0.15454545454545454, "recall": 0.3617021276595745, "f1": 0.21656050955414013}, "Live_in": {"precision": 0.12418300653594772, "recall": 0.19387755102040816, "f1": 0.15139442231075698}, "Organization_based_in": {"precision": 0.024793388429752067, "recall": 0.03125, "f1": 0.027649769585253458}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-13-23-45", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "Qwen/CodeQwen1.5-7B-Chat", "Precision": 0.06257668711656442, "Recall": 0.12530712530712532, "F1_Score": 0.08346972176759412, "rel_type_metrics": {"Organization_based_in": {"precision": 0.07377049180327869, "recall": 0.09375, "f1": 0.08256880733944955}, "Located_in": {"precision": 0.06451612903225806, "recall": 0.13333333333333333, "f1": 0.08695652173913043}, "Live_in": {"precision": 0.06763285024154589, "recall": 0.14285714285714285, "f1": 0.09180327868852459}, "Work_for": {"precision": 0.07042253521126761, "recall": 0.13157894736842105, "f1": 0.09174311926605505}, "Kill": {"precision": 0.05405405405405406, "recall": 0.1276595744680851, "f1": 0.0759493670886076}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-13-30-00", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}]