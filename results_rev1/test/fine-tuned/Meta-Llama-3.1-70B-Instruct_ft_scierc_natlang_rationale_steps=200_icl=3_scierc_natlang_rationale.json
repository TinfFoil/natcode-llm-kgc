[{"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_scierc_natlang_rationale_steps=200_icl=3", "Precision": 0.11444141689373297, "Recall": 0.08624229979466119, "F1_Score": 0.09836065573770492, "rel_type_metrics": {"Part_of": {"precision": 0.1111111111111111, "recall": 0.015873015873015872, "f1": 0.027777777777777776}, "Feature_of": {"precision": 0, "recall": 0.0, "f1": 0}, "Compare": {"precision": 0, "recall": 0.0, "f1": 0}, "Conjunction": {"precision": 0, "recall": 0.0, "f1": 0}, "Hyponym_of": {"precision": 0.043478260869565216, "recall": 0.014925373134328358, "f1": 0.02222222222222222}, "Used_for": {"precision": 0.13486842105263158, "recall": 0.15384615384615385, "f1": 0.1437335670464505}, "Evaluate_for": {"precision": 0, "recall": 0.0, "f1": 0}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2025-01-09-00-27-21", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_scierc_natlang_rationale_steps=200_icl=3", "Precision": 0.12891113892365458, "Recall": 0.10574948665297741, "F1_Score": 0.11618725324309082, "rel_type_metrics": {"Hyponym_of": {"precision": 0, "recall": 0.0, "f1": 0}, "Feature_of": {"precision": 0, "recall": 0.0, "f1": 0}, "Evaluate_for": {"precision": 0.0, "recall": 0.0, "f1": 0}, "Part_of": {"precision": 0.17105263157894737, "recall": 0.20634920634920634, "f1": 0.1870503597122302}, "Used_for": {"precision": 0.24064171122994651, "recall": 0.16885553470919323, "f1": 0.19845644983461963}, "Conjunction": {"precision": 0, "recall": 0.0, "f1": 0}, "Compare": {"precision": 0, "recall": 0.0, "f1": 0}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2025-01-09-01-15-24", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_scierc_natlang_rationale_steps=200_icl=3", "Precision": 0.10513447432762836, "Recall": 0.08829568788501027, "F1_Score": 0.09598214285714286, "rel_type_metrics": {"Hyponym_of": {"precision": 0, "recall": 0.0, "f1": 0}, "Part_of": {"precision": 0.2, "recall": 0.015873015873015872, "f1": 0.029411764705882353}, "Feature_of": {"precision": 0, "recall": 0.0, "f1": 0}, "Conjunction": {"precision": 0, "recall": 0.0, "f1": 0}, "Used_for": {"precision": 0.14685314685314685, "recall": 0.1575984990619137, "f1": 0.1520361990950226}, "Evaluate_for": {"precision": 0.03125, "recall": 0.01098901098901099, "f1": 0.016260162601626018}, "Compare": {"precision": 0, "recall": 0.0, "f1": 0}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2025-01-09-02-03-58", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}]