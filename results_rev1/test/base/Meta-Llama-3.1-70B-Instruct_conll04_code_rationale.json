[{"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.2090032154340836, "Recall": 0.3194103194103194, "F1_Score": 0.25267249757045673, "rel_type_metrics": {"Work_for": {"precision": 0.27692307692307694, "recall": 0.47368421052631576, "f1": 0.3495145631067961}, "Located_in": {"precision": 0.15384615384615385, "recall": 0.6, "f1": 0.2448979591836735}, "Kill": {"precision": 0.41025641025641024, "recall": 0.3404255319148936, "f1": 0.37209302325581395}, "Organization_based_in": {"precision": 0.25, "recall": 0.09375, "f1": 0.13636363636363635}, "Live_in": {"precision": 0.25, "recall": 0.15306122448979592, "f1": 0.189873417721519}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-11-16-01-01", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.24050632911392406, "Recall": 0.37346437346437344, "F1_Score": 0.29258902791145336, "rel_type_metrics": {"Work_for": {"precision": 0.25882352941176473, "recall": 0.5789473684210527, "f1": 0.3577235772357724}, "Organization_based_in": {"precision": 0.4791666666666667, "recall": 0.23958333333333334, "f1": 0.3194444444444445}, "Kill": {"precision": 0.32727272727272727, "recall": 0.3829787234042553, "f1": 0.35294117647058826}, "Located_in": {"precision": 0.15018315018315018, "recall": 0.45555555555555555, "f1": 0.22589531680440772}, "Live_in": {"precision": 0.34210526315789475, "recall": 0.2653061224489796, "f1": 0.2988505747126437}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-11-16-48-05", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.15648286140089418, "Recall": 0.257985257985258, "F1_Score": 0.19480519480519481, "rel_type_metrics": {"Kill": {"precision": 0.5357142857142857, "recall": 0.6382978723404256, "f1": 0.5825242718446602}, "Work_for": {"precision": 0.1592920353982301, "recall": 0.47368421052631576, "f1": 0.23841059602649006}, "Live_in": {"precision": 0.24390243902439024, "recall": 0.10204081632653061, "f1": 0.14388489208633093}, "Organization_based_in": {"precision": 0.09090909090909091, "recall": 0.020833333333333332, "f1": 0.03389830508474576}, "Located_in": {"precision": 0.09152542372881356, "recall": 0.3, "f1": 0.14025974025974028}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-11-17-44-40", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}]