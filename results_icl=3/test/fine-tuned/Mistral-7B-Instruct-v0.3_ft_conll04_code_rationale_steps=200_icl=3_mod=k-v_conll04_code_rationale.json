[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_code_rationale_steps=200_icl=3_mod=k-v", "Precision": 0.5, "Recall": 0.5036855036855037, "F1_Score": 0.5018359853121175, "rel_type_metrics": {"Work_for": {"precision": 0.5125, "recall": 0.5394736842105263, "f1": 0.5256410256410255}, "Located_in": {"precision": 0.31690140845070425, "recall": 0.5, "f1": 0.3879310344827586}, "Organization_based_in": {"precision": 0.5849056603773585, "recall": 0.3229166666666667, "f1": 0.41610738255033564}, "Live_in": {"precision": 0.5617977528089888, "recall": 0.5102040816326531, "f1": 0.5347593582887701}, "Kill": {"precision": 0.8444444444444444, "recall": 0.8085106382978723, "f1": 0.8260869565217391}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-18-21-55", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_code_rationale_steps=200_icl=3_mod=k-v", "Precision": 0.4817351598173516, "Recall": 0.5184275184275184, "F1_Score": 0.49940828402366866, "rel_type_metrics": {"Kill": {"precision": 0.8444444444444444, "recall": 0.8085106382978723, "f1": 0.8260869565217391}, "Organization_based_in": {"precision": 0.43333333333333335, "recall": 0.40625, "f1": 0.41935483870967744}, "Live_in": {"precision": 0.5875, "recall": 0.47959183673469385, "f1": 0.5280898876404494}, "Located_in": {"precision": 0.3055555555555556, "recall": 0.4888888888888889, "f1": 0.37606837606837606}, "Work_for": {"precision": 0.5443037974683544, "recall": 0.5657894736842105, "f1": 0.5548387096774193}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-18-25-12", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_code_rationale_steps=200_icl=3_mod=k-v", "Precision": 0.5303030303030303, "Recall": 0.42997542997543, "F1_Score": 0.47489823609226595, "rel_type_metrics": {"Live_in": {"precision": 0.78, "recall": 0.3979591836734694, "f1": 0.5270270270270271}, "Organization_based_in": {"precision": 0.5102040816326531, "recall": 0.2604166666666667, "f1": 0.34482758620689663}, "Work_for": {"precision": 0.4875, "recall": 0.5131578947368421, "f1": 0.5}, "Located_in": {"precision": 0.3434343434343434, "recall": 0.37777777777777777, "f1": 0.35978835978835977}, "Kill": {"precision": 0.7307692307692307, "recall": 0.8085106382978723, "f1": 0.7676767676767676}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-18-27-21", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}]