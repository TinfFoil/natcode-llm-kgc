[{"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.4010840108401084, "Recall": 0.36363636363636365, "F1_Score": 0.38144329896907214, "rel_type_metrics": {"Work_for": {"precision": 0.5245901639344263, "recall": 0.42105263157894735, "f1": 0.4671532846715329}, "Live_in": {"precision": 0.59375, "recall": 0.3877551020408163, "f1": 0.46913580246913583}, "Organization_based_in": {"precision": 0.38636363636363635, "recall": 0.17708333333333334, "f1": 0.24285714285714285}, "Located_in": {"precision": 0.43283582089552236, "recall": 0.32222222222222224, "f1": 0.3694267515923567}, "Kill": {"precision": 0.2909090909090909, "recall": 0.6808510638297872, "f1": 0.40764331210191085}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-26-21-08-56", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.3875, "Recall": 0.3808353808353808, "F1_Score": 0.38413878562577447, "rel_type_metrics": {"Located_in": {"precision": 0.43243243243243246, "recall": 0.35555555555555557, "f1": 0.3902439024390244}, "Organization_based_in": {"precision": 0.4074074074074074, "recall": 0.22916666666666666, "f1": 0.2933333333333333}, "Kill": {"precision": 0.25203252032520324, "recall": 0.6595744680851063, "f1": 0.36470588235294116}, "Work_for": {"precision": 0.5223880597014925, "recall": 0.4605263157894737, "f1": 0.4895104895104895}, "Live_in": {"precision": 0.5223880597014925, "recall": 0.35714285714285715, "f1": 0.4242424242424242}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-14-51-32", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.4, "Recall": 0.3783783783783784, "F1_Score": 0.3888888888888889, "rel_type_metrics": {"Work_for": {"precision": 0.3135593220338983, "recall": 0.4868421052631579, "f1": 0.38144329896907214}, "Kill": {"precision": 0.7391304347826086, "recall": 0.723404255319149, "f1": 0.7311827956989247}, "Located_in": {"precision": 0.4142857142857143, "recall": 0.32222222222222224, "f1": 0.36250000000000004}, "Organization_based_in": {"precision": 0.425, "recall": 0.17708333333333334, "f1": 0.25}, "Live_in": {"precision": 0.3978494623655914, "recall": 0.37755102040816324, "f1": 0.38743455497382195}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-01-49-57", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}]