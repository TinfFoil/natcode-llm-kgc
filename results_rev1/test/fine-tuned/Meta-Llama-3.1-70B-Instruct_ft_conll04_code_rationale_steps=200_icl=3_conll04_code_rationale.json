[{"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.24919093851132687, "Recall": 0.3783783783783784, "F1_Score": 0.30048780487804877, "rel_type_metrics": {"Live_in": {"precision": 0.19014084507042253, "recall": 0.2755102040816326, "f1": 0.225}, "Kill": {"precision": 0.39705882352941174, "recall": 0.574468085106383, "f1": 0.46956521739130436}, "Organization_based_in": {"precision": 0.22727272727272727, "recall": 0.052083333333333336, "f1": 0.0847457627118644}, "Located_in": {"precision": 0.199203187250996, "recall": 0.5555555555555556, "f1": 0.29325513196480935}, "Work_for": {"precision": 0.3543307086614173, "recall": 0.5921052631578947, "f1": 0.44334975369458124}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-11-11-08-34", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.19420289855072465, "Recall": 0.16461916461916462, "F1_Score": 0.17819148936170215, "rel_type_metrics": {"Organization_based_in": {"precision": 0.14285714285714285, "recall": 0.010416666666666666, "f1": 0.01941747572815534}, "Work_for": {"precision": 0.14814814814814814, "recall": 0.10526315789473684, "f1": 0.12307692307692308}, "Live_in": {"precision": 0.13095238095238096, "recall": 0.11224489795918367, "f1": 0.12087912087912088}, "Located_in": {"precision": 0.19318181818181818, "recall": 0.37777777777777777, "f1": 0.25563909774436094}, "Kill": {"precision": 0.5909090909090909, "recall": 0.2765957446808511, "f1": 0.37681159420289856}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-11-11-38-29", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.20827178729689808, "Recall": 0.3464373464373464, "F1_Score": 0.2601476014760148, "rel_type_metrics": {"Organization_based_in": {"precision": 0.17391304347826086, "recall": 0.041666666666666664, "f1": 0.06722689075630252}, "Live_in": {"precision": 0.20118343195266272, "recall": 0.3469387755102041, "f1": 0.2546816479400749}, "Work_for": {"precision": 0.2891566265060241, "recall": 0.631578947368421, "f1": 0.396694214876033}, "Kill": {"precision": 0.40298507462686567, "recall": 0.574468085106383, "f1": 0.4736842105263158}, "Located_in": {"precision": 0.11155378486055777, "recall": 0.3111111111111111, "f1": 0.16422287390029325}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-11-12-07-27", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}]