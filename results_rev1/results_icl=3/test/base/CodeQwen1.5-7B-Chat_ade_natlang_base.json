[{"Model": "Qwen/CodeQwen1.5-7B-Chat", "Precision": 0.05894736842105263, "Recall": 0.06334841628959276, "F1_Score": 0.06106870229007633, "rel_type_metrics": {"Adverse_effect": {"precision": 0.25, "recall": 0.06334841628959276, "f1": 0.10108303249097472}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-28-20-28-31", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "Qwen/CodeQwen1.5-7B-Chat", "Precision": 0.08716707021791767, "Recall": 0.08144796380090498, "F1_Score": 0.08421052631578946, "rel_type_metrics": {"Adverse_effect": {"precision": 0.23841059602649006, "recall": 0.08144796380090498, "f1": 0.12141652613827994}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-29-10-21-09", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "Qwen/CodeQwen1.5-7B-Chat", "Precision": 0.10194174757281553, "Recall": 0.09502262443438914, "F1_Score": 0.0983606557377049, "rel_type_metrics": {"Adverse_effect": {"precision": 0.302158273381295, "recall": 0.09502262443438914, "f1": 0.14457831325301204}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-29-10-24-31", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": true, "chat_model": true}]