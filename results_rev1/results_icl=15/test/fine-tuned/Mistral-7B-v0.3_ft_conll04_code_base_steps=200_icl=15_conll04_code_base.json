[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=15", "Precision": 0.6954436450839329, "Recall": 0.7125307125307125, "F1_Score": 0.7038834951456312, "rel_type_metrics": {"Live_in": {"precision": 0.7019230769230769, "recall": 0.7448979591836735, "f1": 0.7227722772277229}, "Organization_based_in": {"precision": 0.6701030927835051, "recall": 0.6770833333333334, "f1": 0.6735751295336787}, "Located_in": {"precision": 0.5769230769230769, "recall": 0.6666666666666666, "f1": 0.6185567010309277}, "Kill": {"precision": 0.8775510204081632, "recall": 0.9148936170212766, "f1": 0.8958333333333333}, "Work_for": {"precision": 0.7777777777777778, "recall": 0.6447368421052632, "f1": 0.7050359712230216}}, "n_icl_samples": 15, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-23-06-18", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=15", "Precision": 0.7167919799498746, "Recall": 0.7027027027027027, "F1_Score": 0.7096774193548387, "rel_type_metrics": {"Organization_based_in": {"precision": 0.7176470588235294, "recall": 0.6354166666666666, "f1": 0.6740331491712708}, "Kill": {"precision": 0.6935483870967742, "recall": 0.9148936170212766, "f1": 0.7889908256880733}, "Work_for": {"precision": 0.7936507936507936, "recall": 0.6578947368421053, "f1": 0.7194244604316548}, "Located_in": {"precision": 0.6477272727272727, "recall": 0.6333333333333333, "f1": 0.6404494382022472}, "Live_in": {"precision": 0.7425742574257426, "recall": 0.7653061224489796, "f1": 0.7537688442211056}}, "n_icl_samples": 15, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-23-09-52", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=15", "Precision": 0.7086419753086419, "Recall": 0.7051597051597052, "F1_Score": 0.7068965517241379, "rel_type_metrics": {"Kill": {"precision": 0.8723404255319149, "recall": 0.8723404255319149, "f1": 0.8723404255319149}, "Located_in": {"precision": 0.6037735849056604, "recall": 0.7111111111111111, "f1": 0.6530612244897959}, "Work_for": {"precision": 0.7761194029850746, "recall": 0.6842105263157895, "f1": 0.7272727272727273}, "Organization_based_in": {"precision": 0.6941176470588235, "recall": 0.6145833333333334, "f1": 0.6519337016574587}, "Live_in": {"precision": 0.71, "recall": 0.7244897959183674, "f1": 0.7171717171717172}}, "n_icl_samples": 15, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-23-13-46", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}]