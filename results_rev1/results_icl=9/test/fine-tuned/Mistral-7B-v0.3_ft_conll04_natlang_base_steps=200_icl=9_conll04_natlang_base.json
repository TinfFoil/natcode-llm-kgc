[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=9", "Precision": 0.7066666666666667, "Recall": 0.6511056511056511, "F1_Score": 0.6777493606138107, "rel_type_metrics": {"Kill": {"precision": 0.7592592592592593, "recall": 0.8723404255319149, "f1": 0.8118811881188118}, "Work_for": {"precision": 0.7272727272727273, "recall": 0.631578947368421, "f1": 0.676056338028169}, "Organization_based_in": {"precision": 0.6588235294117647, "recall": 0.5833333333333334, "f1": 0.6187845303867403}, "Located_in": {"precision": 0.7397260273972602, "recall": 0.6, "f1": 0.6625766871165644}, "Live_in": {"precision": 0.6804123711340206, "recall": 0.673469387755102, "f1": 0.676923076923077}}, "n_icl_samples": 9, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-16-18", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=9", "Precision": 0.7047872340425532, "Recall": 0.6511056511056511, "F1_Score": 0.6768837803320561, "rel_type_metrics": {"Located_in": {"precision": 0.6962025316455697, "recall": 0.6111111111111112, "f1": 0.650887573964497}, "Kill": {"precision": 0.8541666666666666, "recall": 0.8723404255319149, "f1": 0.8631578947368421}, "Live_in": {"precision": 0.6666666666666666, "recall": 0.673469387755102, "f1": 0.6700507614213197}, "Organization_based_in": {"precision": 0.6746987951807228, "recall": 0.5833333333333334, "f1": 0.6256983240223464}, "Work_for": {"precision": 0.7014925373134329, "recall": 0.618421052631579, "f1": 0.6573426573426574}}, "n_icl_samples": 9, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-18-08", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=9", "Precision": 0.6666666666666666, "Recall": 0.6388206388206388, "F1_Score": 0.6524466750313676, "rel_type_metrics": {"Located_in": {"precision": 0.6463414634146342, "recall": 0.5888888888888889, "f1": 0.6162790697674418}, "Organization_based_in": {"precision": 0.6625, "recall": 0.5520833333333334, "f1": 0.6022727272727273}, "Kill": {"precision": 0.8541666666666666, "recall": 0.8723404255319149, "f1": 0.8631578947368421}, "Work_for": {"precision": 0.6818181818181818, "recall": 0.5921052631578947, "f1": 0.6338028169014085}, "Live_in": {"precision": 0.5964912280701754, "recall": 0.6938775510204082, "f1": 0.6415094339622641}}, "n_icl_samples": 9, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-20-14", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}]