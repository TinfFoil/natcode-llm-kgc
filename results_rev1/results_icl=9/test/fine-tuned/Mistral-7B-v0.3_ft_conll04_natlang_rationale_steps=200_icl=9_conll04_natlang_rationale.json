[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=9", "Precision": 0.6785714285714286, "Recall": 0.6535626535626535, "F1_Score": 0.6658322903629537, "rel_type_metrics": {"Work_for": {"precision": 0.5517241379310345, "recall": 0.631578947368421, "f1": 0.588957055214724}, "Organization_based_in": {"precision": 0.7073170731707317, "recall": 0.6041666666666666, "f1": 0.6516853932584269}, "Kill": {"precision": 0.8235294117647058, "recall": 0.8936170212765957, "f1": 0.8571428571428571}, "Located_in": {"precision": 0.6506024096385542, "recall": 0.6, "f1": 0.6242774566473989}, "Live_in": {"precision": 0.7441860465116279, "recall": 0.6530612244897959, "f1": 0.6956521739130435}}, "n_icl_samples": 9, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-18-13-23", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=9", "Precision": 0.6948717948717948, "Recall": 0.6658476658476659, "F1_Score": 0.6800501882057716, "rel_type_metrics": {"Live_in": {"precision": 0.7272727272727273, "recall": 0.6530612244897959, "f1": 0.6881720430107526}, "Kill": {"precision": 0.8, "recall": 0.851063829787234, "f1": 0.8247422680412372}, "Organization_based_in": {"precision": 0.7209302325581395, "recall": 0.6458333333333334, "f1": 0.6813186813186812}, "Located_in": {"precision": 0.6829268292682927, "recall": 0.6222222222222222, "f1": 0.6511627906976744}, "Work_for": {"precision": 0.5975609756097561, "recall": 0.6447368421052632, "f1": 0.620253164556962}}, "n_icl_samples": 9, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-18-17-56", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=9", "Precision": 0.6529126213592233, "Recall": 0.6609336609336609, "F1_Score": 0.6568986568986569, "rel_type_metrics": {"Live_in": {"precision": 0.6770833333333334, "recall": 0.6632653061224489, "f1": 0.6701030927835052}, "Work_for": {"precision": 0.6666666666666666, "recall": 0.6052631578947368, "f1": 0.6344827586206896}, "Located_in": {"precision": 0.6, "recall": 0.6333333333333333, "f1": 0.6162162162162161}, "Kill": {"precision": 0.7321428571428571, "recall": 0.8723404255319149, "f1": 0.7961165048543688}, "Organization_based_in": {"precision": 0.6382978723404256, "recall": 0.625, "f1": 0.631578947368421}}, "n_icl_samples": 9, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-18-22-23", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}]