[{"Model": "./models/deepseek-coder-7b-base-v1.5_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.42771084337349397, "Recall": 0.5233415233415234, "F1_Score": 0.47071823204419894, "rel_type_metrics": {"Located_in": {"precision": 0.6451612903225806, "recall": 0.4444444444444444, "f1": 0.5263157894736842}, "Organization_based_in": {"precision": 0.2802547770700637, "recall": 0.4583333333333333, "f1": 0.34782608695652173}, "Kill": {"precision": 0.3877551020408163, "recall": 0.8085106382978723, "f1": 0.5241379310344828}, "Live_in": {"precision": 0.4322033898305085, "recall": 0.5204081632653061, "f1": 0.47222222222222227}, "Work_for": {"precision": 0.6349206349206349, "recall": 0.5263157894736842, "f1": 0.5755395683453238}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-26-22-25-06", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}, {"Model": "./models/deepseek-coder-7b-base-v1.5_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.5531400966183575, "Recall": 0.5626535626535627, "F1_Score": 0.5578562728380024, "rel_type_metrics": {"Located_in": {"precision": 0.3409090909090909, "recall": 0.5, "f1": 0.40540540540540543}, "Organization_based_in": {"precision": 0.703125, "recall": 0.46875, "f1": 0.5625}, "Live_in": {"precision": 0.6082474226804123, "recall": 0.6020408163265306, "f1": 0.6051282051282051}, "Kill": {"precision": 0.7916666666666666, "recall": 0.8085106382978723, "f1": 0.7999999999999999}, "Work_for": {"precision": 0.5753424657534246, "recall": 0.5526315789473685, "f1": 0.563758389261745}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-16-18-44", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}, {"Model": "./models/deepseek-coder-7b-base-v1.5_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.5760869565217391, "Recall": 0.5208845208845209, "F1_Score": 0.5470967741935484, "rel_type_metrics": {"Organization_based_in": {"precision": 0.8070175438596491, "recall": 0.4791666666666667, "f1": 0.6013071895424837}, "Kill": {"precision": 0.36792452830188677, "recall": 0.8297872340425532, "f1": 0.5098039215686274}, "Located_in": {"precision": 0.6166666666666667, "recall": 0.4111111111111111, "f1": 0.4933333333333334}, "Work_for": {"precision": 0.6363636363636364, "recall": 0.4605263157894737, "f1": 0.5343511450381679}, "Live_in": {"precision": 0.6111111111111112, "recall": 0.5612244897959183, "f1": 0.5851063829787235}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-04-05-11", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}]