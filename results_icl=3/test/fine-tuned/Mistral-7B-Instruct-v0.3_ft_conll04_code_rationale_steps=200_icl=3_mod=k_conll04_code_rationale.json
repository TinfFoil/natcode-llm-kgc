[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_code_rationale_steps=200_icl=3_mod=k", "Precision": 0.38903394255874674, "Recall": 0.36609336609336607, "F1_Score": 0.3772151898734177, "rel_type_metrics": {"Live_in": {"precision": 0.40384615384615385, "recall": 0.21428571428571427, "f1": 0.27999999999999997}, "Kill": {"precision": 0.6938775510204082, "recall": 0.723404255319149, "f1": 0.7083333333333333}, "Work_for": {"precision": 0.35, "recall": 0.5526315789473685, "f1": 0.4285714285714286}, "Organization_based_in": {"precision": 0.6428571428571429, "recall": 0.09375, "f1": 0.16363636363636364}, "Located_in": {"precision": 0.3115942028985507, "recall": 0.4777777777777778, "f1": 0.37719298245614036}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-43-43", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_code_rationale_steps=200_icl=3_mod=k", "Precision": 0.4745762711864407, "Recall": 0.41277641277641275, "F1_Score": 0.4415243101182655, "rel_type_metrics": {"Work_for": {"precision": 0.35789473684210527, "recall": 0.4473684210526316, "f1": 0.39766081871345027}, "Kill": {"precision": 0.723404255319149, "recall": 0.723404255319149, "f1": 0.723404255319149}, "Located_in": {"precision": 0.4222222222222222, "recall": 0.4222222222222222, "f1": 0.4222222222222222}, "Live_in": {"precision": 0.525, "recall": 0.42857142857142855, "f1": 0.4719101123595506}, "Organization_based_in": {"precision": 0.6451612903225806, "recall": 0.20833333333333334, "f1": 0.31496062992125984}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-47-29", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_code_rationale_steps=200_icl=3_mod=k", "Precision": 0.4920212765957447, "Recall": 0.45454545454545453, "F1_Score": 0.4725415070242656, "rel_type_metrics": {"Kill": {"precision": 0.5833333333333334, "recall": 0.7446808510638298, "f1": 0.6542056074766355}, "Work_for": {"precision": 0.42105263157894735, "recall": 0.42105263157894735, "f1": 0.42105263157894735}, "Located_in": {"precision": 0.4696969696969697, "recall": 0.34444444444444444, "f1": 0.3974358974358974}, "Live_in": {"precision": 0.5487804878048781, "recall": 0.45918367346938777, "f1": 0.5}, "Organization_based_in": {"precision": 0.5121951219512195, "recall": 0.4375, "f1": 0.47191011235955055}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-54-30", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}]