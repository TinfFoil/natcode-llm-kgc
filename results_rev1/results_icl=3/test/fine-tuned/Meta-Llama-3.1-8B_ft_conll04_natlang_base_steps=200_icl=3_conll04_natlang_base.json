[{"Model": "./models/Meta-Llama-3.1-8B_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.49206349206349204, "Recall": 0.5331695331695332, "F1_Score": 0.5117924528301886, "rel_type_metrics": {"Live_in": {"precision": 0.40186915887850466, "recall": 0.4387755102040816, "f1": 0.4195121951219512}, "Located_in": {"precision": 0.4835164835164835, "recall": 0.4888888888888889, "f1": 0.4861878453038674}, "Kill": {"precision": 0.8235294117647058, "recall": 0.8936170212765957, "f1": 0.8571428571428571}, "Work_for": {"precision": 0.39090909090909093, "recall": 0.5657894736842105, "f1": 0.46236559139784944}, "Organization_based_in": {"precision": 0.5625, "recall": 0.46875, "f1": 0.5113636363636364}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-27-00-55-58", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/Meta-Llama-3.1-8B_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.44136460554371004, "Recall": 0.5085995085995086, "F1_Score": 0.4726027397260274, "rel_type_metrics": {"Live_in": {"precision": 0.25949367088607594, "recall": 0.41836734693877553, "f1": 0.3203125}, "Kill": {"precision": 0.6730769230769231, "recall": 0.7446808510638298, "f1": 0.7070707070707072}, "Located_in": {"precision": 0.4639175257731959, "recall": 0.5, "f1": 0.4812834224598931}, "Organization_based_in": {"precision": 0.5287356321839081, "recall": 0.4791666666666667, "f1": 0.5027322404371585}, "Work_for": {"precision": 0.5333333333333333, "recall": 0.5263157894736842, "f1": 0.5298013245033113}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-19-46-37", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/Meta-Llama-3.1-8B_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.43125, "Recall": 0.5085995085995086, "F1_Score": 0.46674182638105977, "rel_type_metrics": {"Kill": {"precision": 0.35714285714285715, "recall": 0.7446808510638298, "f1": 0.4827586206896552}, "Located_in": {"precision": 0.4205607476635514, "recall": 0.5, "f1": 0.4568527918781726}, "Organization_based_in": {"precision": 0.45918367346938777, "recall": 0.46875, "f1": 0.4639175257731959}, "Work_for": {"precision": 0.5616438356164384, "recall": 0.5394736842105263, "f1": 0.5503355704697986}, "Live_in": {"precision": 0.3942307692307692, "recall": 0.41836734693877553, "f1": 0.4059405940594059}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-09-27-38", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}]