[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=6", "Precision": 0.6770833333333334, "Recall": 0.6388206388206388, "F1_Score": 0.6573957016434894, "rel_type_metrics": {"Kill": {"precision": 0.84, "recall": 0.8936170212765957, "f1": 0.8659793814432989}, "Organization_based_in": {"precision": 0.6506024096385542, "recall": 0.5625, "f1": 0.6033519553072625}, "Work_for": {"precision": 0.7166666666666667, "recall": 0.5657894736842105, "f1": 0.6323529411764706}, "Located_in": {"precision": 0.675, "recall": 0.6, "f1": 0.6352941176470589}, "Live_in": {"precision": 0.6036036036036037, "recall": 0.6836734693877551, "f1": 0.6411483253588517}}, "n_icl_samples": 6, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-18-12-36", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=6", "Precision": 0.6735751295336787, "Recall": 0.6388206388206388, "F1_Score": 0.6557377049180327, "rel_type_metrics": {"Live_in": {"precision": 0.6666666666666666, "recall": 0.6938775510204082, "f1": 0.6799999999999999}, "Organization_based_in": {"precision": 0.6973684210526315, "recall": 0.5520833333333334, "f1": 0.6162790697674418}, "Work_for": {"precision": 0.6338028169014085, "recall": 0.5921052631578947, "f1": 0.6122448979591837}, "Kill": {"precision": 0.8571428571428571, "recall": 0.8936170212765957, "f1": 0.875}, "Located_in": {"precision": 0.5977011494252874, "recall": 0.5777777777777777, "f1": 0.5875706214689267}}, "n_icl_samples": 6, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-18-16-37", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=6", "Precision": 0.6912928759894459, "Recall": 0.6437346437346437, "F1_Score": 0.6666666666666666, "rel_type_metrics": {"Live_in": {"precision": 0.6868686868686869, "recall": 0.6938775510204082, "f1": 0.6903553299492386}, "Kill": {"precision": 0.82, "recall": 0.8723404255319149, "f1": 0.8453608247422681}, "Located_in": {"precision": 0.6835443037974683, "recall": 0.6, "f1": 0.6390532544378698}, "Organization_based_in": {"precision": 0.5977011494252874, "recall": 0.5416666666666666, "f1": 0.5683060109289617}, "Work_for": {"precision": 0.746031746031746, "recall": 0.618421052631579, "f1": 0.6762589928057554}}, "n_icl_samples": 6, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-18-20-34", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}]