[{"Model": "./models/Meta-Llama-3.1-70B_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.1796875, "Recall": 0.33906633906633904, "F1_Score": 0.23489361702127656, "rel_type_metrics": {"Live_in": {"precision": 0.16129032258064516, "recall": 0.25510204081632654, "f1": 0.1976284584980237}, "Organization_based_in": {"precision": 0.24193548387096775, "recall": 0.15625, "f1": 0.189873417721519}, "Located_in": {"precision": 0.16145833333333334, "recall": 0.34444444444444444, "f1": 0.21985815602836878}, "Kill": {"precision": 0.33783783783783783, "recall": 0.5319148936170213, "f1": 0.4132231404958678}, "Work_for": {"precision": 0.168, "recall": 0.5526315789473685, "f1": 0.25766871165644173}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-13-01-18-54", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Meta-Llama-3.1-70B_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.20614035087719298, "Recall": 0.3464373464373464, "F1_Score": 0.25847846012832265, "rel_type_metrics": {"Live_in": {"precision": 0.12658227848101267, "recall": 0.20408163265306123, "f1": 0.15625}, "Work_for": {"precision": 0.3007518796992481, "recall": 0.5263157894736842, "f1": 0.3827751196172249}, "Kill": {"precision": 0.3225806451612903, "recall": 0.425531914893617, "f1": 0.3669724770642202}, "Located_in": {"precision": 0.28205128205128205, "recall": 0.24444444444444444, "f1": 0.2619047619047619}, "Organization_based_in": {"precision": 0.1566265060240964, "recall": 0.40625, "f1": 0.22608695652173916}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-13-04-13-27", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Meta-Llama-3.1-70B_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.1552511415525114, "Recall": 0.16707616707616707, "F1_Score": 0.16094674556213018, "rel_type_metrics": {"Live_in": {"precision": 0.1412639405204461, "recall": 0.3877551020408163, "f1": 0.20708446866485014}, "Kill": {"precision": 0.44, "recall": 0.23404255319148937, "f1": 0.3055555555555556}, "Organization_based_in": {"precision": 0, "recall": 0.0, "f1": 0}, "Located_in": {"precision": 0.06741573033707865, "recall": 0.06666666666666667, "f1": 0.0670391061452514}, "Work_for": {"precision": 0.40625, "recall": 0.17105263157894737, "f1": 0.24074074074074076}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-13-05-50-55", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}]