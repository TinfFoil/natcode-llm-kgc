[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=15", "Precision": 0.7028423772609819, "Recall": 0.6683046683046683, "F1_Score": 0.6851385390428212, "rel_type_metrics": {"Organization_based_in": {"precision": 0.6477272727272727, "recall": 0.59375, "f1": 0.6195652173913043}, "Live_in": {"precision": 0.6261682242990654, "recall": 0.6836734693877551, "f1": 0.6536585365853659}, "Kill": {"precision": 0.8958333333333334, "recall": 0.9148936170212766, "f1": 0.9052631578947369}, "Located_in": {"precision": 0.7341772151898734, "recall": 0.6444444444444445, "f1": 0.6863905325443788}, "Work_for": {"precision": 0.7230769230769231, "recall": 0.618421052631579, "f1": 0.6666666666666667}}, "n_icl_samples": 15, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-32-36", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=15", "Precision": 0.6928934010152284, "Recall": 0.6707616707616708, "F1_Score": 0.6816479400749065, "rel_type_metrics": {"Located_in": {"precision": 0.7160493827160493, "recall": 0.6444444444444445, "f1": 0.6783625730994153}, "Organization_based_in": {"precision": 0.6590909090909091, "recall": 0.6041666666666666, "f1": 0.6304347826086956}, "Work_for": {"precision": 0.7121212121212122, "recall": 0.618421052631579, "f1": 0.6619718309859155}, "Kill": {"precision": 0.7543859649122807, "recall": 0.9148936170212766, "f1": 0.8269230769230769}, "Live_in": {"precision": 0.67, "recall": 0.6836734693877551, "f1": 0.6767676767676768}}, "n_icl_samples": 15, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-34-53", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=15", "Precision": 0.7028423772609819, "Recall": 0.6683046683046683, "F1_Score": 0.6851385390428212, "rel_type_metrics": {"Kill": {"precision": 0.8958333333333334, "recall": 0.9148936170212766, "f1": 0.9052631578947369}, "Organization_based_in": {"precision": 0.5462962962962963, "recall": 0.6145833333333334, "f1": 0.5784313725490196}, "Work_for": {"precision": 0.746031746031746, "recall": 0.618421052631579, "f1": 0.6762589928057554}, "Live_in": {"precision": 0.7052631578947368, "recall": 0.6836734693877551, "f1": 0.694300518134715}, "Located_in": {"precision": 0.7671232876712328, "recall": 0.6222222222222222, "f1": 0.6871165644171779}}, "n_icl_samples": 15, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-36-58", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}]