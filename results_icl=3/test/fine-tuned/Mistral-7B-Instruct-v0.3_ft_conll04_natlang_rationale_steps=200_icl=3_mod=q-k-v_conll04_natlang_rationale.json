[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3_mod=q-k-v", "Precision": 0.5421686746987951, "Recall": 0.5528255528255528, "F1_Score": 0.5474452554744524, "rel_type_metrics": {"Organization_based_in": {"precision": 0.5970149253731343, "recall": 0.4166666666666667, "f1": 0.4907975460122699}, "Work_for": {"precision": 0.5625, "recall": 0.5921052631578947, "f1": 0.5769230769230769}, "Located_in": {"precision": 0.4842105263157895, "recall": 0.5111111111111111, "f1": 0.4972972972972973}, "Kill": {"precision": 0.6721311475409836, "recall": 0.8723404255319149, "f1": 0.7592592592592592}, "Live_in": {"precision": 0.5145631067961165, "recall": 0.5408163265306123, "f1": 0.527363184079602}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-15-09", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3_mod=q-k-v", "Precision": 0.38534278959810875, "Recall": 0.4004914004914005, "F1_Score": 0.3927710843373494, "rel_type_metrics": {"Located_in": {"precision": 0.36470588235294116, "recall": 0.34444444444444444, "f1": 0.35428571428571426}, "Kill": {"precision": 0.6923076923076923, "recall": 0.574468085106383, "f1": 0.627906976744186}, "Organization_based_in": {"precision": 0.4358974358974359, "recall": 0.3541666666666667, "f1": 0.3908045977011494}, "Work_for": {"precision": 0.5070422535211268, "recall": 0.47368421052631576, "f1": 0.489795918367347}, "Live_in": {"precision": 0.29914529914529914, "recall": 0.35714285714285715, "f1": 0.3255813953488372}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-35-45", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3_mod=q-k-v", "Precision": 0.4968421052631579, "Recall": 0.5798525798525799, "F1_Score": 0.5351473922902495, "rel_type_metrics": {"Organization_based_in": {"precision": 0.5180722891566265, "recall": 0.4479166666666667, "f1": 0.4804469273743016}, "Kill": {"precision": 0.6268656716417911, "recall": 0.8936170212765957, "f1": 0.7368421052631579}, "Work_for": {"precision": 0.5280898876404494, "recall": 0.618421052631579, "f1": 0.5696969696969697}, "Live_in": {"precision": 0.4778761061946903, "recall": 0.5510204081632653, "f1": 0.5118483412322274}, "Located_in": {"precision": 0.4854368932038835, "recall": 0.5555555555555556, "f1": 0.5181347150259067}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-53-32", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}]