[{"Model": "mistralai/Mistral-7B-Instruct-v0.3", "Precision": 0.1276595744680851, "Recall": 0.08845208845208845, "F1_Score": 0.10449927431059507, "rel_type_metrics": {"Work_for": {"precision": 0.15584415584415584, "recall": 0.3157894736842105, "f1": 0.20869565217391303}, "Live_in": {"precision": 0.13043478260869565, "recall": 0.030612244897959183, "f1": 0.04958677685950413}, "Kill": {"precision": 0.6153846153846154, "recall": 0.1702127659574468, "f1": 0.26666666666666666}, "Located_in": {"precision": 0.0, "recall": 0.0, "f1": 0}, "Organization_based_in": {"precision": 0.2, "recall": 0.010416666666666666, "f1": 0.019801980198019802}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-08-38-48", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "mistralai/Mistral-7B-Instruct-v0.3", "Precision": 0.14849187935034802, "Recall": 0.15724815724815724, "F1_Score": 0.15274463007159905, "rel_type_metrics": {"Located_in": {"precision": 0.22, "recall": 0.12222222222222222, "f1": 0.15714285714285714}, "Organization_based_in": {"precision": 0.03076923076923077, "recall": 0.020833333333333332, "f1": 0.02484472049689441}, "Kill": {"precision": 0.6363636363636364, "recall": 0.2978723404255319, "f1": 0.40579710144927533}, "Work_for": {"precision": 0.18446601941747573, "recall": 0.25, "f1": 0.2122905027932961}, "Live_in": {"precision": 0.1978021978021978, "recall": 0.1836734693877551, "f1": 0.19047619047619047}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-08-42-15", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "mistralai/Mistral-7B-Instruct-v0.3", "Precision": 0.1556122448979592, "Recall": 0.14987714987714987, "F1_Score": 0.15269086357947434, "rel_type_metrics": {"Live_in": {"precision": 0.2, "recall": 0.1326530612244898, "f1": 0.15950920245398775}, "Work_for": {"precision": 0.16521739130434782, "recall": 0.25, "f1": 0.19895287958115182}, "Kill": {"precision": 0.6, "recall": 0.3191489361702128, "f1": 0.4166666666666667}, "Organization_based_in": {"precision": 0.25, "recall": 0.010416666666666666, "f1": 0.019999999999999997}, "Located_in": {"precision": 0.16455696202531644, "recall": 0.14444444444444443, "f1": 0.15384615384615383}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-08-45-34", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}]