[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3_mod=k", "Precision": 0.4623115577889447, "Recall": 0.22604422604422605, "F1_Score": 0.30363036303630364, "rel_type_metrics": {"Work_for": {"precision": 0.42857142857142855, "recall": 0.3157894736842105, "f1": 0.36363636363636365}, "Live_in": {"precision": 0.6956521739130435, "recall": 0.16326530612244897, "f1": 0.2644628099173554}, "Kill": {"precision": 0.8636363636363636, "recall": 0.40425531914893614, "f1": 0.5507246376811594}, "Located_in": {"precision": 0.4084507042253521, "recall": 0.32222222222222224, "f1": 0.3602484472049689}, "Organization_based_in": {"precision": 0.18181818181818182, "recall": 0.041666666666666664, "f1": 0.06779661016949153}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-04-23", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3_mod=k", "Precision": 0.5308641975308642, "Recall": 0.31695331695331697, "F1_Score": 0.396923076923077, "rel_type_metrics": {"Organization_based_in": {"precision": 0.4358974358974359, "recall": 0.17708333333333334, "f1": 0.2518518518518518}, "Kill": {"precision": 0.5428571428571428, "recall": 0.40425531914893614, "f1": 0.46341463414634143}, "Work_for": {"precision": 0.5409836065573771, "recall": 0.4342105263157895, "f1": 0.48175182481751827}, "Located_in": {"precision": 0.4794520547945205, "recall": 0.3888888888888889, "f1": 0.42944785276073616}, "Live_in": {"precision": 0.7575757575757576, "recall": 0.25510204081632654, "f1": 0.3816793893129771}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-07-25", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3_mod=k", "Precision": 0.48494983277591974, "Recall": 0.35626535626535627, "F1_Score": 0.4107648725212465, "rel_type_metrics": {"Work_for": {"precision": 0.4, "recall": 0.3157894736842105, "f1": 0.35294117647058826}, "Live_in": {"precision": 0.8, "recall": 0.08163265306122448, "f1": 0.1481481481481481}, "Organization_based_in": {"precision": 0.40384615384615385, "recall": 0.4375, "f1": 0.42000000000000004}, "Kill": {"precision": 0.8636363636363636, "recall": 0.8085106382978723, "f1": 0.8351648351648351}, "Located_in": {"precision": 0.4177215189873418, "recall": 0.36666666666666664, "f1": 0.39053254437869817}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-10-24", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}]