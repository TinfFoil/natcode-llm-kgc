[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=0", "Precision": 0.7526132404181185, "Recall": 0.5307125307125307, "F1_Score": 0.622478386167147, "rel_type_metrics": {"Organization_based_in": {"precision": 0.6875, "recall": 0.4583333333333333, "f1": 0.5499999999999999}, "Kill": {"precision": 0.9148936170212766, "recall": 0.9148936170212766, "f1": 0.9148936170212766}, "Live_in": {"precision": 0.7532467532467533, "recall": 0.5918367346938775, "f1": 0.6628571428571429}, "Located_in": {"precision": 0.7380952380952381, "recall": 0.34444444444444444, "f1": 0.46969696969696967}, "Work_for": {"precision": 0.7017543859649122, "recall": 0.5263157894736842, "f1": 0.6015037593984963}}, "n_icl_samples": 0, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-00-19-36", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=0", "Precision": 0.7491289198606271, "Recall": 0.5282555282555282, "F1_Score": 0.6195965417867435, "rel_type_metrics": {"Organization_based_in": {"precision": 0.6875, "recall": 0.4583333333333333, "f1": 0.5499999999999999}, "Located_in": {"precision": 0.7441860465116279, "recall": 0.35555555555555557, "f1": 0.481203007518797}, "Live_in": {"precision": 0.7368421052631579, "recall": 0.5714285714285714, "f1": 0.6436781609195403}, "Kill": {"precision": 0.9148936170212766, "recall": 0.9148936170212766, "f1": 0.9148936170212766}, "Work_for": {"precision": 0.7017543859649122, "recall": 0.5263157894736842, "f1": 0.6015037593984963}}, "n_icl_samples": 0, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-00-22-33", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=0", "Precision": 0.7491289198606271, "Recall": 0.5282555282555282, "F1_Score": 0.6195965417867435, "rel_type_metrics": {"Kill": {"precision": 0.9148936170212766, "recall": 0.9148936170212766, "f1": 0.9148936170212766}, "Organization_based_in": {"precision": 0.6875, "recall": 0.4583333333333333, "f1": 0.5499999999999999}, "Located_in": {"precision": 0.7209302325581395, "recall": 0.34444444444444444, "f1": 0.4661654135338345}, "Work_for": {"precision": 0.7142857142857143, "recall": 0.5263157894736842, "f1": 0.6060606060606061}, "Live_in": {"precision": 0.7402597402597403, "recall": 0.5816326530612245, "f1": 0.6514285714285715}}, "n_icl_samples": 0, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-00-25-26", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}]