[{"Model": "./models/deepseek-coder-7b-base-v1.5_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.529045643153527, "Recall": 0.6265356265356266, "F1_Score": 0.5736782902137234, "rel_type_metrics": {"Work_for": {"precision": 0.5681818181818182, "recall": 0.6578947368421053, "f1": 0.6097560975609756}, "Kill": {"precision": 0.7368421052631579, "recall": 0.8936170212765957, "f1": 0.8076923076923077}, "Located_in": {"precision": 0.4778761061946903, "recall": 0.6, "f1": 0.5320197044334977}, "Organization_based_in": {"precision": 0.5681818181818182, "recall": 0.5208333333333334, "f1": 0.5434782608695652}, "Live_in": {"precision": 0.4338235294117647, "recall": 0.6020408163265306, "f1": 0.5042735042735041}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-27-00-43-37", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/deepseek-coder-7b-base-v1.5_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.5916473317865429, "Recall": 0.6265356265356266, "F1_Score": 0.6085918854415274, "rel_type_metrics": {"Live_in": {"precision": 0.6105263157894737, "recall": 0.5918367346938775, "f1": 0.6010362694300517}, "Located_in": {"precision": 0.5185185185185185, "recall": 0.6222222222222222, "f1": 0.5656565656565656}, "Kill": {"precision": 0.8431372549019608, "recall": 0.9148936170212766, "f1": 0.8775510204081632}, "Work_for": {"precision": 0.4948453608247423, "recall": 0.631578947368421, "f1": 0.5549132947976879}, "Organization_based_in": {"precision": 0.625, "recall": 0.5208333333333334, "f1": 0.5681818181818181}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-19-34-15", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/deepseek-coder-7b-base-v1.5_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.5422680412371134, "Recall": 0.6461916461916462, "F1_Score": 0.5896860986547084, "rel_type_metrics": {"Organization_based_in": {"precision": 0.5795454545454546, "recall": 0.53125, "f1": 0.5543478260869565}, "Live_in": {"precision": 0.41496598639455784, "recall": 0.6224489795918368, "f1": 0.4979591836734694}, "Work_for": {"precision": 0.6621621621621622, "recall": 0.6447368421052632, "f1": 0.6533333333333333}, "Located_in": {"precision": 0.48412698412698413, "recall": 0.6777777777777778, "f1": 0.5648148148148148}, "Kill": {"precision": 0.82, "recall": 0.8723404255319149, "f1": 0.8453608247422681}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-09-15-54", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}]