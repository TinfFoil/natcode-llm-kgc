[{"Model": "unsloth/Meta-Llama-3.1-8B-Instruct", "Precision": 0.4782608695652174, "Recall": 0.024886877828054297, "F1_Score": 0.047311827956989246, "rel_type_metrics": {"Adverse_effect": {"precision": 0.4782608695652174, "recall": 0.024886877828054297, "f1": 0.047311827956989246}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-26-23-58-25", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-8B-Instruct", "Precision": 0.2857142857142857, "Recall": 0.01809954751131222, "F1_Score": 0.03404255319148936, "rel_type_metrics": {"Adverse_effect": {"precision": 0.2857142857142857, "recall": 0.01809954751131222, "f1": 0.03404255319148936}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-28-18-50-25", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-8B-Instruct", "Precision": 0.5909090909090909, "Recall": 0.08823529411764706, "F1_Score": 0.15354330708661418, "rel_type_metrics": {"Adverse_effect": {"precision": 0.609375, "recall": 0.08823529411764706, "f1": 0.1541501976284585}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-29-08-01-16", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}]