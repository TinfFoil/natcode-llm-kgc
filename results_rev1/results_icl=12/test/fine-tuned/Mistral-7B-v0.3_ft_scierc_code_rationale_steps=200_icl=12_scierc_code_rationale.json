[{"Model": "./models/Mistral-7B-v0.3_ft_scierc_code_rationale_steps=200_icl=12", "Precision": 0.31646825396825395, "Recall": 0.3275154004106776, "F1_Score": 0.3218970736629667, "rel_type_metrics": {"Conjunction": {"precision": 0.39823008849557523, "recall": 0.36585365853658536, "f1": 0.38135593220338976}, "Feature_of": {"precision": 0.11627906976744186, "recall": 0.0847457627118644, "f1": 0.09803921568627452}, "Hyponym_of": {"precision": 0.3559322033898305, "recall": 0.31343283582089554, "f1": 0.33333333333333337}, "Evaluate_for": {"precision": 0.2595419847328244, "recall": 0.37362637362637363, "f1": 0.3063063063063063}, "Used_for": {"precision": 0.3310344827586207, "recall": 0.3602251407129456, "f1": 0.3450134770889488}, "Compare": {"precision": 0.2558139534883721, "recall": 0.2894736842105263, "f1": 0.2716049382716049}, "Part_of": {"precision": 0.28205128205128205, "recall": 0.1746031746031746, "f1": 0.2156862745098039}}, "n_icl_samples": 12, "n_samples_test": 397, "dataset": "scierc", "date": "2025-01-18-21-37-10", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_scierc_code_rationale_steps=200_icl=12", "Precision": 0.3154819863680623, "Recall": 0.3326488706365503, "F1_Score": 0.3238380809595202, "rel_type_metrics": {"Used_for": {"precision": 0.3215547703180212, "recall": 0.34146341463414637, "f1": 0.33121019108280253}, "Evaluate_for": {"precision": 0.2653061224489796, "recall": 0.42857142857142855, "f1": 0.3277310924369748}, "Part_of": {"precision": 0.27906976744186046, "recall": 0.19047619047619047, "f1": 0.22641509433962262}, "Hyponym_of": {"precision": 0.3384615384615385, "recall": 0.3283582089552239, "f1": 0.33333333333333337}, "Conjunction": {"precision": 0.4065040650406504, "recall": 0.4065040650406504, "f1": 0.4065040650406504}, "Compare": {"precision": 0.35714285714285715, "recall": 0.39473684210526316, "f1": 0.37500000000000006}, "Feature_of": {"precision": 0.0975609756097561, "recall": 0.06779661016949153, "f1": 0.08}}, "n_icl_samples": 12, "n_samples_test": 397, "dataset": "scierc", "date": "2025-01-18-21-57-29", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_scierc_code_rationale_steps=200_icl=12", "Precision": 0.3162743091095189, "Recall": 0.31724845995893225, "F1_Score": 0.3167606355715018, "rel_type_metrics": {"Hyponym_of": {"precision": 0.39285714285714285, "recall": 0.3283582089552239, "f1": 0.35772357723577236}, "Evaluate_for": {"precision": 0.24113475177304963, "recall": 0.37362637362637363, "f1": 0.29310344827586204}, "Compare": {"precision": 0.275, "recall": 0.2894736842105263, "f1": 0.28205128205128205}, "Conjunction": {"precision": 0.41284403669724773, "recall": 0.36585365853658536, "f1": 0.3879310344827586}, "Part_of": {"precision": 0.2894736842105263, "recall": 0.1746031746031746, "f1": 0.21782178217821782}, "Used_for": {"precision": 0.33090909090909093, "recall": 0.34146341463414637, "f1": 0.3361034164358265}, "Feature_of": {"precision": 0.0975609756097561, "recall": 0.06779661016949153, "f1": 0.08}}, "n_icl_samples": 12, "n_samples_test": 397, "dataset": "scierc", "date": "2025-01-18-22-22-00", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}]