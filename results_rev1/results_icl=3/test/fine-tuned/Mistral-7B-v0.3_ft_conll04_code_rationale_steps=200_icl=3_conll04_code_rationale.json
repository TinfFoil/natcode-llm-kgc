[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.6292428198433421, "Recall": 0.5921375921375921, "F1_Score": 0.610126582278481, "rel_type_metrics": {"Organization_based_in": {"precision": 0.4491525423728814, "recall": 0.5520833333333334, "f1": 0.4953271028037384}, "Located_in": {"precision": 0.6290322580645161, "recall": 0.43333333333333335, "f1": 0.513157894736842}, "Kill": {"precision": 0.8571428571428571, "recall": 0.8936170212765957, "f1": 0.875}, "Work_for": {"precision": 0.75, "recall": 0.5921052631578947, "f1": 0.6617647058823529}, "Live_in": {"precision": 0.6595744680851063, "recall": 0.6326530612244898, "f1": 0.6458333333333333}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-26-21-52-56", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.612590799031477, "Recall": 0.6216216216216216, "F1_Score": 0.6170731707317073, "rel_type_metrics": {"Located_in": {"precision": 0.36764705882352944, "recall": 0.5555555555555556, "f1": 0.4424778761061947}, "Organization_based_in": {"precision": 0.620253164556962, "recall": 0.5104166666666666, "f1": 0.5599999999999999}, "Kill": {"precision": 0.8571428571428571, "recall": 0.8936170212765957, "f1": 0.875}, "Live_in": {"precision": 0.7701149425287356, "recall": 0.6836734693877551, "f1": 0.7243243243243244}, "Work_for": {"precision": 0.7258064516129032, "recall": 0.5921052631578947, "f1": 0.6521739130434783}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-15-50-02", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.5961995249406176, "Recall": 0.6167076167076168, "F1_Score": 0.606280193236715, "rel_type_metrics": {"Live_in": {"precision": 0.7078651685393258, "recall": 0.6428571428571429, "f1": 0.6737967914438502}, "Located_in": {"precision": 0.5638297872340425, "recall": 0.5888888888888889, "f1": 0.5760869565217391}, "Kill": {"precision": 0.8571428571428571, "recall": 0.8936170212765957, "f1": 0.875}, "Work_for": {"precision": 0.39823008849557523, "recall": 0.5921052631578947, "f1": 0.47619047619047616}, "Organization_based_in": {"precision": 0.631578947368421, "recall": 0.5, "f1": 0.5581395348837209}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-03-34-20", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}]