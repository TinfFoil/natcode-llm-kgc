[{"Model": "./models/CodeQwen1.5-7B-Chat_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.4904862579281184, "Recall": 0.5700245700245701, "F1_Score": 0.5272727272727273, "rel_type_metrics": {"Work_for": {"precision": 0.4939759036144578, "recall": 0.5394736842105263, "f1": 0.5157232704402516}, "Organization_based_in": {"precision": 0.3409090909090909, "recall": 0.46875, "f1": 0.39473684210526316}, "Live_in": {"precision": 0.5686274509803921, "recall": 0.5918367346938775, "f1": 0.58}, "Located_in": {"precision": 0.46788990825688076, "recall": 0.5666666666666667, "f1": 0.5125628140703516}, "Kill": {"precision": 0.7872340425531915, "recall": 0.7872340425531915, "f1": 0.7872340425531915}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-19-53-08", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/CodeQwen1.5-7B-Chat_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.5511627906976744, "Recall": 0.5823095823095823, "F1_Score": 0.5663082437275986, "rel_type_metrics": {"Located_in": {"precision": 0.4065040650406504, "recall": 0.5555555555555556, "f1": 0.4694835680751174}, "Live_in": {"precision": 0.5714285714285714, "recall": 0.5714285714285714, "f1": 0.5714285714285714}, "Work_for": {"precision": 0.6666666666666666, "recall": 0.5789473684210527, "f1": 0.619718309859155}, "Organization_based_in": {"precision": 0.5632183908045977, "recall": 0.5104166666666666, "f1": 0.53551912568306}, "Kill": {"precision": 0.6785714285714286, "recall": 0.8085106382978723, "f1": 0.737864077669903}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-09-34-30", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/CodeQwen1.5-7B-Chat_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.5382882882882883, "Recall": 0.5872235872235873, "F1_Score": 0.5616921269095183, "rel_type_metrics": {"Work_for": {"precision": 0.39655172413793105, "recall": 0.6052631578947368, "f1": 0.47916666666666674}, "Located_in": {"precision": 0.5875, "recall": 0.5222222222222223, "f1": 0.5529411764705883}, "Live_in": {"precision": 0.6037735849056604, "recall": 0.6530612244897959, "f1": 0.6274509803921567}, "Organization_based_in": {"precision": 0.5056179775280899, "recall": 0.46875, "f1": 0.4864864864864865}, "Kill": {"precision": 0.6981132075471698, "recall": 0.7872340425531915, "f1": 0.74}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-09-35-39", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}]