[{"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.43636363636363634, "Recall": 0.5307125307125307, "F1_Score": 0.4789356984478936, "rel_type_metrics": {"Located_in": {"precision": 0.3300970873786408, "recall": 0.37777777777777777, "f1": 0.35233160621761656}, "Work_for": {"precision": 0.31137724550898205, "recall": 0.6842105263157895, "f1": 0.4279835390946502}, "Organization_based_in": {"precision": 0.6140350877192983, "recall": 0.3645833333333333, "f1": 0.45751633986928103}, "Kill": {"precision": 0.7288135593220338, "recall": 0.9148936170212766, "f1": 0.8113207547169811}, "Live_in": {"precision": 0.47706422018348627, "recall": 0.5306122448979592, "f1": 0.5024154589371982}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-26-23-16-27", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.42828685258964144, "Recall": 0.5282555282555282, "F1_Score": 0.4730473047304731, "rel_type_metrics": {"Located_in": {"precision": 0.2356687898089172, "recall": 0.4111111111111111, "f1": 0.29959514170040485}, "Work_for": {"precision": 0.3790322580645161, "recall": 0.618421052631579, "f1": 0.47000000000000003}, "Kill": {"precision": 0.7647058823529411, "recall": 0.8297872340425532, "f1": 0.7959183673469387}, "Live_in": {"precision": 0.4636363636363636, "recall": 0.5204081632653061, "f1": 0.4903846153846154}, "Organization_based_in": {"precision": 0.6833333333333333, "recall": 0.4270833333333333, "f1": 0.5256410256410257}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-17-48-21", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.40643863179074446, "Recall": 0.4963144963144963, "F1_Score": 0.4469026548672566, "rel_type_metrics": {"Kill": {"precision": 0.7307692307692307, "recall": 0.8085106382978723, "f1": 0.7676767676767676}, "Work_for": {"precision": 0.4, "recall": 0.5, "f1": 0.4444444444444445}, "Live_in": {"precision": 0.32386363636363635, "recall": 0.5816326530612245, "f1": 0.41605839416058393}, "Located_in": {"precision": 0.2956521739130435, "recall": 0.37777777777777777, "f1": 0.3317073170731707}, "Organization_based_in": {"precision": 0.5932203389830508, "recall": 0.3645833333333333, "f1": 0.4516129032258065}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-06-24-47", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}]