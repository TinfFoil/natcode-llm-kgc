[{"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.18063314711359404, "Recall": 0.23832923832923833, "F1_Score": 0.2055084745762712, "rel_type_metrics": {"Work_for": {"precision": 0.6666666666666666, "recall": 0.02631578947368421, "f1": 0.05063291139240506}, "Live_in": {"precision": 0.30097087378640774, "recall": 0.3163265306122449, "f1": 0.30845771144278605}, "Organization_based_in": {"precision": 0.3763440860215054, "recall": 0.3645833333333333, "f1": 0.3703703703703703}, "Located_in": {"precision": 0.5178571428571429, "recall": 0.32222222222222224, "f1": 0.3972602739726027}, "Kill": {"precision": 0, "recall": 0.0, "f1": 0}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-11-23-57-42", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.16666666666666666, "Recall": 0.22113022113022113, "F1_Score": 0.1900739176346357, "rel_type_metrics": {"Organization_based_in": {"precision": 0.25, "recall": 0.13541666666666666, "f1": 0.17567567567567569}, "Kill": {"precision": 0.3333333333333333, "recall": 0.02127659574468085, "f1": 0.04}, "Live_in": {"precision": 0.24342105263157895, "recall": 0.37755102040816324, "f1": 0.296}, "Work_for": {"precision": 0.6, "recall": 0.039473684210526314, "f1": 0.07407407407407407}, "Located_in": {"precision": 0.3829787234042553, "recall": 0.4, "f1": 0.3913043478260869}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-12-00-10-16", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.21458333333333332, "Recall": 0.25307125307125306, "F1_Score": 0.2322435174746336, "rel_type_metrics": {"Located_in": {"precision": 0.3548387096774194, "recall": 0.36666666666666664, "f1": 0.36065573770491804}, "Organization_based_in": {"precision": 0, "recall": 0.0, "f1": 0}, "Kill": {"precision": 0.5294117647058824, "recall": 0.19148936170212766, "f1": 0.28125}, "Work_for": {"precision": 0.37719298245614036, "recall": 0.5657894736842105, "f1": 0.45263157894736844}, "Live_in": {"precision": 0.23076923076923078, "recall": 0.1836734693877551, "f1": 0.20454545454545456}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-12-00-21-38", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}]