[{"Model": "deepseek-ai/deepseek-coder-7b-instruct-v1.5", "Precision": 0.13026052104208416, "Recall": 0.1597051597051597, "F1_Score": 0.14348785871964678, "rel_type_metrics": {"Organization_based_in": {"precision": 0.14953271028037382, "recall": 0.16666666666666666, "f1": 0.15763546798029554}, "Located_in": {"precision": 0.13333333333333333, "recall": 0.13333333333333333, "f1": 0.13333333333333333}, "Live_in": {"precision": 0.22807017543859648, "recall": 0.1326530612244898, "f1": 0.16774193548387098}, "Work_for": {"precision": 0.10365853658536585, "recall": 0.2236842105263158, "f1": 0.14166666666666666}, "Kill": {"precision": 0.25, "recall": 0.14893617021276595, "f1": 0.18666666666666665}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-26-23-30-09", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "deepseek-ai/deepseek-coder-7b-instruct-v1.5", "Precision": 0.10633946830265849, "Recall": 0.12776412776412777, "F1_Score": 0.11607142857142858, "rel_type_metrics": {"Work_for": {"precision": 0.13636363636363635, "recall": 0.11842105263157894, "f1": 0.1267605633802817}, "Live_in": {"precision": 0.5, "recall": 0.1326530612244898, "f1": 0.20967741935483872}, "Kill": {"precision": 0.42105263157894735, "recall": 0.1702127659574468, "f1": 0.24242424242424243}, "Located_in": {"precision": 0.08673469387755102, "recall": 0.18888888888888888, "f1": 0.11888111888111888}, "Organization_based_in": {"precision": 0.4166666666666667, "recall": 0.052083333333333336, "f1": 0.0925925925925926}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-18-01-50", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "deepseek-ai/deepseek-coder-7b-instruct-v1.5", "Precision": 0.08333333333333333, "Recall": 0.08845208845208845, "F1_Score": 0.08581644815256258, "rel_type_metrics": {"Work_for": {"precision": 0.0963855421686747, "recall": 0.10526315789473684, "f1": 0.10062893081761007}, "Live_in": {"precision": 0.0, "recall": 0.0, "f1": 0}, "Located_in": {"precision": 0.23529411764705882, "recall": 0.08888888888888889, "f1": 0.12903225806451613}, "Organization_based_in": {"precision": 0, "recall": 0.0, "f1": 0}, "Kill": {"precision": 0.21978021978021978, "recall": 0.425531914893617, "f1": 0.28985507246376807}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-06-43-17", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}]