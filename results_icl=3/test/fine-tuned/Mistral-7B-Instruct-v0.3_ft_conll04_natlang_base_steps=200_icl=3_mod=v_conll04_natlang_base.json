[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_base_steps=200_icl=3_mod=v", "Precision": 0.6512820512820513, "Recall": 0.6240786240786241, "F1_Score": 0.6373902132998746, "rel_type_metrics": {"Live_in": {"precision": 0.5806451612903226, "recall": 0.5510204081632653, "f1": 0.5654450261780105}, "Work_for": {"precision": 0.5698924731182796, "recall": 0.6973684210526315, "f1": 0.6272189349112426}, "Located_in": {"precision": 0.675, "recall": 0.6, "f1": 0.6352941176470589}, "Kill": {"precision": 0.84, "recall": 0.8936170212765957, "f1": 0.8659793814432989}, "Organization_based_in": {"precision": 0.6891891891891891, "recall": 0.53125, "f1": 0.6000000000000001}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-23-03-51", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_base_steps=200_icl=3_mod=v", "Precision": 0.6613756613756614, "Recall": 0.6142506142506142, "F1_Score": 0.6369426751592357, "rel_type_metrics": {"Kill": {"precision": 0.875, "recall": 0.8936170212765957, "f1": 0.8842105263157894}, "Live_in": {"precision": 0.5789473684210527, "recall": 0.5612244897959183, "f1": 0.5699481865284974}, "Located_in": {"precision": 0.6235294117647059, "recall": 0.5888888888888889, "f1": 0.6057142857142856}, "Organization_based_in": {"precision": 0.6911764705882353, "recall": 0.4895833333333333, "f1": 0.5731707317073171}, "Work_for": {"precision": 0.6463414634146342, "recall": 0.6973684210526315, "f1": 0.6708860759493671}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-23-07-28", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_base_steps=200_icl=3_mod=v", "Precision": 0.6550802139037433, "Recall": 0.601965601965602, "F1_Score": 0.6274007682458388, "rel_type_metrics": {"Located_in": {"precision": 0.6363636363636364, "recall": 0.5444444444444444, "f1": 0.5868263473053892}, "Organization_based_in": {"precision": 0.7391304347826086, "recall": 0.53125, "f1": 0.6181818181818182}, "Kill": {"precision": 0.875, "recall": 0.8936170212765957, "f1": 0.8842105263157894}, "Live_in": {"precision": 0.5454545454545454, "recall": 0.5510204081632653, "f1": 0.5482233502538071}, "Work_for": {"precision": 0.6049382716049383, "recall": 0.6447368421052632, "f1": 0.6242038216560509}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-23-11-04", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}]