[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3_mod=q-k", "Precision": 0.5186440677966102, "Recall": 0.3759213759213759, "F1_Score": 0.4358974358974359, "rel_type_metrics": {"Live_in": {"precision": 0.7017543859649122, "recall": 0.40816326530612246, "f1": 0.5161290322580645}, "Kill": {"precision": 0.78125, "recall": 0.5319148936170213, "f1": 0.6329113924050632}, "Work_for": {"precision": 0.625, "recall": 0.4605263157894737, "f1": 0.5303030303030303}, "Located_in": {"precision": 0.6222222222222222, "recall": 0.3111111111111111, "f1": 0.4148148148148148}, "Organization_based_in": {"precision": 0.45454545454545453, "recall": 0.2604166666666667, "f1": 0.33112582781456956}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-03-28", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3_mod=q-k", "Precision": 0.5674846625766872, "Recall": 0.45454545454545453, "F1_Score": 0.504774897680764, "rel_type_metrics": {"Work_for": {"precision": 0.5223880597014925, "recall": 0.4605263157894737, "f1": 0.4895104895104895}, "Live_in": {"precision": 0.7868852459016393, "recall": 0.4897959183673469, "f1": 0.6037735849056604}, "Kill": {"precision": 0.8461538461538461, "recall": 0.7021276595744681, "f1": 0.7674418604651163}, "Located_in": {"precision": 0.5423728813559322, "recall": 0.35555555555555557, "f1": 0.42953020134228187}, "Organization_based_in": {"precision": 0.6065573770491803, "recall": 0.3854166666666667, "f1": 0.4713375796178344}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-07-10", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3_mod=q-k", "Precision": 0.1841541755888651, "Recall": 0.2113022113022113, "F1_Score": 0.19679633867276886, "rel_type_metrics": {"Organization_based_in": {"precision": 0.8095238095238095, "recall": 0.17708333333333334, "f1": 0.2905982905982906}, "Kill": {"precision": 0.6, "recall": 0.2553191489361702, "f1": 0.3582089552238805}, "Live_in": {"precision": 0.35, "recall": 0.14285714285714285, "f1": 0.20289855072463767}, "Work_for": {"precision": 0.475, "recall": 0.25, "f1": 0.3275862068965517}, "Located_in": {"precision": 0.2222222222222222, "recall": 0.26666666666666666, "f1": 0.2424242424242424}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-10-14", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}]