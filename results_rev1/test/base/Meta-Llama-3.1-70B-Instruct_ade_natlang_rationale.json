[{"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.497029702970297, "Recall": 0.5678733031674208, "F1_Score": 0.5300950369588173, "rel_type_metrics": {"Adverse_effect": {"precision": 0.5080971659919028, "recall": 0.5678733031674208, "f1": 0.5363247863247862}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2025-01-09-02-48-35", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.7130434782608696, "Recall": 0.5565610859728507, "F1_Score": 0.6251588310038119, "rel_type_metrics": {"Adverse_effect": {"precision": 0.7214076246334311, "recall": 0.5565610859728507, "f1": 0.628352490421456}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2025-01-09-03-14-21", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.672289156626506, "Recall": 0.6312217194570136, "F1_Score": 0.6511085180863477, "rel_type_metrics": {"Adverse_effect": {"precision": 0.6838235294117647, "recall": 0.6312217194570136, "f1": 0.6564705882352941}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2025-01-09-03-51-03", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": true, "chat_model": true}]