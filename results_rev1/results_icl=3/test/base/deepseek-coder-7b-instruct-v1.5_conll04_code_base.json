[{"Model": "deepseek-ai/deepseek-coder-7b-instruct-v1.5", "Precision": 0.12722298221614228, "Recall": 0.2285012285012285, "F1_Score": 0.1634446397188049, "rel_type_metrics": {"Live_in": {"precision": 0.13333333333333333, "recall": 0.16326530612244897, "f1": 0.14678899082568808}, "Located_in": {"precision": 0.22377622377622378, "recall": 0.35555555555555557, "f1": 0.27467811158798283}, "Organization_based_in": {"precision": 0.07407407407407407, "recall": 0.08333333333333333, "f1": 0.07843137254901962}, "Work_for": {"precision": 0.11304347826086956, "recall": 0.34210526315789475, "f1": 0.16993464052287582}, "Kill": {"precision": 0.2391304347826087, "recall": 0.23404255319148937, "f1": 0.23655913978494622}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-27-02-59-09", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "deepseek-ai/deepseek-coder-7b-instruct-v1.5", "Precision": 0.09766454352441614, "Recall": 0.11302211302211303, "F1_Score": 0.10478359908883826, "rel_type_metrics": {"Kill": {"precision": 0.37037037037037035, "recall": 0.2127659574468085, "f1": 0.2702702702702703}, "Work_for": {"precision": 0.09090909090909091, "recall": 0.14473684210526316, "f1": 0.1116751269035533}, "Organization_based_in": {"precision": 0.2, "recall": 0.03125, "f1": 0.05405405405405406}, "Live_in": {"precision": 0.1323529411764706, "recall": 0.09183673469387756, "f1": 0.10843373493975902}, "Located_in": {"precision": 0.11711711711711711, "recall": 0.14444444444444443, "f1": 0.12935323383084577}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-22-21-07", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "deepseek-ai/deepseek-coder-7b-instruct-v1.5", "Precision": 0.08682170542635659, "Recall": 0.1375921375921376, "F1_Score": 0.1064638783269962, "rel_type_metrics": {"Work_for": {"precision": 0.09765625, "recall": 0.32894736842105265, "f1": 0.15060240963855423}, "Kill": {"precision": 0.5555555555555556, "recall": 0.3191489361702128, "f1": 0.40540540540540543}, "Live_in": {"precision": 0.25925925925925924, "recall": 0.14285714285714285, "f1": 0.18421052631578946}, "Organization_based_in": {"precision": 0.0, "recall": 0.0, "f1": 0}, "Located_in": {"precision": 0.02702702702702703, "recall": 0.022222222222222223, "f1": 0.024390243902439025}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-12-52-12", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}]