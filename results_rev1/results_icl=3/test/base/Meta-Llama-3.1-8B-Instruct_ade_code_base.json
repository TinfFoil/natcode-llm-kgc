[{"Model": "unsloth/Meta-Llama-3.1-8B-Instruct", "Precision": 0.07692307692307693, "Recall": 0.0022624434389140274, "F1_Score": 0.004395604395604396, "rel_type_metrics": {"Adverse_effect": {"precision": 0.1, "recall": 0.0022624434389140274, "f1": 0.004424778761061947}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-27-03-21-11", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-8B-Instruct", "Precision": 0.1111111111111111, "Recall": 0.0022624434389140274, "F1_Score": 0.00443458980044346, "rel_type_metrics": {"Adverse_effect": {"precision": 0.1111111111111111, "recall": 0.0022624434389140274, "f1": 0.00443458980044346}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-28-23-02-48", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-8B-Instruct", "Precision": 0.05, "Recall": 0.0022624434389140274, "F1_Score": 0.00432900432900433, "rel_type_metrics": {"Adverse_effect": {"precision": 0.07692307692307693, "recall": 0.0022624434389140274, "f1": 0.004395604395604396}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-29-14-00-04", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}]