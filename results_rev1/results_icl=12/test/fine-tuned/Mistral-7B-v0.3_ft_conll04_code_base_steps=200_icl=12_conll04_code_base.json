[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=12", "Precision": 0.7047146401985112, "Recall": 0.6977886977886978, "F1_Score": 0.7012345679012346, "rel_type_metrics": {"Kill": {"precision": 0.8775510204081632, "recall": 0.9148936170212766, "f1": 0.8958333333333333}, "Organization_based_in": {"precision": 0.7073170731707317, "recall": 0.6041666666666666, "f1": 0.6516853932584269}, "Work_for": {"precision": 0.746268656716418, "recall": 0.6578947368421053, "f1": 0.6993006993006994}, "Live_in": {"precision": 0.6698113207547169, "recall": 0.7244897959183674, "f1": 0.696078431372549}, "Located_in": {"precision": 0.6262626262626263, "recall": 0.6888888888888889, "f1": 0.6560846560846562}}, "n_icl_samples": 12, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-23-04-48", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=12", "Precision": 0.6934673366834171, "Recall": 0.6781326781326781, "F1_Score": 0.6857142857142857, "rel_type_metrics": {"Located_in": {"precision": 0.5454545454545454, "recall": 0.6666666666666666, "f1": 0.6}, "Work_for": {"precision": 0.7538461538461538, "recall": 0.6447368421052632, "f1": 0.6950354609929077}, "Kill": {"precision": 0.8571428571428571, "recall": 0.8936170212765957, "f1": 0.875}, "Live_in": {"precision": 0.6826923076923077, "recall": 0.7244897959183674, "f1": 0.702970297029703}, "Organization_based_in": {"precision": 0.7714285714285715, "recall": 0.5625, "f1": 0.6506024096385543}}, "n_icl_samples": 12, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-23-08-16", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=12", "Precision": 0.7131979695431472, "Recall": 0.6904176904176904, "F1_Score": 0.7016229712858927, "rel_type_metrics": {"Live_in": {"precision": 0.6605504587155964, "recall": 0.7346938775510204, "f1": 0.6956521739130436}, "Work_for": {"precision": 0.7419354838709677, "recall": 0.6052631578947368, "f1": 0.6666666666666666}, "Located_in": {"precision": 0.7191011235955056, "recall": 0.7111111111111111, "f1": 0.7150837988826815}, "Organization_based_in": {"precision": 0.6785714285714286, "recall": 0.59375, "f1": 0.6333333333333334}, "Kill": {"precision": 0.84, "recall": 0.8936170212765957, "f1": 0.8659793814432989}}, "n_icl_samples": 12, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-23-11-28", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}]