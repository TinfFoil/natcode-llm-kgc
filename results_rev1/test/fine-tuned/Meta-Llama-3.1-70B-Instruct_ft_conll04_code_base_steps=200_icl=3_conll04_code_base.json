[{"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.22356495468277945, "Recall": 0.36363636363636365, "F1_Score": 0.2768942937324603, "rel_type_metrics": {"Kill": {"precision": 0.4666666666666667, "recall": 0.5957446808510638, "f1": 0.5233644859813084}, "Organization_based_in": {"precision": 0.17647058823529413, "recall": 0.03125, "f1": 0.05309734513274336}, "Work_for": {"precision": 0.2781065088757396, "recall": 0.618421052631579, "f1": 0.3836734693877551}, "Live_in": {"precision": 0.24752475247524752, "recall": 0.25510204081632654, "f1": 0.2512562814070352}, "Located_in": {"precision": 0.14376996805111822, "recall": 0.5, "f1": 0.22332506203473948}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-12-03-14-18", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.19817073170731708, "Recall": 0.3194103194103194, "F1_Score": 0.24459078080903104, "rel_type_metrics": {"Organization_based_in": {"precision": 0.0, "recall": 0.0, "f1": 0}, "Kill": {"precision": 0.54, "recall": 0.574468085106383, "f1": 0.5567010309278351}, "Live_in": {"precision": 0.2608695652173913, "recall": 0.1836734693877551, "f1": 0.2155688622754491}, "Located_in": {"precision": 0.13709677419354838, "recall": 0.5666666666666667, "f1": 0.22077922077922077}, "Work_for": {"precision": 0.22818791946308725, "recall": 0.4473684210526316, "f1": 0.3022222222222222}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-12-03-50-05", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.16824196597353497, "Recall": 0.21867321867321868, "F1_Score": 0.19017094017094013, "rel_type_metrics": {"Kill": {"precision": 0.5714285714285714, "recall": 0.2553191489361702, "f1": 0.35294117647058826}, "Live_in": {"precision": 0.28888888888888886, "recall": 0.1326530612244898, "f1": 0.1818181818181818}, "Located_in": {"precision": 0.10670731707317073, "recall": 0.3888888888888889, "f1": 0.16746411483253587}, "Work_for": {"precision": 0.23931623931623933, "recall": 0.3684210526315789, "f1": 0.2901554404145078}, "Organization_based_in": {"precision": 0.2, "recall": 0.010416666666666666, "f1": 0.019801980198019802}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-12-04-36-01", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": true}]