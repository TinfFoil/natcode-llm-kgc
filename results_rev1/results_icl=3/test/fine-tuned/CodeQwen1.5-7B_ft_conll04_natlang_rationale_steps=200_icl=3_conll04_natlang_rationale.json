[{"Model": "./models/CodeQwen1.5-7B_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.550251256281407, "Recall": 0.538083538083538, "F1_Score": 0.5440993788819876, "rel_type_metrics": {"Work_for": {"precision": 0.5641025641025641, "recall": 0.5789473684210527, "f1": 0.5714285714285715}, "Located_in": {"precision": 0.5714285714285714, "recall": 0.4888888888888889, "f1": 0.5269461077844311}, "Live_in": {"precision": 0.6666666666666666, "recall": 0.4897959183673469, "f1": 0.5647058823529412}, "Kill": {"precision": 0.7551020408163265, "recall": 0.7872340425531915, "f1": 0.7708333333333333}, "Organization_based_in": {"precision": 0.3770491803278688, "recall": 0.4791666666666667, "f1": 0.4220183486238532}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-14-46-07", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/CodeQwen1.5-7B_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.5146067415730337, "Recall": 0.5626535626535627, "F1_Score": 0.5375586854460095, "rel_type_metrics": {"Work_for": {"precision": 0.2787878787878788, "recall": 0.6052631578947368, "f1": 0.3817427385892116}, "Kill": {"precision": 0.7755102040816326, "recall": 0.8085106382978723, "f1": 0.7916666666666665}, "Organization_based_in": {"precision": 0.5903614457831325, "recall": 0.5104166666666666, "f1": 0.5474860335195529}, "Live_in": {"precision": 0.7428571428571429, "recall": 0.5306122448979592, "f1": 0.6190476190476191}, "Located_in": {"precision": 0.5641025641025641, "recall": 0.4888888888888889, "f1": 0.5238095238095238}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-01-40-15", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/CodeQwen1.5-7B_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.5165094339622641, "Recall": 0.538083538083538, "F1_Score": 0.5270758122743682, "rel_type_metrics": {"Live_in": {"precision": 0.7014925373134329, "recall": 0.47959183673469385, "f1": 0.5696969696969697}, "Work_for": {"precision": 0.4891304347826087, "recall": 0.5921052631578947, "f1": 0.5357142857142857}, "Organization_based_in": {"precision": 0.5844155844155844, "recall": 0.46875, "f1": 0.5202312138728324}, "Kill": {"precision": 0.5362318840579711, "recall": 0.7872340425531915, "f1": 0.6379310344827587}, "Located_in": {"precision": 0.37815126050420167, "recall": 0.5, "f1": 0.43062200956937796}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-01-41-45", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}]