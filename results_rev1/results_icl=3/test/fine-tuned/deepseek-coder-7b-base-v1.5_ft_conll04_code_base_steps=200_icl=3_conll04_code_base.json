[{"Model": "./models/deepseek-coder-7b-base-v1.5_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.478, "Recall": 0.5872235872235873, "F1_Score": 0.5270121278941566, "rel_type_metrics": {"Kill": {"precision": 0.6774193548387096, "recall": 0.8936170212765957, "f1": 0.7706422018348622}, "Located_in": {"precision": 0.29444444444444445, "recall": 0.5888888888888889, "f1": 0.39259259259259266}, "Live_in": {"precision": 0.5670103092783505, "recall": 0.5612244897959183, "f1": 0.5641025641025641}, "Organization_based_in": {"precision": 0.5466666666666666, "recall": 0.4270833333333333, "f1": 0.47953216374269003}, "Work_for": {"precision": 0.5783132530120482, "recall": 0.631578947368421, "f1": 0.6037735849056604}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-27-02-11-57", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/deepseek-coder-7b-base-v1.5_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.5161290322580645, "Recall": 0.628992628992629, "F1_Score": 0.566998892580288, "rel_type_metrics": {"Work_for": {"precision": 0.38524590163934425, "recall": 0.618421052631579, "f1": 0.4747474747474748}, "Kill": {"precision": 0.7857142857142857, "recall": 0.9361702127659575, "f1": 0.854368932038835}, "Located_in": {"precision": 0.4112903225806452, "recall": 0.5666666666666667, "f1": 0.47663551401869153}, "Organization_based_in": {"precision": 0.6022727272727273, "recall": 0.5520833333333334, "f1": 0.5760869565217391}, "Live_in": {"precision": 0.5754716981132075, "recall": 0.6224489795918368, "f1": 0.5980392156862745}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-21-16-28", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/deepseek-coder-7b-base-v1.5_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.4691119691119691, "Recall": 0.597051597051597, "F1_Score": 0.5254054054054054, "rel_type_metrics": {"Work_for": {"precision": 0.5050505050505051, "recall": 0.6578947368421053, "f1": 0.5714285714285714}, "Organization_based_in": {"precision": 0.6024096385542169, "recall": 0.5208333333333334, "f1": 0.558659217877095}, "Live_in": {"precision": 0.3254437869822485, "recall": 0.5612244897959183, "f1": 0.41198501872659177}, "Kill": {"precision": 0.7321428571428571, "recall": 0.8723404255319149, "f1": 0.7961165048543688}, "Located_in": {"precision": 0.42342342342342343, "recall": 0.5222222222222223, "f1": 0.4676616915422886}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-11-32-52", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}]