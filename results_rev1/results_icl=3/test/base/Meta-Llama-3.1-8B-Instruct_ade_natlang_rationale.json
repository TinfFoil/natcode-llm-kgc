[{"Model": "unsloth/Meta-Llama-3.1-8B-Instruct", "Precision": 0.30691056910569103, "Recall": 0.3416289592760181, "F1_Score": 0.3233404710920771, "rel_type_metrics": {"Adverse_effect": {"precision": 0.40591397849462363, "recall": 0.3416289592760181, "f1": 0.371007371007371}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-26-21-25-46", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-8B-Instruct", "Precision": 0.36363636363636365, "Recall": 0.4253393665158371, "F1_Score": 0.3920750782064651, "rel_type_metrics": {"Adverse_effect": {"precision": 0.39166666666666666, "recall": 0.4253393665158371, "f1": 0.4078091106290672}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-28-15-24-27", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-8B-Instruct", "Precision": 0.2766497461928934, "Recall": 0.24660633484162897, "F1_Score": 0.2607655502392345, "rel_type_metrics": {"Adverse_effect": {"precision": 0.4658119658119658, "recall": 0.24660633484162897, "f1": 0.32248520710059175}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-29-02-48-53", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": true, "chat_model": true}]