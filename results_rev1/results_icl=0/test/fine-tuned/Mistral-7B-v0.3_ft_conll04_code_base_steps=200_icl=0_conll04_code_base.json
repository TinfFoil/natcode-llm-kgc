[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=0", "Precision": 0.6528497409326425, "Recall": 0.6191646191646192, "F1_Score": 0.635561160151324, "rel_type_metrics": {"Live_in": {"precision": 0.6018518518518519, "recall": 0.6632653061224489, "f1": 0.6310679611650485}, "Located_in": {"precision": 0.5934065934065934, "recall": 0.6, "f1": 0.5966850828729282}, "Organization_based_in": {"precision": 0.6666666666666666, "recall": 0.5416666666666666, "f1": 0.5977011494252873}, "Kill": {"precision": 0.7843137254901961, "recall": 0.851063829787234, "f1": 0.8163265306122448}, "Work_for": {"precision": 0.7068965517241379, "recall": 0.5394736842105263, "f1": 0.6119402985074627}}, "n_icl_samples": 0, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-02-35-05", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=0", "Precision": 0.6469072164948454, "Recall": 0.6167076167076168, "F1_Score": 0.6314465408805031, "rel_type_metrics": {"Work_for": {"precision": 0.7068965517241379, "recall": 0.5394736842105263, "f1": 0.6119402985074627}, "Kill": {"precision": 0.7843137254901961, "recall": 0.851063829787234, "f1": 0.8163265306122448}, "Located_in": {"precision": 0.5824175824175825, "recall": 0.5888888888888889, "f1": 0.5856353591160222}, "Organization_based_in": {"precision": 0.6582278481012658, "recall": 0.5416666666666666, "f1": 0.5942857142857143}, "Live_in": {"precision": 0.5963302752293578, "recall": 0.6632653061224489, "f1": 0.6280193236714976}}, "n_icl_samples": 0, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-02-40-51", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=0", "Precision": 0.6571428571428571, "Recall": 0.6216216216216216, "F1_Score": 0.6388888888888888, "rel_type_metrics": {"Organization_based_in": {"precision": 0.6753246753246753, "recall": 0.5416666666666666, "f1": 0.6011560693641618}, "Work_for": {"precision": 0.7241379310344828, "recall": 0.5526315789473685, "f1": 0.6268656716417911}, "Live_in": {"precision": 0.6074766355140186, "recall": 0.6632653061224489, "f1": 0.6341463414634146}, "Kill": {"precision": 0.7843137254901961, "recall": 0.851063829787234, "f1": 0.8163265306122448}, "Located_in": {"precision": 0.5869565217391305, "recall": 0.6, "f1": 0.5934065934065934}}, "n_icl_samples": 0, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-02-46-31", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}]