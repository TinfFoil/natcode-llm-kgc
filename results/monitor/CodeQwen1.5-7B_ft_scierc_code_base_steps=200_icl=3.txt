text: We propose a novel step toward the unsupervised seg-mentation of whole objects by combining '' hints '' of partial scene segmentation offered by multiple soft , binary mattes .
result: extract = Extract([Triple(Other_scientific_term("'' hints '' of partial scene segmentation"), Rel("Used_for"), Task("unsupervised seg-mentation of whole objects")), Triple(Other_scientific_term("soft , binary mattes"), Rel("Used_for"), Other_scientific_term("'' hints '' of partial scene segmentation"))])
pred: [["'' hints '' of partial scene segmentation", 'Used_for', 'unsupervised seg-mentation of whole objects'], ['soft , binary mattes', 'Used_for', "'' hints '' of partial scene segmentation"]]
trues: [['partial scene segmentation', 'Used_for', 'unsupervised seg-mentation of whole objects'], ['soft , binary mattes', 'Used_for', 'partial scene segmentation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.0, 0.0, 0]
text: In practice , the current deep embedding methods use the Euclidean distance for the training and test .
result: extract = Extract([Triple(Other_scientific_term("Euclidean distance"), Rel("Used_for"), Method("deep embedding methods"))])
pred: [['Euclidean distance', 'Used_for', 'deep embedding methods']]
trues: [['Euclidean distance', 'Used_for', 'deep embedding methods']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
text: It is presented as a generalization of the recursive descent parser .
result: extract = Extract([Triple(Method("generalization of the recursive descent parser"), Rel("Used_for"), Generic("It"))])
pred: [['generalization of the recursive descent parser', 'Used_for', 'It']]
trues: [['recursive descent parser', 'Used_for', 'It']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.25, 0.25, 0.25]
text: Starting from a DP-based solution to the traveling salesman problem , we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an efficient search algorithm .
result: extract = Extract([Triple(Method("DP-based solution"), Rel("Used_for"), Task("traveling salesman problem")), Triple(Generic("technique"), Rel("Used_for"), Other_scientific_term("word reordering")), Triple(Other_scientific_term("source and target language"), Rel("Feature_of"), Other_scientific_term("word reordering")), Triple(Method("efficient search algorithm"), Rel("Used_for"), Generic("technique"))])
pred: [['DP-based solution', 'Used_for', 'traveling salesman problem'], ['technique', 'Used_for', 'word reordering'], ['source and target language', 'Feature_of', 'word reordering'], ['efficient search algorithm', 'Used_for', 'technique']]
trues: [['technique', 'Used_for', 'search algorithm']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.125, 0.2, 0.15384615384615385]
text: We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing .
result: extract = Extract([Triple(Other_scientific_term("structure of email-threads"), Rel("Used_for"), Other_scientific_term("features")), Triple(Other_scientific_term("features"), Rel("Used_for"), Metric("lexical similarity of discourse segments")), Triple(Metric("lexical similarity of discourse segments"), Rel("Used_for"), Task("question-answer pairing"))])
pred: [['structure of email-threads', 'Used_for', 'features'], ['features', 'Used_for', 'lexical similarity of discourse segments'], ['lexical similarity of discourse segments', 'Used_for', 'question-answer pairing']]
trues: [['features', 'Used_for', 'lexical similarity'], ['features', 'Used_for', 'question-answer pairing'], ['structure of email-threads', 'Used_for', 'features'], ['lexical similarity', 'Feature_of', 'discourse segments']]
metrics_sample: [0.3333333333333333, 0.25, 0.28571428571428575]
metrics_current: [0.18181818181818182, 0.2222222222222222, 0.19999999999999998]
text: In general , our CRF model yields a lower error rate than the HMM and Max-ent models on the NIST sentence boundary detection task in speech , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .
result: extract = Extract([Triple(Method("CRF model"), Rel("Compare"), Method("HMM and Max-ent models")), Triple(Metric("error rate"), Rel("Evaluate_for"), Method("CRF model")), Triple(Metric("error rate"), Rel("Evaluate_for"), Method("HMM and Max-ent models")), Triple(Metric("error rate"), Rel("Evaluate_for"), Task("NIST sentence boundary detection task")), Triple(Method("HMM and Max-ent models"), Rel("Used_for"), Task("NIST sentence boundary detection task")), Triple(Task("NIST sentence boundary detection task"), Rel("Evaluate_for"), Method("speech"))])
pred: [['CRF model', 'Compare', 'HMM and Max-ent models'], ['error rate', 'Evaluate_for', 'CRF model'], ['error rate', 'Evaluate_for', 'HMM and Max-ent models'], ['error rate', 'Evaluate_for', 'NIST sentence boundary detection task'], ['HMM and Max-ent models', 'Used_for', 'NIST sentence boundary detection task'], ['NIST sentence boundary detection task', 'Evaluate_for', 'speech']]
trues: [['CRF model', 'Compare', 'HMM and Max-ent models'], ['error rate', 'Evaluate_for', 'CRF model'], ['error rate', 'Evaluate_for', 'HMM and Max-ent models'], ['NIST sentence boundary detection task', 'Evaluate_for', 'CRF model'], ['NIST sentence boundary detection task', 'Evaluate_for', 'HMM and Max-ent models'], ['speech', 'Feature_of', 'NIST sentence boundary detection task'], ['classifiers', 'Used_for', 'three-way voting']]
metrics_sample: [0.5, 0.42857142857142855, 0.4615384615384615]
metrics_current: [0.29411764705882354, 0.3125, 0.30303030303030304]
text: Our technique is based on an improved , dynamic-programming , stereo algorithm for efficient novel-view generation .
result: extract = Extract([Triple(Method("dynamic-programming , stereo algorithm"), Rel("Used_for"), Generic("technique")), Triple(Method("dynamic-programming , stereo algorithm"), Rel("Used_for"), Task("novel-view generation"))])
pred: [['dynamic-programming , stereo algorithm', 'Used_for', 'technique'], ['dynamic-programming , stereo algorithm', 'Used_for', 'novel-view generation']]
trues: [['technique', 'Used_for', 'novel-view generation'], ['dynamic-programming , stereo algorithm', 'Used_for', 'technique']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3157894736842105, 0.3333333333333333, 0.3243243243243243]
text: We propose a multi-task end-to-end Joint Classification-Regression Recurrent Neural Network to better explore the action type and temporal localiza-tion information .
result: extract = Extract([Triple(Method("Joint Classification-Regression Recurrent Neural Network"), Rel("Used_for"), Other_scientific_term("action type and temporal localiza-tion information"))])
pred: [['Joint Classification-Regression Recurrent Neural Network', 'Used_for', 'action type and temporal localiza-tion information']]
trues: [['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'Used_for', 'action type'], ['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'Used_for', 'temporal localiza-tion information'], ['action type', 'Conjunction', 'temporal localiza-tion information']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3, 0.2857142857142857, 0.2926829268292683]
text: The perception of transparent objects from images is known to be a very hard problem in vision .
result: extract = Extract([Triple(Task("perception of transparent objects"), Rel("Part_of"), Task("vision"))])
pred: [['perception of transparent objects', 'Part_of', 'vision']]
trues: [['images', 'Used_for', 'perception of transparent objects']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2857142857142857, 0.2727272727272727, 0.2790697674418604]
text: The intentional structure captures the discourse-relevant purposes , expressed in each of the linguistic segments as well as relationships among them .
result: extract = Extract([Triple(Other_scientific_term("intentional structure"), Rel("Used_for"), Other_scientific_term("discourse-relevant purposes")), Triple(Other_scientific_term("linguistic segments"), Rel("Part_of"), Other_scientific_term("intentional structure")), Triple(Other_scientific_term("linguistic segments"), Rel("Conjunction"), Other_scientific_term("relationships")), Triple(Other_scientific_term("relationships"), Rel("Part_of"), Other_scientific_term("intentional structure"))])
pred: [['intentional structure', 'Used_for', 'discourse-relevant purposes'], ['linguistic segments', 'Part_of', 'intentional structure'], ['linguistic segments', 'Conjunction', 'relationships'], ['relationships', 'Part_of', 'intentional structure']]
trues: [['intentional structure', 'Used_for', 'discourse-relevant purposes']]
metrics_sample: [0.25, 1.0, 0.4]
metrics_current: [0.28, 0.30434782608695654, 0.2916666666666667]
text: Our approach is among the first to propose a generative proba-bilistic framework for 3D object categorization .
result: extract = Extract([Triple(Method("generative proba-bilistic framework"), Rel("Used_for"), Task("3D object categorization"))])
pred: [['generative proba-bilistic framework', 'Used_for', '3D object categorization']]
trues: [['generative proba-bilistic framework', 'Used_for', '3D object categorization']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3076923076923077, 0.3333333333333333, 0.32]
text: A domain independent model is proposed for the automated interpretation of nominal compounds in English .
result: extract = Extract([Triple(Generic("domain independent model"), Rel("Used_for"), Task("automated interpretation of nominal compounds")), Triple(Material("English"), Rel("Used_for"), Task("automated interpretation of nominal compounds"))])
pred: [['domain independent model', 'Used_for', 'automated interpretation of nominal compounds'], ['English', 'Used_for', 'automated interpretation of nominal compounds']]
trues: [['domain independent model', 'Used_for', 'automated interpretation of nominal compounds'], ['English', 'Feature_of', 'nominal compounds']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.32142857142857145, 0.34615384615384615, 0.3333333333333333]
text: b ) The UV procedure is based on three different confidence tests , two based on acoustic measures and one founded on linguistic information , applied in a hierarchical structure .
result: extract = Extract([Triple(Method("UV procedure"), Rel("Used_for"), Other_scientific_term("confidence tests")), Triple(Other_scientific_term("acoustic measures"), Rel("Part_of"), Other_scientific_term("confidence tests")), Triple(Other_scientific_term("acoustic measures"), Rel("Conjunction"), Other_scientific_term("linguistic information")), Triple(Other_scientific_term("linguistic information"), Rel("Part_of"), Other_scientific_term("confidence tests")), Triple(Other_scientific_term("hierarchical structure"), Rel("Used_for"), Method("UV procedure"))])
pred: [['UV procedure', 'Used_for', 'confidence tests'], ['acoustic measures', 'Part_of', 'confidence tests'], ['acoustic measures', 'Conjunction', 'linguistic information'], ['linguistic information', 'Part_of', 'confidence tests'], ['hierarchical structure', 'Used_for', 'UV procedure']]
trues: [['confidence tests', 'Used_for', 'UV procedure'], ['confidence tests', 'Used_for', 'hierarchical structure'], ['two', 'Hyponym_of', 'confidence tests'], ['acoustic measures', 'Used_for', 'two'], ['one', 'Hyponym_of', 'confidence tests'], ['linguistic information', 'Used_for', 'one']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2727272727272727, 0.28125, 0.2769230769230769]
text: In this paper , we present our approach for using information extraction annotations to augment document retrieval for distillation .
result: extract = Extract([Triple(Other_scientific_term("information extraction annotations"), Rel("Used_for"), Task("document retrieval")), Triple(Task("document retrieval"), Rel("Used_for"), Task("distillation"))])
pred: [['information extraction annotations', 'Used_for', 'document retrieval'], ['document retrieval', 'Used_for', 'distillation']]
trues: [['information extraction annotations', 'Used_for', 'document retrieval for distillation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2571428571428571, 0.2727272727272727, 0.2647058823529411]
text: Our experiments on real data sets show that the resulting detector is more robust to the choice of training examples , and substantially improves both linear and kernel SVM when trained on 10 positive and 10 negative examples .
result: extract = Extract([Triple(Generic("detector"), Rel("Compare"), Method("linear and kernel SVM")), Triple(Other_scientific_term("training examples"), Rel("Used_for"), Generic("detector")), Triple(Other_scientific_term("training examples"), Rel("Used_for"), Method("linear and kernel SVM")), Triple(Material("real data sets"), Rel("Evaluate_for"), Generic("detector")), Triple(Material("real data sets"), Rel("Evaluate_for"), Method("linear and kernel SVM"))])
pred: [['detector', 'Compare', 'linear and kernel SVM'], ['training examples', 'Used_for', 'detector'], ['training examples', 'Used_for', 'linear and kernel SVM'], ['real data sets', 'Evaluate_for', 'detector'], ['real data sets', 'Evaluate_for', 'linear and kernel SVM']]
trues: [['real data sets', 'Evaluate_for', 'detector'], ['detector', 'Compare', 'linear and kernel SVM']]
metrics_sample: [0.4, 1.0, 0.5714285714285715]
metrics_current: [0.275, 0.3142857142857143, 0.29333333333333333]
text: This probably occurs because each model has different strengths and weaknesses for modeling the knowledge sources .
result: extract = Extract([Triple(Generic("model"), Rel("Used_for"), Other_scientific_term("knowledge sources"))])
pred: [['model', 'Used_for', 'knowledge sources']]
trues: [['model', 'Used_for', 'knowledge sources']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.2926829268292683, 0.3333333333333333, 0.3116883116883117]
text: Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel .
result: extract = Extract([Triple(Other_scientific_term("syntactic structure features"), Rel("Used_for"), Task("relation extraction")), Triple(Other_scientific_term("syntactic structure features"), Rel("Used_for"), Method("convolution tree kernel"))])
pred: [['syntactic structure features', 'Used_for', 'relation extraction'], ['syntactic structure features', 'Used_for', 'convolution tree kernel']]
trues: [['syntactic structure features', 'Feature_of', 'parse tree'], ['syntactic structure features', 'Used_for', 'relation extraction'], ['convolution tree kernel', 'Used_for', 'features']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.3023255813953488, 0.3333333333333333, 0.31707317073170727]
text: We provide a unified account of sentence-level and text-level anaphora within the framework of a dependency-based grammar model .
result: extract = Extract([Triple(Method("dependency-based grammar model"), Rel("Used_for"), Other_scientific_term("sentence-level and text-level anaphora"))])
pred: [['dependency-based grammar model', 'Used_for', 'sentence-level and text-level anaphora']]
trues: [['dependency-based grammar model', 'Used_for', 'sentence-level and text-level anaphora']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3181818181818182, 0.35, 0.3333333333333333]
text: Turkish is an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words .
result: extract = Extract([Triple(Other_scientific_term("derivational and inflectional suffixes"), Rel("Part_of"), Other_scientific_term("word structures"))])
pred: [['derivational and inflectional suffixes', 'Part_of', 'word structures']]
trues: [['Turkish', 'Hyponym_of', 'agglutinative language'], ['word structures', 'Feature_of', 'agglutinative language'], ['productive affixations of derivational and inflectional suffixes', 'Part_of', 'word structures']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3111111111111111, 0.32558139534883723, 0.3181818181818182]
text: The surface realizations of morphological constructions are constrained and modified by a number of phonetic rules such as vowel harmony .
result: extract = Extract([Triple(Other_scientific_term("vowel harmony"), Rel("Hyponym_of"), Other_scientific_term("phonetic rules"))])
pred: [['vowel harmony', 'Hyponym_of', 'phonetic rules']]
trues: [['phonetic rules', 'Used_for', 'surface realizations of morphological constructions'], ['vowel harmony', 'Hyponym_of', 'phonetic rules']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.32608695652173914, 0.3333333333333333, 0.32967032967032966]
text: We apply cluster analysis on the sampled parameter space to redetect the object and renew the local tracker .
result: extract = Extract([Triple(Method("cluster analysis"), Rel("Used_for"), Other_scientific_term("sampled parameter space")), Triple(Method("cluster analysis"), Rel("Used_for"), Task("redetect the object")), Triple(Method("local tracker"), Rel("Used_for"), Task("redetect the object"))])
pred: [['cluster analysis', 'Used_for', 'sampled parameter space'], ['cluster analysis', 'Used_for', 'redetect the object'], ['local tracker', 'Used_for', 'redetect the object']]
trues: [['cluster analysis', 'Used_for', 'sampled parameter space'], ['cluster analysis', 'Used_for', 'local tracker']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.32653061224489793, 0.3404255319148936, 0.3333333333333333]
text: Our extensive experimental results significantly improve over both uniform sampling and standard stratified sampling which are de-facto the industry standards .
result: extract = Extract([Triple(Method("uniform sampling"), Rel("Compare"), Method("stratified sampling")), Triple(Method("uniform sampling"), Rel("Hyponym_of"), Generic("industry standards")), Triple(Method("stratified sampling"), Rel("Hyponym_of"), Generic("industry standards"))])
pred: [['uniform sampling', 'Compare', 'stratified sampling'], ['uniform sampling', 'Hyponym_of', 'industry standards'], ['stratified sampling', 'Hyponym_of', 'industry standards']]
trues: [['uniform sampling', 'Conjunction', 'stratified sampling']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3076923076923077, 0.3333333333333333, 0.32]
text: It works by calculating eigenvectors of an adjacency graph 's Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors .
result: extract = Extract([Triple(Other_scientific_term("eigenvectors"), Rel("Used_for"), Other_scientific_term("submanifold of data")), Triple(Other_scientific_term("eigenvectors"), Rel("Used_for"), Task("cluster number estimation")), Triple(Other_scientific_term("Laplacian"), Rel("Used_for"), Other_scientific_term("eigenvectors"))])
pred: [['eigenvectors', 'Used_for', 'submanifold of data'], ['eigenvectors', 'Used_for', 'cluster number estimation'], ['Laplacian', 'Used_for', 'eigenvectors']]
trues: [['It', 'Used_for', 'submanifold'], ['eigenvectors', 'Used_for', 'It'], ["adjacency graph 's Laplacian", 'Feature_of', 'eigenvectors'], ['high dimensionality space', 'Used_for', 'submanifold'], ['cluster number estimation', 'Used_for', 'It'], ['cluster number estimation', 'Used_for', 'eigenvectors']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2909090909090909, 0.2962962962962963, 0.2935779816513761]
text: It also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ACE relation major types .
result: extract = Extract([Triple(Generic("method"), Rel("Compare"), Method("dependency tree kernels")), Triple(Metric("ACE relation major types"), Rel("Evaluate_for"), Generic("method")), Triple(Metric("ACE relation major types"), Rel("Evaluate_for"), Method("dependency tree kernels"))])
pred: [['method', 'Compare', 'dependency tree kernels'], ['ACE relation major types', 'Evaluate_for', 'method'], ['ACE relation major types', 'Evaluate_for', 'dependency tree kernels']]
trues: [['method', 'Compare', 'dependency tree kernels']]
metrics_sample: [0.3333333333333333, 1.0, 0.5]
metrics_current: [0.29310344827586204, 0.3090909090909091, 0.3008849557522123]
text: This model mimics the way in which images are processed in the visual pathway , rendering a feasible alternative for the implementation of early vision applications in standard technologies .
result: extract = Extract([Triple(Generic("model"), Rel("Used_for"), Task("early vision applications")), Triple(Method("standard technologies"), Rel("Used_for"), Task("early vision applications"))])
pred: [['model', 'Used_for', 'early vision applications'], ['standard technologies', 'Used_for', 'early vision applications']]
trues: [['visual pathway', 'Used_for', 'images']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2833333333333333, 0.30357142857142855, 0.29310344827586204]
text: This paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , dags , graphs , and products of them .
result: extract = Extract([Triple(Other_scientific_term("strings"), Rel("Part_of"), Generic("structures")), Triple(Other_scientific_term("strings"), Rel("Conjunction"), Other_scientific_term("trees")), Triple(Other_scientific_term("trees"), Rel("Part_of"), Generic("structures")), Triple(Other_scientific_term("trees"), Rel("Conjunction"), Other_scientific_term("dags")), Triple(Other_scientific_term("dags"), Rel("Part_of"), Generic("structures")), Triple(Other_scientific_term("dags"), Rel("Conjunction"), Other_scientific_term("graphs")), Triple(Other_scientific_term("graphs"), Rel("Part_of"), Generic("structures")), Triple(Other_scientific_term("graphs"), Rel("Conjunction"), Other_scientific_term("products of them")), Triple(Other_scientific_term("products of them"), Rel("Part_of"), Generic("structures"))])
pred: [['strings', 'Part_of', 'structures'], ['strings', 'Conjunction', 'trees'], ['trees', 'Part_of', 'structures'], ['trees', 'Conjunction', 'dags'], ['dags', 'Part_of', 'structures'], ['dags', 'Conjunction', 'graphs'], ['graphs', 'Part_of', 'structures'], ['graphs', 'Conjunction', 'products of them'], ['products of them', 'Part_of', 'structures']]
trues: [['strings', 'Hyponym_of', 'structures'], ['strings', 'Conjunction', 'trees'], ['trees', 'Hyponym_of', 'structures'], ['trees', 'Conjunction', 'dags'], ['dags', 'Hyponym_of', 'structures'], ['dags', 'Conjunction', 'graphs'], ['graphs', 'Hyponym_of', 'structures']]
metrics_sample: [0.3333333333333333, 0.42857142857142855, 0.375]
metrics_current: [0.2898550724637681, 0.31746031746031744, 0.30303030303030304]
text: Coedition of a natural language text and its representation in some interlingual form seems the best and simplest way to share text revision across languages .
result: extract = Extract([Triple(Other_scientific_term("interlingual form"), Rel("Used_for"), Other_scientific_term("natural language text")), Triple(Other_scientific_term("interlingual form"), Rel("Used_for"), Task("text revision"))])
pred: [['interlingual form', 'Used_for', 'natural language text'], ['interlingual form', 'Used_for', 'text revision']]
trues: [['Coedition', 'Used_for', 'text revision'], ['natural language text', 'Used_for', 'Coedition']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.28169014084507044, 0.3076923076923077, 0.29411764705882354]
text: It models reflection as regions containing two different layers moving over each other .
result: extract = Extract([Triple(Other_scientific_term("layers"), Rel("Part_of"), Other_scientific_term("regions"))])
pred: [['layers', 'Part_of', 'regions']]
trues: [['It', 'Used_for', 'reflection']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2777777777777778, 0.30303030303030304, 0.28985507246376807]
text: Many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions .
result: extract = Extract([Triple(Other_scientific_term("multinomial or categorical distributions"), Rel("Used_for"), Material("discrete data"))])
pred: [['multinomial or categorical distributions', 'Used_for', 'discrete data']]
trues: [['discrete data', 'Used_for', 'modeling problems'], ['multinomial or categorical distributions', 'Used_for', 'modeling problems']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.273972602739726, 0.29411764705882354, 0.2836879432624113]
text: In this paper we specialize the projective unifocal , bifo-cal , and trifocal tensors to the affine case , and show how the tensors obtained relate to the registered tensors encountered in previous work .
result: extract = Extract([Triple(Method("projective unifocal , bifocal , and trifocal tensors"), Rel("Used_for"), Other_scientific_term("affine case")), Triple(Method("projective unifocal , bifocal , and trifocal tensors"), Rel("Used_for"), Other_scientific_term("registered tensors")), Triple(Other_scientific_term("registered tensors"), Rel("Used_for"), Generic("work"))])
pred: [['projective unifocal , bifocal , and trifocal tensors', 'Used_for', 'affine case'], ['projective unifocal , bifocal , and trifocal tensors', 'Used_for', 'registered tensors'], ['registered tensors', 'Used_for', 'work']]
trues: [['projective unifocal , bifo-cal , and trifocal tensors', 'Used_for', 'affine case']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2631578947368421, 0.2898550724637681, 0.27586206896551724]
text: SVMs are known to be robust to overfitting ; however , a few training examples usually do not represent well the structure of the class .
result: extract = Extract([Triple(Other_scientific_term("overfitting"), Rel("Feature_of"), Method("SVMs"))])
pred: [['overfitting', 'Feature_of', 'SVMs']]
trues: [['SVMs', 'Used_for', 'overfitting']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2597402597402597, 0.2857142857142857, 0.27210884353741494]
text: Our method has two major components : a ) a pattern extraction scheme for efficiently harvesting patterns of given size and shape from expert game records and b ) a Bayesian learning algorithm ( in two variants ) that learns a distribution over the values of a move given a board position based on the local pattern context .
result: extract = Extract([Triple(Method("pattern extraction scheme"), Rel("Used_for"), Generic("method")), Triple(Method("pattern extraction scheme"), Rel("Conjunction"), Method("Bayesian learning algorithm")), Triple(Method("Bayesian learning algorithm"), Rel("Used_for"), Generic("method")), Triple(Method("Bayesian learning algorithm"), Rel("Used_for"), Other_scientific_term("distribution")), Triple(Other_scientific_term("local pattern context"), Rel("Feature_of"), Other_scientific_term("distribution"))])
pred: [['pattern extraction scheme', 'Used_for', 'method'], ['pattern extraction scheme', 'Conjunction', 'Bayesian learning algorithm'], ['Bayesian learning algorithm', 'Used_for', 'method'], ['Bayesian learning algorithm', 'Used_for', 'distribution'], ['local pattern context', 'Feature_of', 'distribution']]
trues: [['pattern extraction scheme', 'Part_of', 'method'], ['pattern extraction scheme', 'Conjunction', 'Bayesian learning algorithm'], ['Bayesian learning algorithm', 'Part_of', 'method']]
metrics_sample: [0.2, 0.3333333333333333, 0.25]
metrics_current: [0.25609756097560976, 0.2876712328767123, 0.27096774193548384]
text: A method for producing such phrases from a word-aligned corpora is proposed .
result: extract = Extract([Triple(Material("word-aligned corpora"), Rel("Used_for"), Other_scientific_term("phrases"))])
pred: [['word-aligned corpora', 'Used_for', 'phrases']]
trues: [['method', 'Used_for', 'phrases'], ['word-aligned corpora', 'Evaluate_for', 'method']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.25301204819277107, 0.28, 0.26582278481012656]
text: There are four language pairs currently supported by GLOSSER : English-Bulgarian , English-Estonian , English-Hungarian and French-Dutch .
result: extract = Extract([Triple(Material("English-Bulgarian"), Rel("Hyponym_of"), Material("language pairs")), Triple(Material("English-Bulgarian"), Rel("Conjunction"), Material("English-Estonian")), Triple(Material("English-Estonian"), Rel("Hyponym_of"), Material("language pairs")), Triple(Material("English-Estonian"), Rel("Conjunction"), Material("English-Hungarian")), Triple(Material("English-Hungarian"), Rel("Hyponym_of"), Material("language pairs")), Triple(Material("English-Hungarian"), Rel("Conjunction"), Material("French-Dutch")), Triple(Material("French-Dutch"), Rel("Hyponym_of"), Material("language pairs"))])
pred: [['English-Bulgarian', 'Hyponym_of', 'language pairs'], ['English-Bulgarian', 'Conjunction', 'English-Estonian'], ['English-Estonian', 'Hyponym_of', 'language pairs'], ['English-Estonian', 'Conjunction', 'English-Hungarian'], ['English-Hungarian', 'Hyponym_of', 'language pairs'], ['English-Hungarian', 'Conjunction', 'French-Dutch'], ['French-Dutch', 'Hyponym_of', 'language pairs']]
trues: [['language pairs', 'Used_for', 'GLOSSER'], ['English-Bulgarian', 'Hyponym_of', 'language pairs'], ['English-Bulgarian', 'Conjunction', 'English-Estonian'], ['English-Estonian', 'Hyponym_of', 'language pairs'], ['English-Estonian', 'Conjunction', 'English-Hungarian'], ['English-Hungarian', 'Hyponym_of', 'language pairs'], ['English-Hungarian', 'Conjunction', 'French-Dutch'], ['French-Dutch', 'Hyponym_of', 'language pairs']]
metrics_sample: [1.0, 0.875, 0.9333333333333333]
metrics_current: [0.3111111111111111, 0.3373493975903614, 0.3236994219653179]
text: Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB 's binding theory , while those for text-level anaphora incorporate an adapted version of a Grosz-Sidner-style focus model .
result: extract = Extract([Triple(Method("GB 's binding theory"), Rel("Used_for"), Task("anaphora resolution")), Triple(Method("GB 's binding theory"), Rel("Conjunction"), Method("focus model")), Triple(Method("focus model"), Rel("Used_for"), Task("text-level anaphora"))])
pred: [["GB 's binding theory", 'Used_for', 'anaphora resolution'], ["GB 's binding theory", 'Conjunction', 'focus model'], ['focus model', 'Used_for', 'text-level anaphora']]
trues: [['Criteria', 'Used_for', 'anaphora resolution within sentence boundaries'], ["GB 's binding theory", 'Used_for', 'Criteria'], ['those', 'Used_for', 'text-level anaphora'], ['Grosz-Sidner-style focus model', 'Part_of', 'those']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3010752688172043, 0.3218390804597701, 0.3111111111111111]
text: MINPRAN 's properties are connrmed experimentally on synthetic data and compare favorably to least median of squares .
result: extract = Extract([Triple(Generic("synthetic data"), Rel("Used_for"), Generic("MINPRAN 's properties")), Triple(Generic("synthetic data"), Rel("Used_for"), Method("least median of squares"))])
pred: [['synthetic data', 'Used_for', "MINPRAN 's properties"], ['synthetic data', 'Used_for', 'least median of squares']]
trues: [['synthetic data', 'Evaluate_for', 'MINPRAN'], ['least median of squares', 'Compare', 'MINPRAN']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.29473684210526313, 0.3146067415730337, 0.30434782608695654]
text: We believe that these evaluation techniques will provide information about both the human language learning process , the translation process and the development of machine translation systems .
result: extract = Extract([Triple(Generic("evaluation techniques"), Rel("Used_for"), Task("human language learning process")), Triple(Generic("evaluation techniques"), Rel("Used_for"), Task("translation process")), Triple(Generic("evaluation techniques"), Rel("Used_for"), Task("machine translation systems")), Triple(Task("human language learning process"), Rel("Conjunction"), Task("translation process")), Triple(Task("translation process"), Rel("Conjunction"), Task("machine translation systems"))])
pred: [['evaluation techniques', 'Used_for', 'human language learning process'], ['evaluation techniques', 'Used_for', 'translation process'], ['evaluation techniques', 'Used_for', 'machine translation systems'], ['human language learning process', 'Conjunction', 'translation process'], ['translation process', 'Conjunction', 'machine translation systems']]
trues: [['evaluation techniques', 'Used_for', 'human language learning process'], ['evaluation techniques', 'Used_for', 'translation process'], ['evaluation techniques', 'Used_for', 'machine translation systems'], ['human language learning process', 'Conjunction', 'translation process'], ['translation process', 'Conjunction', 'machine translation systems']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.33, 0.35106382978723405, 0.3402061855670104]
text: Contrary to most of the existing mixture of viewpoints models , our model establishes explicit correspondences of parts across different viewpoints of the object class .
result: extract = Extract([Triple(Generic("model"), Rel("Hyponym_of"), Method("mixture of viewpoints models"))])
pred: [['model', 'Hyponym_of', 'mixture of viewpoints models']]
trues: [['model', 'Compare', 'mixture of viewpoints models']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32673267326732675, 0.3473684210526316, 0.33673469387755106]
text: In this paper , we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games .
result: extract = Extract([Triple(Generic("approach"), Rel("Used_for"), Other_scientific_term("pixel-accurate semantic label maps")), Triple(Material("images"), Rel("Used_for"), Other_scientific_term("pixel-accurate semantic label maps")), Triple(Material("computer games"), Rel("Feature_of"), Material("images"))])
pred: [['approach', 'Used_for', 'pixel-accurate semantic label maps'], ['images', 'Used_for', 'pixel-accurate semantic label maps'], ['computer games', 'Feature_of', 'images']]
trues: [['approach', 'Used_for', 'pixel-accurate semantic label maps'], ['pixel-accurate semantic label maps', 'Used_for', 'images'], ['images', 'Part_of', 'modern computer games']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.3269230769230769, 0.3469387755102041, 0.3366336633663366]
text: The unique properties of tree-adjoining grammars ( TAG ) present a challenge for the application of TAGs beyond the limited confines of syntax , for instance , to the task of semantic interpretation or automatic translation of natural language .
result: extract = Extract([Triple(Method("TAGs"), Rel("Used_for"), Task("semantic interpretation")), Triple(Method("TAGs"), Rel("Used_for"), Task("automatic translation of natural language")), Triple(Task("semantic interpretation"), Rel("Conjunction"), Task("automatic translation of natural language"))])
pred: [['TAGs', 'Used_for', 'semantic interpretation'], ['TAGs', 'Used_for', 'automatic translation of natural language'], ['semantic interpretation', 'Conjunction', 'automatic translation of natural language']]
trues: [['TAGs', 'Used_for', 'semantic interpretation'], ['TAGs', 'Used_for', 'automatic translation of natural language'], ['semantic interpretation', 'Conjunction', 'automatic translation of natural language']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.34579439252336447, 0.36633663366336633, 0.3557692307692307]
text: From this point of view , selecting suitable positive ( i.e. intra-class ) training samples within a local range is critical for training the CNN embedding , especially when the data has large intra-class variations .
result: extract = Extract([Triple(Other_scientific_term("positive ( i.e. intra-class ) training samples"), Rel("Used_for"), Method("CNN embedding")), Triple(Other_scientific_term("local range"), Rel("Feature_of"), Other_scientific_term("positive ( i.e. intra-class ) training samples")), Triple(Other_scientific_term("intra-class variations"), Rel("Feature_of"), Material("data"))])
pred: [['positive ( i.e. intra-class ) training samples', 'Used_for', 'CNN embedding'], ['local range', 'Feature_of', 'positive ( i.e. intra-class ) training samples'], ['intra-class variations', 'Feature_of', 'data']]
trues: [['intra-class variations', 'Feature_of', 'data']]
metrics_sample: [0.3333333333333333, 1.0, 0.5]
metrics_current: [0.34545454545454546, 0.37254901960784315, 0.3584905660377358]
text: Towards this goal , we also propose a novel Multiple Instance Learning ( MIL ) formulation for the problem based on the logistic softmax function of covariance-based region features within a MAP estimation framework .
result: extract = Extract([Triple(Method("Multiple Instance Learning ( MIL ) formulation"), Rel("Used_for"), Generic("problem")), Triple(Other_scientific_term("logistic softmax function"), Rel("Used_for"), Method("Multiple Instance Learning ( MIL ) formulation")), Triple(Other_scientific_term("covariance-based region features"), Rel("Used_for"), Other_scientific_term("logistic softmax function"))])
pred: [['Multiple Instance Learning ( MIL ) formulation', 'Used_for', 'problem'], ['logistic softmax function', 'Used_for', 'Multiple Instance Learning ( MIL ) formulation'], ['covariance-based region features', 'Used_for', 'logistic softmax function']]
trues: [['logistic softmax function of covariance-based region features', 'Used_for', 'Multiple Instance Learning ( MIL ) formulation'], ['MAP estimation framework', 'Used_for', 'Multiple Instance Learning ( MIL ) formulation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.336283185840708, 0.36538461538461536, 0.35023041474654376]
text: We rigorously establish the correctness of the methodology and show by experimentation using LSH that it is competitive in practice with available alternatives .
result: extract = Extract([Triple(Generic("it"), Rel("Compare"), Generic("alternatives"))])
pred: [['it', 'Compare', 'alternatives']]
trues: [['it', 'Compare', 'alternatives']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.34210526315789475, 0.37142857142857144, 0.35616438356164387]
text: Recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus , so we used visual inspection and automatic speech recognition ( ASR ) to investigate an offset adaptation ( OA ) model proposed by Zhang et al. [ 1 ] .
result: extract = Extract([Triple(Method("pool models of the inner hair cell synapse"), Rel("Used_for"), Other_scientific_term("dead time period")), Triple(Method("visual inspection"), Rel("Conjunction"), Method("automatic speech recognition ( ASR )")), Triple(Method("automatic speech recognition ( ASR )"), Rel("Used_for"), Other_scientific_term("offset adaptation ( OA ) model"))])
pred: [['pool models of the inner hair cell synapse', 'Used_for', 'dead time period'], ['visual inspection', 'Conjunction', 'automatic speech recognition ( ASR )'], ['automatic speech recognition ( ASR )', 'Used_for', 'offset adaptation ( OA ) model']]
trues: [['pool models', 'Used_for', 'inner hair cell synapse'], ['visual inspection', 'Conjunction', 'automatic speech recognition ( ASR )'], ['visual inspection', 'Used_for', 'offset adaptation ( OA ) model'], ['automatic speech recognition ( ASR )', 'Used_for', 'offset adaptation ( OA ) model']]
metrics_sample: [0.6666666666666666, 0.5, 0.5714285714285715]
metrics_current: [0.3504273504273504, 0.3761467889908257, 0.3628318584070796]
text: For non-LR grammars the time-complexity of our parser is cubic if the functions that constitute the parser are implemented as memo-functions , i.e. functions that memorize the results of previous invocations .
result: extract = Extract([Triple(Other_scientific_term("memo-functions"), Rel("Used_for"), Method("parser"))])
pred: [['memo-functions', 'Used_for', 'parser']]
trues: [['time-complexity', 'Evaluate_for', 'parser'], ['parser', 'Used_for', 'non-LR grammars'], ['memo-functions', 'Used_for', 'parser']]
metrics_sample: [1.0, 0.3333333333333333, 0.5]
metrics_current: [0.3559322033898305, 0.375, 0.3652173913043479]
text: Computing power per area and power consumption is amongst the highest reported for a single chip .
result: extract = Extract([Triple(Metric("Computing power"), Rel("Conjunction"), Metric("area")), Triple(Metric("Computing power"), Rel("Feature_of"), Other_scientific_term("single chip")), Triple(Metric("area"), Rel("Feature_of"), Other_scientific_term("single chip")), Triple(Metric("power consumption"), Rel("Feature_of"), Other_scientific_term("single chip"))])
pred: [['Computing power', 'Conjunction', 'area'], ['Computing power', 'Feature_of', 'single chip'], ['area', 'Feature_of', 'single chip'], ['power consumption', 'Feature_of', 'single chip']]
trues: [['Computing power per area', 'Conjunction', 'power consumption']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3442622950819672, 0.37168141592920356, 0.35744680851063826]
text: We show promising results in both the detection and viewpoint classification tasks on these two challenging datasets .
result: extract = Extract([Triple(Generic("datasets"), Rel("Evaluate_for"), Task("detection and viewpoint classification tasks"))])
pred: [['datasets', 'Evaluate_for', 'detection and viewpoint classification tasks']]
trues: [['datasets', 'Evaluate_for', 'detection and viewpoint classification tasks']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.34959349593495936, 0.37719298245614036, 0.3628691983122363]
text: In both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .
result: extract = Extract([Triple(Method("unsupervised methods"), Rel("Compare"), Method("supervised methods")), Triple(Other_scientific_term("50 labeled examples"), Rel("Used_for"), Method("supervised methods")), Triple(Method("semi-supervised methods"), Rel("Used_for"), Other_scientific_term("labeled data"))])
pred: [['unsupervised methods', 'Compare', 'supervised methods'], ['50 labeled examples', 'Used_for', 'supervised methods'], ['semi-supervised methods', 'Used_for', 'labeled data']]
trues: [['unsupervised methods', 'Compare', 'supervised methods'], ['accuracies', 'Evaluate_for', 'unsupervised methods'], ['accuracies', 'Evaluate_for', 'supervised methods'], ['unlabeled examples', 'Used_for', 'unsupervised methods'], ['labeled examples', 'Used_for', 'supervised methods'], ['labeled data', 'Used_for', 'semi-supervised methods']]
metrics_sample: [0.3333333333333333, 0.16666666666666666, 0.2222222222222222]
metrics_current: [0.3492063492063492, 0.36666666666666664, 0.3577235772357723]
text: While Plume is well adapted to simple declarative and imperative utterances , it handles passives , relative clauses and interrogatives in an ad hoc manner leading to patchy syntactic coverage .
result: extract = Extract([Triple(Other_scientific_term("passives"), Rel("Conjunction"), Other_scientific_term("relative clauses")), Triple(Other_scientific_term("relative clauses"), Rel("Conjunction"), Other_scientific_term("interrogatives")), Triple(Other_scientific_term("patchy syntactic coverage"), Rel("Feature_of"), Other_scientific_term("passives")), Triple(Other_scientific_term("patchy syntactic coverage"), Rel("Feature_of"), Other_scientific_term("relative clauses")), Triple(Other_scientific_term("patchy syntactic coverage"), Rel("Feature_of"), Other_scientific_term("interrogatives"))])
pred: [['passives', 'Conjunction', 'relative clauses'], ['relative clauses', 'Conjunction', 'interrogatives'], ['patchy syntactic coverage', 'Feature_of', 'passives'], ['patchy syntactic coverage', 'Feature_of', 'relative clauses'], ['patchy syntactic coverage', 'Feature_of', 'interrogatives']]
trues: [['Plume', 'Used_for', 'declarative and imperative utterances'], ['it', 'Used_for', 'passives'], ['it', 'Used_for', 'relative clauses'], ['it', 'Used_for', 'interrogatives'], ['passives', 'Conjunction', 'relative clauses'], ['relative clauses', 'Conjunction', 'interrogatives']]
metrics_sample: [0.4, 0.3333333333333333, 0.3636363636363636]
metrics_current: [0.3511450381679389, 0.36507936507936506, 0.35797665369649806]
text: We report experiments conducted on a multilingual corpus to estimate the number of analogies among the sentences that it contains .
result: extract = Extract([Triple(Material("multilingual corpus"), Rel("Evaluate_for"), Other_scientific_term("analogies"))])
pred: [['multilingual corpus', 'Evaluate_for', 'analogies']]
trues: [['multilingual corpus', 'Evaluate_for', 'analogies']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3560606060606061, 0.3700787401574803, 0.36293436293436293]
text: This phenomenon causes many image processing techniques to fail as they assume the presence of only one layer at each examined site e.g. motion estimation and object recognition .
result: extract = Extract([Triple(Generic("phenomenon"), Rel("Used_for"), Method("image processing techniques")), Triple(Other_scientific_term("one layer"), Rel("Used_for"), Generic("they")), Triple(Other_scientific_term("one layer"), Rel("Feature_of"), Other_scientific_term("examined site")), Triple(Task("motion estimation"), Rel("Conjunction"), Task("object recognition"))])
pred: [['phenomenon', 'Used_for', 'image processing techniques'], ['one layer', 'Used_for', 'they'], ['one layer', 'Feature_of', 'examined site'], ['motion estimation', 'Conjunction', 'object recognition']]
trues: [['motion estimation', 'Conjunction', 'object recognition']]
metrics_sample: [0.25, 1.0, 0.4]
metrics_current: [0.35294117647058826, 0.375, 0.3636363636363636]
text: This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction .
result: extract = Extract([Triple(Method("convolution kernel"), Rel("Used_for"), Task("syntactic structure information")), Triple(Method("convolution kernel"), Rel("Used_for"), Task("relation extraction")), Triple(Other_scientific_term("parse trees"), Rel("Used_for"), Method("convolution kernel"))])
pred: [['convolution kernel', 'Used_for', 'syntactic structure information'], ['convolution kernel', 'Used_for', 'relation extraction'], ['parse trees', 'Used_for', 'convolution kernel']]
trues: [['convolution kernel over parse trees', 'Used_for', 'syntactic structure information'], ['syntactic structure information', 'Used_for', 'relation extraction']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.34532374100719426, 0.36923076923076925, 0.35687732342007433]
text: The approach is validated with real image data , and with experimental results from a CID/DRAM analog array prototype in 0.5 cents m CMOS .
result: extract = Extract([Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("real image data"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("
pred: [['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach'], ['real image data', 'Evaluate_for', 'approach']]
trues: [['real image data', 'Evaluate_for', 'approach']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35, 0.37404580152671757, 0.36162361623616235]
text: A general procedure for the estimation and quantization of these cycloidal motion parameters for arbitrary handwriting is presented .
result: extract = Extract([Triple(Generic("procedure"), Rel("Used_for"), Task("estimation and quantization of these cycloidal motion parameters")), Triple(Task("estimation and quantization of these cycloidal motion parameters"), Rel("Used_for"), Other_scientific_term("handwriting"))])
pred: [['procedure', 'Used_for', 'estimation and quantization of these cycloidal motion parameters'], ['estimation and quantization of these cycloidal motion parameters', 'Used_for', 'handwriting']]
trues: [['cycloidal motion parameters', 'Used_for', 'arbitrary handwriting']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.34507042253521125, 0.3712121212121212, 0.35766423357664234]
text: Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of Chinese text available in corpora and on the Web .
result: extract = Extract([Triple(Other_scientific_term("word senses"), Rel("Used_for"), Generic("method")), Triple(Material("Chinese text"), Rel("Used_for"), Generic("method")), Triple(Material("Chinese text"), Rel("Part_of"), Material("corpora")), Triple(Material("Chinese text"), Rel("Part_of"), Material("Web"))])
pred: [['word senses', 'Used_for', 'method'], ['Chinese text', 'Used_for', 'method'], ['Chinese text', 'Part_of', 'corpora'], ['Chinese text', 'Part_of', 'Web']]
trues: [['Chinese text', 'Part_of', 'corpora'], ['Chinese text', 'Part_of', 'Web'], ['corpora', 'Conjunction', 'Web']]
metrics_sample: [0.5, 0.6666666666666666, 0.5714285714285715]
metrics_current: [0.3493150684931507, 0.37777777777777777, 0.36298932384341637]
text: We conclude that previous approaches have neglected to evaluate systems in the context of their use , e.g. solving a task requiring data retrieval .
result: extract = Extract([Triple(Generic("approaches"), Rel("Used_for"), Task("systems")), Triple(Task("data retrieval"), Rel("Used_for"), Generic("task"))])
pred: [['approaches', 'Used_for', 'systems'], ['data retrieval', 'Used_for', 'task']]
trues: [['approaches', 'Evaluate_for', 'systems'], ['systems', 'Used_for', 'task'], ['data retrieval', 'Part_of', 'task']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.34459459459459457, 0.3695652173913043, 0.3566433566433566]
text: Owing to these variations , the pedestrian data is distributed as highly-curved manifolds in the feature space , despite the current convolutional neural networks ( CNN ) 's capability of feature extraction .
result: extract = Extract([Triple(Other_scientific_term("highly-curved manifolds"), Rel("Feature_of"), Material("pedestrian data")), Triple(Other_scientific_term("highly-curved manifolds"), Rel("Compare"), Method("convolutional neural networks ( CNN )")), Triple(Method("convolutional neural networks ( CNN )"), Rel("Used_for"), Task("feature extraction"))])
pred: [['highly-curved manifolds', 'Feature_of', 'pedestrian data'], ['highly-curved manifolds', 'Compare', 'convolutional neural networks ( CNN )'], ['convolutional neural networks ( CNN )', 'Used_for', 'feature extraction']]
trues: [['highly-curved manifolds', 'Used_for', 'pedestrian data'], ['feature space', 'Feature_of', 'highly-curved manifolds'], ['convolutional neural networks ( CNN )', 'Used_for', 'feature extraction']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.3443708609271523, 0.36879432624113473, 0.35616438356164376]
text: We evaluate several proposals for constraining them , basing our assessment on computational tractability and explanatory adequacy .
result: extract = Extract([Triple(Generic("proposals"), Rel("Used_for"), Generic("them")), Triple(Metric("computational tractability"), Rel("Evaluate_for"), Generic("proposals")), Triple(Metric("computational tractability"), Rel("Conjunction"), Metric("explanatory adequacy")), Triple(Metric("explanatory adequacy"), Rel("Evaluate_for"), Generic("proposals"))])
pred: [['proposals', 'Used_for', 'them'], ['computational tractability', 'Evaluate_for', 'proposals'], ['computational tractability', 'Conjunction', 'explanatory adequacy'], ['explanatory adequacy', 'Evaluate_for', 'proposals']]
trues: [['computational tractability and explanatory adequacy', 'Evaluate_for', 'them']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.33548387096774196, 0.36619718309859156, 0.3501683501683502]
text: The new criterion -- meaning-entailing substitutability -- fits the needs of semantic-oriented NLP applications and can be evaluated directly ( independent of an application ) at a good level of human agreement .
result: extract = Extract([Triple(Other_scientific_term("meaning-entailing substitutability"), Rel("Used_for"), Task("semantic-oriented NLP applications")), Triple(Metric("level of human agreement"), Rel("Evaluate_for"), Other_scientific_term("meaning-entailing substitutability"))])
pred: [['meaning-entailing substitutability', 'Used_for', 'semantic-oriented NLP applications'], ['level of human agreement', 'Evaluate_for', 'meaning-entailing substitutability']]
trues: [['meaning-entailing substitutability', 'Used_for', 'semantic-oriented NLP applications'], ['human agreement', 'Evaluate_for', 'meaning-entailing substitutability']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3375796178343949, 0.3680555555555556, 0.35215946843853824]
text: Towards the modeling of the dynamic characteristics , optical flow is computed and utilized as a feature in a higher dimensional space .
result: extract = Extract([Triple(Other_scientific_term("optical flow"), Rel("Used_for"), Other_scientific_term("dynamic characteristics")), Triple(Other_scientific_term("optical flow"), Rel("Used_for"), Other_scientific_term("feature")), Triple(Other_scientific_term("higher dimensional space"), Rel("Feature_of"), Other_scientific_term("feature"))])
pred: [['optical flow', 'Used_for', 'dynamic characteristics'], ['optical flow', 'Used_for', 'feature'], ['higher dimensional space', 'Feature_of', 'feature']]
trues: [['optical flow', 'Used_for', 'modeling of the dynamic characteristics'], ['optical flow', 'Used_for', 'feature'], ['feature', 'Used_for', 'modeling of the dynamic characteristics'], ['higher dimensional space', 'Feature_of', 'feature']]
metrics_sample: [0.6666666666666666, 0.5, 0.5714285714285715]
metrics_current: [0.34375, 0.3716216216216216, 0.35714285714285715]
text: Therefore , the study presented in this paper may be useful in inspiring new designs of deep models for person re-identification .
result: extract = Extract([Triple(Generic("paper"), Rel("Used_for"), Task("deep models")), Triple(Task("deep models"), Rel("Used_for"), Task("person re-identification"))])
pred: [['paper', 'Used_for', 'deep models'], ['deep models', 'Used_for', 'person re-identification']]
trues: [['deep models', 'Used_for', 'person re-identification']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.345679012345679, 0.37583892617449666, 0.360128617363344]
text: It is demonstrated qualitatively and quantitatively on a dataset of scenes and is suitable for current work in unsupervised object discovery without top-down knowledge .
result: extract = Extract([Triple(Generic("It"), Rel("Used_for"), Task("unsupervised object discovery"))])
pred: [['It', 'Used_for', 'unsupervised object discovery']]
trues: [['It', 'Used_for', 'unsupervised object discovery'], ['dataset of scenes', 'Evaluate_for', 'It']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3496932515337423, 0.37748344370860926, 0.3630573248407643]
text: In this model , on-line handwriting is considered as a modulation of a simple cycloidal pen motion , described by two coupled oscillations with a constant linear drift along the line of the writing .
result: extract = Extract([Triple(Other_scientific_term("cycloidal pen motion"), Rel("Used_for"), Other_scientific_term("on-line handwriting"))])
pred: [['cycloidal pen motion', 'Used_for', 'on-line handwriting']]
trues: [['model', 'Used_for', 'on-line handwriting'], ['on-line handwriting', 'Part_of', 'cycloidal pen motion']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3475609756097561, 0.37254901960784315, 0.359621451104101]
text: This paper presents the results of automatically inducing a Combinatory Categorial Grammar ( CCG ) lexicon from a Turkish dependency treebank .
result: extract = Extract([Triple(Material("Turkish dependency treebank"), Rel("Used_for"), Method("Combinatory Categorial Grammar ( CCG ) lexicon"))])
pred: [['Turkish dependency treebank', 'Used_for', 'Combinatory Categorial Grammar ( CCG ) lexicon']]
trues: [['Combinatory Categorial Grammar ( CCG ) lexicon', 'Part_of', 'Turkish dependency treebank']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.34545454545454546, 0.37012987012987014, 0.35736677115987464]
text: We show how features that are imaged through a transparent object behave differently from those that are rigidly attached to the scene .
result: extract = Extract([Triple(Other_scientific_term("transparent object"), Rel("Used_for"), Other_scientific_term("features"))])
pred: [['transparent object', 'Used_for', 'features']]
trues: [['those', 'Compare', 'features']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3433734939759036, 0.36774193548387096, 0.35514018691588783]
text: This paper presents an unsupervised learning approach to disambiguate various relations between named entities by use of various lexical and syntactic features from the contexts .
result: extract = Extract([Triple(Method("unsupervised learning approach"), Rel("Used_for"), Other_scientific_term("relations")), Triple(Other_scientific_term("named entities"), Rel("Feature_of"), Other_scientific_term("relations")), Triple(Other_scientific_term("lexical and syntactic features"), Rel("Used_for"), Method("unsupervised learning approach"))])
pred: [['unsupervised learning approach', 'Used_for', 'relations'], ['named entities', 'Feature_of', 'relations'], ['lexical and syntactic features', 'Used_for', 'unsupervised learning approach']]
trues: [['unsupervised learning approach', 'Used_for', 'relations between named entities'], ['lexical and syntactic features', 'Used_for', 'unsupervised learning approach']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.3431952662721893, 0.36942675159235666, 0.3558282208588957]
text: We explored possible ways to obtain a compact lexicon , consistent with CCG principles , from a treebank which is an order of magnitude smaller than Penn WSJ .
result: extract = Extract([Triple(Material("treebank"), Rel("Used_for"), Other_scientific_term("compact lexicon")), Triple(Material("treebank"), Rel("Compare"), Material("Penn WSJ"))])
pred: [['treebank', 'Used_for', 'compact lexicon'], ['treebank', 'Compare', 'Penn WSJ']]
trues: [['compact lexicon', 'Part_of', 'treebank'], ['treebank', 'Compare', 'Penn WSJ']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.34502923976608185, 0.3710691823899371, 0.35757575757575755]
text: This posterior encodes sparse se-lectional preferences between a head word and its dependents .
result: extract = Extract([Triple(Other_scientific_term("sparse se-lectional preferences"), Rel("Feature_of"), Other_scientific_term("posterior"))])
pred: [['sparse se-lectional preferences', 'Feature_of', 'posterior']]
trues: [['posterior', 'Used_for', 'sparse se-lectional preferences']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3430232558139535, 0.36875, 0.35542168674698793]
text: In this paper , we describe a search procedure for statistical machine translation ( MT ) based on dynamic programming ( DP ) .
result: extract = Extract([Triple(Method("search procedure"), Rel("Used_for"), Task("statistical machine translation ( MT )")), Triple(Method("dynamic programming ( DP )"), Rel("Used_for"), Method("search procedure"))])
pred: [['search procedure', 'Used_for', 'statistical machine translation ( MT )'], ['dynamic programming ( DP )', 'Used_for', 'search procedure']]
trues: [['search procedure', 'Used_for', 'statistical machine translation ( MT )'], ['dynamic programming ( DP )', 'Used_for', 'statistical machine translation ( MT )']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3448275862068966, 0.37037037037037035, 0.35714285714285715]
text: This paper describes FERRET , an interactive question-answering ( Q/A ) system designed to address the challenges of integrating automatic Q/A applications into real-world environments .
result: extract = Extract([Triple(Method("FERRET"), Rel("Hyponym_of"), Method("interactive question-answering ( Q/A ) system")), Triple(Method("interactive question-answering ( Q/A ) system"), Rel("Used_for"), Task("integrating automatic Q/A applications"))])
pred: [['FERRET', 'Hyponym_of', 'interactive question-answering ( Q/A ) system'], ['interactive question-answering ( Q/A ) system', 'Used_for', 'integrating automatic Q/A applications']]
trues: [['FERRET', 'Hyponym_of', 'interactive question-answering ( Q/A ) system'], ['FERRET', 'Used_for', 'integrating automatic Q/A applications into real-world environments']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3465909090909091, 0.3719512195121951, 0.35882352941176476]
text: In this paper , we explore geometric structures of 3D lines in ray space for improving light field triangulation and stereo matching .
result: extract = Extract([Triple(Other_scientific_term("geometric structures of 3D lines"), Rel("Used_for"), Method("light field triangulation")), Triple(Other_scientific_term("geometric structures of 3D lines"), Rel("Used_for"), Method("stereo matching")), Triple(Other_scientific_term("ray space"), Rel("Feature_of"), Other_scientific_term("geometric structures of 3D lines")), Triple(Method("light field triangulation"), Rel("Conjunction"), Method("stereo matching"))])
pred: [['geometric structures of 3D lines', 'Used_for', 'light field triangulation'], ['geometric structures of 3D lines', 'Used_for', 'stereo matching'], ['ray space', 'Feature_of', 'geometric structures of 3D lines'], ['light field triangulation', 'Conjunction', 'stereo matching']]
trues: [['geometric structures of 3D lines', 'Used_for', 'light field triangulation'], ['geometric structures of 3D lines', 'Used_for', 'stereo matching'], ['ray space', 'Feature_of', 'geometric structures of 3D lines'], ['light field triangulation', 'Conjunction', 'stereo matching']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3611111111111111, 0.3869047619047619, 0.3735632183908046]
text: While current approaches are developed by only considering the low rank plus sparse structure , in many applications , side information of row and/or column entities may also be given , and it is still unclear to what extent could such information help robust PCA .
result: extract = Extract([Triple(Other_scientific_term("side information of row and/or column entities"), Rel("Used_for"), Method("robust PCA"))])
pred: [['side information of row and/or column entities', 'Used_for', 'robust PCA']]
trues: [['low rank plus sparse structure', 'Used_for', 'approaches'], ['information', 'Used_for', 'robust PCA']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35911602209944754, 0.38235294117647056, 0.3703703703703704]
text: We then turn to a discussion comparing the linguistic expressiveness of the two formalisms .
result: extract = Extract([Triple(Method("linguistic expressiveness"), Rel("Evaluate_for"), Generic("formalisms"))])
pred: [['linguistic expressiveness', 'Evaluate_for', 'formalisms']]
trues: [['linguistic expressiveness', 'Feature_of', 'formalisms']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35714285714285715, 0.38011695906432746, 0.36827195467422097]
text: This paper describes a novel system for acquiring adjectival subcategorization frames ( scfs ) and associated frequency information from English corpus data .
result: extract = Extract([Triple(Generic("system"), Rel("Used_for"), Other_scientific_term("adjectival subcategorization frames ( scfs )")), Triple(Material("English corpus data"), Rel("Used_for"), Generic("system"))])
pred: [['system', 'Used_for', 'adjectival subcategorization frames ( scfs )'], ['English corpus data', 'Used_for', 'system']]
trues: [['system', 'Used_for', 'acquiring adjectival subcategorization frames ( scfs )']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3532608695652174, 0.37790697674418605, 0.3651685393258427]
text: To recover from these inevitable tracking failures , we consider object detection as a global optimization problem and solve it via Adaptive Simulated Annealing ( ASA ) , a method that avoids becoming trapped at local modes and is much faster than exhaustive search .
result: extract = Extract([Triple(Method("Adaptive Simulated Annealing ( ASA )"), Rel("Used_for"), Task("object detection")), Triple(Method("Adaptive Simulated Annealing ( ASA )"), Rel("Compare"), Method("exhaustive search"))])
pred: [['Adaptive Simulated Annealing ( ASA )', 'Used_for', 'object detection'], ['Adaptive Simulated Annealing ( ASA )', 'Compare', 'exhaustive search']]
trues: [['global optimization problem', 'Used_for', 'object detection'], ['Adaptive Simulated Annealing ( ASA )', 'Used_for', 'it'], ['method', 'Compare', 'exhaustive search']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.34946236559139787, 0.37142857142857144, 0.3601108033240997]
text: Simulated and experimental results show that our method recovers scene geometry with high subpixel precision , and that it can handle many challenges of active reconstruction systems .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Task("scene geometry")), Triple(Metric("subpixel precision"), Rel("Evaluate_for"), Generic("method")), Triple(Generic("it"), Rel("Used_for"), Method("active reconstruction systems"))])
pred: [['method', 'Used_for', 'scene geometry'], ['subpixel precision', 'Evaluate_for', 'method'], ['it', 'Used_for', 'active reconstruction systems']]
trues: [['method', 'Used_for', 'scene geometry'], ['subpixel precision', 'Feature_of', 'scene geometry'], ['it', 'Used_for', 'active reconstruction systems']]
metrics_sample: [0.6666666666666666, 0.6666666666666666, 0.6666666666666666]
metrics_current: [0.3544973544973545, 0.37640449438202245, 0.36512261580381467]
text: The experiments show that the system is able to detect scf types with 70 % precision and 66 % recall rate .
result: extract = Extract([Triple(Metric("precision"), Rel("Conjunction"), Metric("recall rate"))])
pred: [['precision', 'Conjunction', 'recall rate']]
trues: [['precision', 'Evaluate_for', 'system'], ['precision', 'Conjunction', 'recall'], ['recall', 'Evaluate_for', 'system']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3526315789473684, 0.3701657458563536, 0.36118598382749323]
text: An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings .
result: extract = Extract([Triple(Metric("summarization quality"), Rel("Evaluate_for"), Method("parse-based evaluation")), Triple(Metric("summarization quality"), Rel("Evaluate_for"), Other_scientific_term("generated strings"))])
pred: [['summarization quality', 'Evaluate_for', 'parse-based evaluation'], ['summarization quality', 'Evaluate_for', 'generated strings']]
trues: [['summarization quality', 'Evaluate_for', 'automatic parse-based evaluation'], ['automatic parse-based evaluation', 'Compare', 'manual evaluation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3489583333333333, 0.366120218579235, 0.35733333333333334]
text: Experiments on the PASCAL VOC 2006 dataset show an acceleration of more than 50 % by our method compared to standard cascade evaluation .
result: extract = Extract([Triple(Material("PASCAL VOC 2006 dataset"), Rel("Evaluate_for"), Generic("method")), Triple(Material("PASCAL VOC 2006 dataset"), Rel("Evaluate_for"), Method("cascade evaluation"))])
pred: [['PASCAL VOC 2006 dataset', 'Evaluate_for', 'method'], ['PASCAL VOC 2006 dataset', 'Evaluate_for', 'cascade evaluation']]
trues: [['PASCAL VOC 2006 dataset', 'Evaluate_for', 'method'], ['PASCAL VOC 2006 dataset', 'Evaluate_for', 'cascade evaluation'], ['cascade evaluation', 'Compare', 'method']]
metrics_sample: [1.0, 0.6666666666666666, 0.8]
metrics_current: [0.3556701030927835, 0.3709677419354839, 0.3631578947368421]
text: These methods diier from previous work in the area in two ways : the use of an action model formalism which is better suited to the needs of a re-active agent , and successful implementation of noise-handling mechanisms .
result: extract = Extract([Triple(Method("action model formalism"), Rel("Used_for"), Generic("methods")), Triple(Method("action model formalism"), Rel("Used_for"), Method("re-active agent")), Triple(Generic("methods"), Rel("Compare"), Generic("work")), Triple(Generic("work"), Rel("Used_for"), Other_scientific_term("noise-handling mechanisms"))])
pred: [['action model formalism', 'Used_for', 'methods'], ['action model formalism', 'Used_for', 're-active agent'], ['methods', 'Compare', 'work'], ['work', 'Used_for', 'noise-handling mechanisms']]
trues: [['action model formalism', 'Used_for', 'methods'], ['action model formalism', 'Used_for', 're-active agent'], ['noise-handling mechanisms', 'Used_for', 'methods']]
metrics_sample: [0.5, 0.6666666666666666, 0.5714285714285715]
metrics_current: [0.35858585858585856, 0.37566137566137564, 0.3669250645994832]
text: It would also be effective as a move selector and move sorter for game tree search and as a training tool for Go players .
result: extract = Extract([Triple(Generic("It"), Rel("Used_for"), Method("Go players")), Triple(Generic("It"), Rel("Used_for"), Method("game tree search")), Triple(Generic("It"), Rel("Used_for"), Method("move selector")), Triple(Generic("It"), Rel("Used_for"), Method("move sorter")), Triple(Method("Go players"), Rel("Conjunction"), Method("game tree search")), Triple(Method("game tree search"), Rel("Used_for"), Method("move selector")), Triple(Method("game tree search"), Rel("Used_for"), Method("move sorter")), Triple(Method("move selector"), Rel("Conjunction"), Method("move sorter"))])
pred: [['It', 'Used_for', 'Go players'], ['It', 'Used_for', 'game tree search'], ['It', 'Used_for', 'move selector'], ['It', 'Used_for', 'move sorter'], ['Go players', 'Conjunction', 'game tree search'], ['game tree search', 'Used_for', 'move selector'], ['game tree search', 'Used_for', 'move sorter'], ['move selector', 'Conjunction', 'move sorter']]
trues: [['It', 'Used_for', 'move selector'], ['It', 'Used_for', 'move sorter'], ['It', 'Used_for', 'training tool'], ['move selector', 'Conjunction', 'move sorter'], ['move selector', 'Used_for', 'game tree search'], ['move sorter', 'Used_for', 'game tree search'], ['training tool', 'Used_for', 'Go players']]
metrics_sample: [0.375, 0.42857142857142855, 0.39999999999999997]
metrics_current: [0.3592233009708738, 0.37755102040816324, 0.3681592039800995]
text: This model is meant to account for productive rules of interpretation which are inferred from the morpho-syntactic and semantic characteristics of the nominal constituents .
result: extract = Extract([Triple(Other_scientific_term("morpho-syntactic and semantic characteristics"), Rel("Used_for"), Other_scientific_term("productive rules of interpretation"))])
pred: [['morpho-syntactic and semantic characteristics', 'Used_for', 'productive rules of interpretation']]
trues: [['model', 'Used_for', 'productive rules of interpretation'], ['morpho-syntactic and semantic characteristics', 'Used_for', 'productive rules of interpretation'], ['morpho-syntactic and semantic characteristics', 'Feature_of', 'nominal constituents']]
metrics_sample: [1.0, 0.3333333333333333, 0.5]
metrics_current: [0.36231884057971014, 0.3768844221105528, 0.3694581280788177]
text: Full digital resolution is maintained even with low-resolution analog-to-digital conversion , owing to random statistics in the analog summation of binary products .
result: extract = Extract([Triple(Other_scientific_term("random statistics"), Rel("Feature_of"), Other_scientific_term("analog summation of binary products"))])
pred: [['random statistics', 'Feature_of', 'analog summation of binary products']]
trues: [['random statistics', 'Part_of', 'analog summation of binary products']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3605769230769231, 0.375, 0.3676470588235294]
text: Fast algorithms for nearest neighbor ( NN ) search have in large part focused on 2 distance .
result: extract = Extract([Triple(Metric("nearest neighbor ( NN ) search"), Rel("Used_for"), Generic("algorithms"))])
pred: [['nearest neighbor ( NN ) search', 'Used_for', 'algorithms']]
trues: [['Fast algorithms', 'Used_for', 'nearest neighbor ( NN ) search']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3588516746411483, 0.373134328358209, 0.36585365853658536]
text: Background modeling is an important component of many vision systems .
result: extract = Extract([Triple(Method("Background modeling"), Rel("Part_of"), Method("vision systems"))])
pred: [['Background modeling', 'Part_of', 'vision systems']]
trues: [['Background modeling', 'Part_of', 'vision systems']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3619047619047619, 0.37623762376237624, 0.36893203883495146]
text: Our experiments clearly indicate the potential of this dynamic representation for complete cursive handwriting recognition .
result: extract = Extract([Triple(Method("dynamic representation"), Rel("Used_for"), Task("cursive handwriting recognition"))])
pred: [['dynamic representation', 'Used_for', 'cursive handwriting recognition']]
trues: [['dynamic representation', 'Used_for', 'cursive handwriting recognition']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.36492890995260663, 0.3793103448275862, 0.3719806763285024]
text: We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus ( 100,000 words ) and exploiting a large non-parallel corpus .
result: extract = Extract([Triple(Material("parallel corpus"), Rel("Used_for"), Method("MT system")), Triple(Material("non-parallel corpus"), Rel("Used_for"), Method("MT system")), Triple(Material("non-parallel corpus"), Rel("Conjunction"), Material("parallel corpus"))])
pred: [['parallel corpus', 'Used_for', 'MT system'], ['non-parallel corpus', 'Used_for', 'MT system'], ['non-parallel corpus', 'Conjunction', 'parallel corpus']]
trues: [['parallel corpus', 'Used_for', 'MT system'], ['parallel corpus', 'Conjunction', 'non-parallel corpus'], ['non-parallel corpus', 'Used_for', 'MT system']]
metrics_sample: [0.6666666666666666, 0.6666666666666666, 0.6666666666666666]
metrics_current: [0.3691588785046729, 0.38349514563106796, 0.37619047619047613]
text: In this paper We experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgments .
result: extract = Extract([Triple(Method("trainable sentence planner"), Rel("Used_for"), Method("spoken dialogue system")), Triple(Other_scientific_term("human judgments"), Rel("Used_for"), Method("trainable sentence planner"))])
pred: [['trainable sentence planner', 'Used_for', 'spoken dialogue system'], ['human judgments', 'Used_for', 'trainable sentence planner']]
trues: [['trainable sentence planner', 'Used_for', 'spoken dialogue system'], ['subjective human judgments', 'Evaluate_for', 'trainable sentence planner']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.37037037037037035, 0.38461538461538464, 0.3773584905660377]
text: The description has been implemented using the PC-KIMMO environment ( Antworth , 1990 ) and is based on a root word lexicon of about 23,000 roots words .
result: extract = Extract([Triple(Method("PC-KIMMO environment"), Rel("Used_for"), Generic("description")), Triple(Other_scientific_term("root word lexicon"), Rel("Used_for"), Generic("description"))])
pred: [['PC-KIMMO environment', 'Used_for', 'description'], ['root word lexicon', 'Used_for', 'description']]
trues: [['PC-KIMMO environment', 'Used_for', 'description'], ['root word lexicon', 'Used_for', 'description']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3761467889908257, 0.3904761904761905, 0.383177570093458]
text: An important area of learning in autonomous agents is the ability to learn domain-speciic models of actions to be used by planning systems .
result: extract = Extract([Triple(Other_scientific_term("domain-speciic models of actions"), Rel("Used_for"), Method("planning systems"))])
pred: [['domain-speciic models of actions', 'Used_for', 'planning systems']]
trues: [['learning in autonomous agents', 'Used_for', 'domain-speciic models of actions'], ['planning systems', 'Used_for', 'domain-speciic models of actions']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3744292237442922, 0.3867924528301887, 0.38051044083526686]
text: We present a new method for detecting interest points using histogram information .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Task("detecting interest points")), Triple(Other_scientific_term("histogram information"), Rel("Used_for"), Generic("method"))])
pred: [['method', 'Used_for', 'detecting interest points'], ['histogram information', 'Used_for', 'method']]
trues: [['method', 'Used_for', 'detecting interest points'], ['histogram information', 'Used_for', 'detecting interest points']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3755656108597285, 0.3878504672897196, 0.3816091954022988]
text: We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : SLIPPER , a rule-based learning algorithm , and TiMBL , a memory-based system .
result: extract = Extract([Triple(Other_scientific_term("predicates of such clauses"), Rel("Used_for"), Other_scientific_term("domain independent features")), Triple(Other_scientific_term("domain independent features"), Rel("Used_for"), Generic("input dataset")), Triple(Method("SLIPPER"), Rel("Hyponym_of"), Method("machine learning algorithms")), Triple(Method("SLIPPER"), Rel("Conjunction"), Method("TiMBL")), Triple(Method("TiMBL"), Rel("Hyponym_of"), Method("machine learning algorithms"))])
pred: [['predicates of such clauses', 'Used_for', 'domain independent features'], ['domain independent features', 'Used_for', 'input dataset'], ['SLIPPER', 'Hyponym_of', 'machine learning algorithms'], ['SLIPPER', 'Conjunction', 'TiMBL'], ['TiMBL', 'Hyponym_of', 'machine learning algorithms']]
trues: [['SLIPPER', 'Hyponym_of', 'rule-based learning algorithm'], ['rule-based learning algorithm', 'Part_of', 'machine learning algorithms'], ['rule-based learning algorithm', 'Compare', 'memory-based system'], ['TiMBL', 'Hyponym_of', 'memory-based system'], ['memory-based system', 'Part_of', 'machine learning algorithms']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3672566371681416, 0.3789954337899543, 0.37303370786516854]
text: We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars ( LFG ) to the domain of sentence condensation .
result: extract = Extract([Triple(Method("ambiguity packing and stochastic disambiguation techniques"), Rel("Used_for"), Method("Lexical-Functional Grammars ( LFG )")), Triple(Method("Lexical-Functional Grammars ( LFG )"), Rel("Used_for"), Task("sentence condensation"))])
pred: [['ambiguity packing and stochastic disambiguation techniques', 'Used_for', 'Lexical-Functional Grammars ( LFG )'], ['Lexical-Functional Grammars ( LFG )', 'Used_for', 'sentence condensation']]
trues: [['ambiguity packing and stochastic disambiguation techniques', 'Used_for', 'Lexical-Functional Grammars ( LFG )'], ['ambiguity packing and stochastic disambiguation techniques', 'Used_for', 'sentence condensation']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3684210526315789, 0.38009049773755654, 0.37416481069042307]
text: Our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments .
result: extract = Extract([Triple(Generic("technique"), Rel("Compare"), Generic("models")), Triple(Metric("paraphrase classification accuracy"), Rel("Evaluate_for"), Generic("technique")), Triple(Metric("paraphrase classification accuracy"), Rel("Evaluate_for"), Generic("models"))])
pred: [['technique', 'Compare', 'models'], ['paraphrase classification accuracy', 'Evaluate_for', 'technique'], ['paraphrase classification accuracy', 'Evaluate_for', 'models']]
trues: [['technique', 'Compare', 'models'], ['paraphrase classification accuracy', 'Evaluate_for', 'technique'], ['paraphrase classification accuracy', 'Evaluate_for', 'models']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.37662337662337664, 0.38839285714285715, 0.3824175824175824]
text: We further propose a parameter training algorithm for the present model based on a decision tree-based context clustering .
result: extract = Extract([Triple(Method("decision tree-based context clustering"), Rel("Used_for"), Method("parameter training algorithm"))])
pred: [['decision tree-based context clustering', 'Used_for', 'parameter training algorithm']]
trues: [['parameter training algorithm', 'Used_for', 'model'], ['decision tree-based context clustering', 'Used_for', 'parameter training algorithm']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3793103448275862, 0.3893805309734513, 0.3842794759825327]
text: Specifically , by leveraging the merits of the deep Long Short-Term Memory ( LSTM ) subnetwork , the proposed model automatically captures the complex long-range temporal dynamics , which naturally avoids the typical sliding window design and thus ensures high computational efficiency .
result: extract = Extract([Triple(Method("deep Long Short-Term Memory ( LSTM ) subnetwork"), Rel("Used_for"), Generic("model")), Triple(Method("deep Long Short-Term Memory ( LSTM ) subnetwork"), Rel("Used_for"), Other_scientific_term("complex long-range temporal dynamics")), Triple(Other_scientific_term("sliding window design"), Rel("Compare"), Other_scientific_term("complex long-range temporal dynamics"))])
pred: [['deep Long Short-Term Memory ( LSTM ) subnetwork', 'Used_for', 'model'], ['deep Long Short-Term Memory ( LSTM ) subnetwork', 'Used_for', 'complex long-range temporal dynamics'], ['sliding window design', 'Compare', 'complex long-range temporal dynamics']]
trues: [['deep Long Short-Term Memory ( LSTM ) subnetwork', 'Used_for', 'model'], ['long-range temporal dynamics', 'Feature_of', 'model']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.37872340425531914, 0.39035087719298245, 0.38444924406047515]
text: However , acquiring the necessary training data is typically a tedious and time consuming task that involves manual interaction , active sensing devices and/or synthetic scenes .
result: extract = Extract([Triple(Material("manual interaction"), Rel("Conjunction"), Other_scientific_term("active sensing devices")), Triple(Other_scientific_term("active sensing devices"), Rel("Conjunction"), Other_scientific_term("synthetic scenes"))])
pred: [['manual interaction', 'Conjunction', 'active sensing devices'], ['active sensing devices', 'Conjunction', 'synthetic scenes']]
trues: [['manual interaction', 'Used_for', 'task'], ['manual interaction', 'Conjunction', 'active sensing devices'], ['active sensing devices', 'Used_for', 'task'], ['active sensing devices', 'Conjunction', 'synthetic scenes'], ['synthetic scenes', 'Used_for', 'task']]
metrics_sample: [1.0, 0.4, 0.5714285714285715]
metrics_current: [0.38396624472573837, 0.3905579399141631, 0.3872340425531915]
text: In contrast , object pose estimation using these approaches has received relatively less attention .
result: extract = Extract([Triple(Generic("approaches"), Rel("Used_for"), Task("object pose estimation"))])
pred: [['approaches', 'Used_for', 'object pose estimation']]
trues: [['approaches', 'Used_for', 'object pose estimation']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3865546218487395, 0.39316239316239315, 0.3898305084745763]
text: The system is trained on 181,000 expert games and shows excellent prediction performance as indicated by its ability to perfectly predict the moves made by professional Go players in 34 % of test positions .
result: extract = Extract([Triple(Generic("system"), Rel("Used_for"), Task("prediction")), Triple(Material("expert games"), Rel("Used_for"), Generic("system"))])
pred: [['system', 'Used_for', 'prediction'], ['expert games', 'Used_for', 'system']]
trues: [['expert games', 'Used_for', 'system']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3875, 0.39574468085106385, 0.3915789473684211]
text: Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions .
result: extract = Extract([Triple(Method("MBR decoding"), Rel("Used_for"), Method("statistical MT")), Triple(Other_scientific_term("loss functions"), Rel("Feature_of"), Method("statistical MT"))])
pred: [['MBR decoding', 'Used_for', 'statistical MT'], ['loss functions', 'Feature_of', 'statistical MT']]
trues: [['MBR decoding', 'Used_for', 'statistical MT'], ['MBR decoding', 'Used_for', 'loss functions']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3884297520661157, 0.39662447257383965, 0.3924843423799582]
text: We demonstrate that for certain field structured extraction tasks , such as classified advertisements and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion .
result: extract = Extract([Triple(Generic("prior knowledge"), Rel("Used_for"), Task("classified advertisements and bibliographic citations")), Triple(Generic("prior knowledge"), Rel("Used_for"), Method("models")), Triple(Method("models"), Rel("Used_for"), Task("classified advertisements and bibliographic citations")), Triple(Method("unsupervised fashion"), Rel("Feature_of"), Method("models"))])
pred: [['prior knowledge', 'Used_for', 'classified advertisements and bibliographic citations'], ['prior knowledge', 'Used_for', 'models'], ['models', 'Used_for', 'classified advertisements and bibliographic citations'], ['unsupervised fashion', 'Feature_of', 'models']]
trues: [['classified advertisements', 'Hyponym_of', 'field structured extraction tasks'], ['classified advertisements', 'Conjunction', 'bibliographic citations'], ['bibliographic citations', 'Hyponym_of', 'field structured extraction tasks'], ['prior knowledge', 'Used_for', 'field structured extraction tasks']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3821138211382114, 0.3900414937759336, 0.3860369609856263]
text: Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification , and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification .
result: extract = Extract([Triple(Generic("strategies"), Rel("Used_for"), Method("deep metrics")), Triple(Method("deep metrics"), Rel("Used_for"), Task("person re-identification")), Triple(Method("deep model"), Rel("Compare"), Generic("state-of-the-art methods")), Triple(Generic("benchmarks"), Rel("Evaluate_for"), Task("person re-identification"))])
pred: [['strategies', 'Used_for', 'deep metrics'], ['deep metrics', 'Used_for', 'person re-identification'], ['deep model', 'Compare', 'state-of-the-art methods'], ['benchmarks', 'Evaluate_for', 'person re-identification']]
trues: [['robust deep metrics', 'Used_for', 'person re-identification'], ['deep model', 'Compare', 'state-of-the-art methods'], ['deep model', 'Used_for', 'person re-identification'], ['state-of-the-art methods', 'Used_for', 'person re-identification']]
metrics_sample: [0.25, 0.25, 0.25]
metrics_current: [0.38, 0.3877551020408163, 0.3838383838383838]
text: Second , based on the information resulting from the dictionary lookup stage , a set of rules is applied to the segmented strings in order to identify NE items .
result: extract = Extract([Triple(Other_scientific_term("rules"), Rel("Used_for"), Other_scientific_term("NE items")), Triple(Other_scientific_term("segmented strings"), Rel("Used_for"), Other_scientific_term("rules"))])
pred: [['rules', 'Used_for', 'NE items'], ['segmented strings', 'Used_for', 'rules']]
trues: [['dictionary lookup stage', 'Used_for', 'rules'], ['rules', 'Used_for', 'NE items']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.38095238095238093, 0.38866396761133604, 0.3847695390781563]
text: We propose a novel approach to associate objects across multiple PTZ cameras that can be used to perform camera handoff in wide-area surveillance scenarios .
result: extract = Extract([Triple(Generic("approach"), Rel("Used_for"), Task("camera handoff")), Triple(Task("camera handoff"), Rel("Used_for"), Task("wide-area surveillance scenarios"))])
pred: [['approach', 'Used_for', 'camera handoff'], ['camera handoff', 'Used_for', 'wide-area surveillance scenarios']]
trues: [['approach', 'Used_for', 'camera handoff in wide-area surveillance scenarios']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3779527559055118, 0.3870967741935484, 0.38247011952191234]
text: In this paper , we discuss language model adaptation methods given a word list and a raw corpus .
result: extract = Extract([Triple(Material("word list"), Rel("Used_for"), Method("language model adaptation methods")), Triple(Material("word list"), Rel("Conjunction"), Material("raw corpus")), Triple(Material("raw corpus"), Rel("Used_for"), Method("language model adaptation methods"))])
pred: [['word list', 'Used_for', 'language model adaptation methods'], ['word list', 'Conjunction', 'raw corpus'], ['raw corpus', 'Used_for', 'language model adaptation methods']]
trues: [['word list', 'Used_for', 'language model adaptation methods'], ['word list', 'Conjunction', 'raw corpus'], ['raw corpus', 'Used_for', 'language model adaptation methods']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3852140077821012, 0.3944223107569721, 0.38976377952755903]
text: The experimental results show that the proposed histogram-based interest point detectors perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of repeatability and distinctiveness .
result: extract = Extract([Triple(Method("histogram-based interest point detectors"), Rel("Used_for"), Task("matching textured scenes")), Triple(Metric("repeatability"), Rel("Evaluate_for"), Method("histogram-based interest point detectors")), Triple(Metric("repeatability"), Rel("Conjunction"), Metric("distinctiveness")), Triple(Metric("distinctiveness"), Rel("Evaluate_for"), Method("histogram-based interest point detectors"))])
pred: [['histogram-based interest point detectors', 'Used_for', 'matching textured scenes'], ['repeatability', 'Evaluate_for', 'histogram-based interest point detectors'], ['repeatability', 'Conjunction', 'distinctiveness'], ['distinctiveness', 'Evaluate_for', 'histogram-based interest point detectors']]
trues: [['histogram-based interest point detectors', 'Used_for', 'matching textured scenes'], ['repeatability', 'Evaluate_for', 'histogram-based interest point detectors'], ['repeatability', 'Conjunction', 'distinctiveness'], ['distinctiveness', 'Evaluate_for', 'histogram-based interest point detectors']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3946360153256705, 0.403921568627451, 0.39922480620155043]
text: Using the IEMOCAP database , discrete ( categorical ) and continuous ( attribute ) emotional assessments evaluated by the actors and na ¨ ıve listeners are compared .
result: extract = Extract([Triple(Material("IEMOCAP database"), Rel("Evaluate_for"), Other_scientific_term("discrete ( categorical ) and continuous ( attribute ) emotional assessments")), Triple(Other_scientific_term("discrete ( categorical ) and continuous ( attribute ) emotional assessments"), Rel("Evaluate_for"), Other_scientific_term("actors")), Triple(Other_scientific_term("discrete ( categorical ) and continuous ( attribute ) emotional assessments"), Rel("Evaluate_for"), Other_scientific_term("na ¨ ıve listeners")), Triple(Other_scientific_term("actors"), Rel("Conjunction"), Other_scientific_term("na ¨ ıve listeners"))])
pred: [['IEMOCAP database', 'Evaluate_for', 'discrete ( categorical ) and continuous ( attribute ) emotional assessments'], ['discrete ( categorical ) and continuous ( attribute ) emotional assessments', 'Evaluate_for', 'actors'], ['discrete ( categorical ) and continuous ( attribute ) emotional assessments', 'Evaluate_for', 'na ¨ ıve listeners'], ['actors', 'Conjunction', 'na ¨ ıve listeners']]
trues: [['IEMOCAP database', 'Used_for', 'discrete ( categorical ) and continuous ( attribute ) emotional assessments']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3886792452830189, 0.40234375, 0.39539347408829173]
text: Utterance Verification ( UV ) is a critical function of an Automatic Speech Recognition ( ASR ) System working on real applications where spontaneous speech , out-of-vocabulary ( OOV ) words and acoustic noises are present .
result: extract = Extract([Triple(Task("Utterance Verification ( UV )"), Rel("Part_of"), Task("Automatic Speech Recognition ( ASR ) System")), Triple(Task("Automatic Speech Recognition ( ASR ) System"), Rel("Used_for"), Other_scientific_term("spontaneous speech")), Triple(Task("Automatic Speech Recognition ( ASR ) System"), Rel("Used_for"), Other_scientific_term("OOV words")), Triple(Task("Automatic Speech Recognition ( ASR ) System"), Rel("Used_for"), Other_scientific_term("acoustic noises"))])
pred: [['Utterance Verification ( UV )', 'Part_of', 'Automatic Speech Recognition ( ASR ) System'], ['Automatic Speech Recognition ( ASR ) System', 'Used_for', 'spontaneous speech'], ['Automatic Speech Recognition ( ASR ) System', 'Used_for', 'OOV words'], ['Automatic Speech Recognition ( ASR ) System', 'Used_for', 'acoustic noises']]
trues: [['Utterance Verification ( UV )', 'Hyponym_of', 'Automatic Speech Recognition ( ASR ) System']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3828996282527881, 0.40077821011673154, 0.3916349809885932]
text: A generative framework is used for learning a model that captures the relative position of parts within each of the discretized viewpoints .
result: extract = Extract([Triple(Method("generative framework"), Rel("Used_for"), Generic("model")), Triple(Generic("model"), Rel("Used_for"), Other_scientific_term("relative position of parts"))])
pred: [['generative framework', 'Used_for', 'model'], ['model', 'Used_for', 'relative position of parts']]
trues: [['generative framework', 'Used_for', 'model']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3837638376383764, 0.40310077519379844, 0.393194706994329]
text: With the aid of a logic-based grammar formalism called extraposition grammars , Chat-80 translates English questions into the Prolog subset of logic .
result: extract = Extract([Triple(Method("extraposition grammars"), Rel("Used_for"), Method("Chat-80")), Triple(Method("Prolog subset of logic"), Rel("Used_for"), Method("Chat-80"))])
pred: [['extraposition grammars', 'Used_for', 'Chat-80'], ['Prolog subset of logic', 'Used_for', 'Chat-80']]
trues: [['extraposition grammars', 'Hyponym_of', 'logic-based grammar formalism'], ['extraposition grammars', 'Used_for', 'Chat-80']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.38461538461538464, 0.40384615384615385, 0.39399624765478425]
text: This paper presents a phrase-based statistical machine translation method , based on non-contiguous phrases , i.e. phrases with gaps .
result: extract = Extract([Triple(Method("non-contiguous phrases"), Rel("Used_for"), Method("phrase-based statistical machine translation method"))])
pred: [['non-contiguous phrases', 'Used_for', 'phrase-based statistical machine translation method']]
trues: [['non-contiguous phrases', 'Used_for', 'phrase-based statistical machine translation method']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.38686131386861317, 0.4061302681992337, 0.3962616822429907]
text: The results show that the features in terms of which we formulate our heuristic principles have significant predictive power , and that rules that closely resemble our Horn clauses can be learnt automatically from these features .
result: extract = Extract([Triple(Other_scientific_term("features"), Rel("Used_for"), Generic("heuristic principles")), Triple(Other_scientific_term("features"), Rel("Used_for"), Method("rules")), Triple(Method("Horn clauses"), Rel("Hyponym_of"), Method("rules"))])
pred: [['features', 'Used_for', 'heuristic principles'], ['features', 'Used_for', 'rules'], ['Horn clauses', 'Hyponym_of', 'rules']]
trues: [['features', 'Feature_of', 'heuristic principles']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38267148014440433, 0.40458015267175573, 0.39332096474953615]
text: While sentence extraction as an approach to summarization has been shown to work in documents of certain genres , because of the conversational nature of email communication where utterances are made in relation to one made previously , sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent .
result: extract = Extract([Triple(Task("sentence extraction"), Rel("Used_for"), Task("summarization")), Triple(Task("sentence extraction"), Rel("Used_for"), Task("summarization")), Triple(Material("genres"), Rel("Used_for"), Task("sentence extraction")), Triple(Material("genres"), Rel("Conjunction"), Other_scientific_term("conversational nature of email communication")), Triple(Other_scientific_term("conversational nature of email communication"), Rel("Used_for"), Task("sentence extraction"))])
pred: [['sentence extraction', 'Used_for', 'summarization'], ['sentence extraction', 'Used_for', 'summarization'], ['genres', 'Used_for', 'sentence extraction'], ['genres', 'Conjunction', 'conversational nature of email communication'], ['conversational nature of email communication', 'Used_for', 'sentence extraction']]
trues: [['sentence extraction', 'Used_for', 'summarization']]
metrics_sample: [0.25, 1.0, 0.4]
metrics_current: [0.3807829181494662, 0.4068441064638783, 0.3933823529411764]
text: The proposed detectors are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to rotation , illumination variation , and blur .
result: extract = Extract([Triple(Generic("detectors"), Rel("Used_for"), Other_scientific_term("large-scale structures and distinctive textured patterns")), Triple(Other_scientific_term("rotation"), Rel("Conjunction"), Other_scientific_term("illumination variation")), Triple(Other_scientific_term("illumination variation"), Rel("Conjunction"), Other_scientific_term("blur"))])
pred: [['detectors', 'Used_for', 'large-scale structures and distinctive textured patterns'], ['rotation', 'Conjunction', 'illumination variation'], ['illumination variation', 'Conjunction', 'blur']]
trues: [['detectors', 'Used_for', 'large-scale structures'], ['detectors', 'Used_for', 'distinctive textured patterns'], ['detectors', 'Used_for', 'rotation'], ['detectors', 'Used_for', 'illumination variation'], ['detectors', 'Used_for', 'blur'], ['large-scale structures', 'Conjunction', 'distinctive textured patterns'], ['rotation', 'Conjunction', 'illumination variation'], ['illumination variation', 'Conjunction', 'blur']]
metrics_sample: [0.6666666666666666, 0.25, 0.36363636363636365]
metrics_current: [0.38380281690140844, 0.4022140221402214, 0.39279279279279283]
text: We evaluated the topic signatures on a WSD task , where we trained a second-order vector cooccurrence algorithm on standard WSD datasets , with promising results .
result: extract = Extract([Triple(Other_scientific_term("topic signatures"), Rel("Used_for"), Task("WSD task")), Triple(Method("second-order vector cooccurrence algorithm"), Rel("Used_for"), Task("WSD task")), Triple(Material("WSD datasets"), Rel("Evaluate_for"), Method("second-order vector cooccurrence algorithm"))])
pred: [['topic signatures', 'Used_for', 'WSD task'], ['second-order vector cooccurrence algorithm', 'Used_for', 'WSD task'], ['WSD datasets', 'Evaluate_for', 'second-order vector cooccurrence algorithm']]
trues: [['WSD task', 'Evaluate_for', 'topic signatures'], ['WSD datasets', 'Used_for', 'second-order vector cooccurrence algorithm']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3797909407665505, 0.3992673992673993, 0.3892857142857143]
text: We evaluate the demands that task dialogues collected using this technique , place upon a prototype Natural Language system .
result: extract = Extract([Triple(Generic("task dialogues"), Rel("Used_for"), Method("Natural Language system"))])
pred: [['task dialogues', 'Used_for', 'Natural Language system']]
trues: [['task dialogues', 'Used_for', 'prototype Natural Language system'], ['technique', 'Used_for', 'task dialogues']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3784722222222222, 0.39636363636363636, 0.38721136767317943]
text: Another problem with determiners is their inherent ambiguity .
result: extract = Extract([Triple(Other_scientific_term("ambiguity"), Rel("Feature_of"), Other_scientific_term("determiners"))])
pred: [['ambiguity', 'Feature_of', 'determiners']]
trues: [['ambiguity', 'Feature_of', 'determiners']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3806228373702422, 0.39855072463768115, 0.38938053097345127]
text: This motor control representation enables successful word spotting and matching of cursive scripts .
result: extract = Extract([Triple(Method("motor control representation"), Rel("Used_for"), Task("word spotting")), Triple(Method("motor control representation"), Rel("Used_for"), Task("matching of cursive scripts")), Triple(Task("word spotting"), Rel("Conjunction"), Task("matching of cursive scripts"))])
pred: [['motor control representation', 'Used_for', 'word spotting'], ['motor control representation', 'Used_for', 'matching of cursive scripts'], ['word spotting', 'Conjunction', 'matching of cursive scripts']]
trues: [['motor control representation', 'Used_for', 'word spotting'], ['motor control representation', 'Used_for', 'matching of cursive scripts'], ['word spotting', 'Conjunction', 'matching of cursive scripts']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.386986301369863, 0.4050179211469534, 0.39579684763572676]
text: In addition , we improve the learning by a metric weight constraint , so that the learned metric has a better generalization ability .
result: extract = Extract([Triple(Other_scientific_term("metric weight constraint"), Rel("Used_for"), Method("learning")), Triple(Other_scientific_term("metric weight constraint"), Rel("Used_for"), Other_scientific_term("metric")), Triple(Other_scientific_term("generalization ability"), Rel("Evaluate_for"), Other_scientific_term("metric"))])
pred: [['metric weight constraint', 'Used_for', 'learning'], ['metric weight constraint', 'Used_for', 'metric'], ['generalization ability', 'Evaluate_for', 'metric']]
trues: [['metric weight constraint', 'Used_for', 'learning'], ['generalization ability', 'Feature_of', 'learned metric']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.3864406779661017, 0.40569395017793597, 0.39583333333333337]
text: This work presents an automated technique for detecting reflections in image sequences by analyzing motion trajectories of feature points .
result: extract = Extract([Triple(Generic("automated technique"), Rel("Used_for"), Other_scientific_term("reflections")), Triple(Material("image sequences"), Rel("Used_for"), Generic("automated technique")), Triple(Other_scientific_term("motion trajectories of feature points"), Rel("Used_for"), Generic("automated technique"))])
pred: [['automated technique', 'Used_for', 'reflections'], ['image sequences', 'Used_for', 'automated technique'], ['motion trajectories of feature points', 'Used_for', 'automated technique']]
trues: [['technique', 'Used_for', 'detecting reflections in image sequences'], ['motion trajectories', 'Used_for', 'technique'], ['feature points', 'Feature_of', 'motion trajectories']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3825503355704698, 0.4014084507042254, 0.3917525773195877]
text: Relaxations of these properties expose some of the interesting ( and unavoidable ) trade-offs at work in well-studied clustering techniques such as single-linkage , sum-of-pairs , k-means , and k-median .
result: extract = Extract([Triple(Other_scientific_term("single-linkage"), Rel("Hyponym_of"), Generic("clustering techniques")), Triple(Other_scientific_term("single-linkage"), Rel("Conjunction"), Other_scientific_term("sum-of-pairs")), Triple(Other_scientific_term("sum-of-pairs"), Rel("Hyponym_of"), Generic("clustering techniques")), Triple(Other_scientific_term("sum-of-pairs"), Rel("Conjunction"), Other_scientific_term("k-means")), Triple(Other_scientific_term("k-means"), Rel("Hyponym_of"), Generic("clustering techniques")), Triple(Other_scientific_term("k-means"), Rel("Conjunction"), Other_scientific_term("k-median")), Triple(Other_scientific_term("k-median"), Rel("Hyponym_of"), Generic("clustering techniques"))])
pred: [['single-linkage', 'Hyponym_of', 'clustering techniques'], ['single-linkage', 'Conjunction', 'sum-of-pairs'], ['sum-of-pairs', 'Hyponym_of', 'clustering techniques'], ['sum-of-pairs', 'Conjunction', 'k-means'], ['k-means', 'Hyponym_of', 'clustering techniques'], ['k-means', 'Conjunction', 'k-median'], ['k-median', 'Hyponym_of', 'clustering techniques']]
trues: [['single-linkage', 'Hyponym_of', 'well-studied clustering techniques'], ['single-linkage', 'Conjunction', 'sum-of-pairs'], ['sum-of-pairs', 'Hyponym_of', 'well-studied clustering techniques'], ['sum-of-pairs', 'Conjunction', 'k-means'], ['k-means', 'Hyponym_of', 'well-studied clustering techniques'], ['k-means', 'Conjunction', 'k-median'], ['k-median', 'Hyponym_of', 'well-studied clustering techniques']]
metrics_sample: [0.42857142857142855, 0.42857142857142855, 0.42857142857142855]
metrics_current: [0.3836065573770492, 0.4020618556701031, 0.3926174496644295]
text: Our proposed approach builds on recent advances in spectral clustering , image matting , and boundary detection .
result: extract = Extract([Triple(Generic("approach"), Rel("Used_for"), Method("spectral clustering")), Triple(Generic("approach"), Rel("Used_for"), Method("image matting")), Triple(Generic("approach"), Rel("Used_for"), Method("boundary detection")), Triple(Method("spectral clustering"), Rel("Conjunction"), Method("image matting")), Triple(Method("image matting"), Rel("Conjunction"), Method("boundary detection"))])
pred: [['approach', 'Used_for', 'spectral clustering'], ['approach', 'Used_for', 'image matting'], ['approach', 'Used_for', 'boundary detection'], ['spectral clustering', 'Conjunction', 'image matting'], ['image matting', 'Conjunction', 'boundary detection']]
trues: [['spectral clustering', 'Used_for', 'approach'], ['spectral clustering', 'Conjunction', 'image matting'], ['image matting', 'Used_for', 'approach'], ['image matting', 'Conjunction', 'boundary detection'], ['boundary detection', 'Used_for', 'approach']]
metrics_sample: [0.4, 0.4, 0.4000000000000001]
metrics_current: [0.38387096774193546, 0.40202702702702703, 0.3927392739273927]
text: Techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches .
result: extract = Extract([Triple(Method("natural language generator"), Rel("Used_for"), Other_scientific_term("utterances")), Triple(Method("trainable components"), Rel("Part_of"), Method("natural language generator")), Triple(Method("hand-crafted template-based or rule-based approaches"), Rel("Compare"), Method("natural language generator"))])
pred: [['natural language generator', 'Used_for', 'utterances'], ['trainable components', 'Part_of', 'natural language generator'], ['hand-crafted template-based or rule-based approaches', 'Compare', 'natural language generator']]
trues: [['Techniques', 'Used_for', 'automatically training modules'], ['automatically training modules', 'Part_of', 'natural language generator'], ['utterances', 'Evaluate_for', 'trainable components'], ['utterances', 'Evaluate_for', 'hand-crafted template-based or rule-based approaches'], ['trainable components', 'Compare', 'hand-crafted template-based or rule-based approaches']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3801916932907348, 0.3953488372093023, 0.38762214983713356]
text: We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representations .
result: extract = Extract([Triple(Method("CNN models"), Rel("Used_for"), Other_scientific_term("layers of distributed representations")), Triple(Other_scientific_term("layers of distributed representations"), Rel("Used_for"), Other_scientific_term("object pose information")), Triple(Other_scientific_term("object category representations"), Rel("Contrast"), Other_scientific_term("object pose information"))])
pred: [['CNN models', 'Used_for', 'layers of distributed representations'], ['layers of distributed representations', 'Used_for', 'object pose information'], ['object category representations', 'Contrast', 'object pose information']]
trues: [['layers', 'Part_of', 'CNN models'], ['layers of distributed representations', 'Part_of', 'CNNs'], ['layers of distributed representations', 'Used_for', 'object pose information'], ['this', 'Compare', 'object category representations']]
metrics_sample: [0.3333333333333333, 0.25, 0.28571428571428575]
metrics_current: [0.379746835443038, 0.39344262295081966, 0.3864734299516908]
text: Learned confidence measures gain increasing importance for outlier removal and quality improvement in stereo vision .
result: extract = Extract([Triple(Method("Learned confidence measures"), Rel("Used_for"), Task("outlier removal")), Triple(Method("Learned confidence measures"), Rel("Used_for"), Task("quality improvement")), Triple(Task("outlier removal"), Rel("Conjunction"), Task("quality improvement")), Triple(Task("stereo vision"), Rel("Evaluate_for"), Task("outlier removal")), Triple(Task("stereo vision"), Rel("Evaluate_for"), Task("quality improvement"))])
pred: [['Learned confidence measures', 'Used_for', 'outlier removal'], ['Learned confidence measures', 'Used_for', 'quality improvement'], ['outlier removal', 'Conjunction', 'quality improvement'], ['stereo vision', 'Evaluate_for', 'outlier removal'], ['stereo vision', 'Evaluate_for', 'quality improvement']]
trues: [['Learned confidence measures', 'Used_for', 'outlier removal'], ['Learned confidence measures', 'Used_for', 'quality improvement'], ['outlier removal', 'Conjunction', 'quality improvement'], ['outlier removal', 'Part_of', 'stereo vision'], ['quality improvement', 'Part_of', 'stereo vision']]
metrics_sample: [0.6, 0.6, 0.6]
metrics_current: [0.38317757009345793, 0.3967741935483871, 0.3898573692551506]
text: A statistical translation model is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the NIST evaluation metric .
result: extract = Extract([Triple(Method("statistical translation model"), Rel("Used_for"), Other_scientific_term("phrases")), Triple(Method("training method"), Rel("Used_for"), Other_scientific_term("phrases")), Triple(Metric("translation accuracy"), Rel("Evaluate_for"), Method("training method")), Triple(Metric("NIST evaluation metric"), Rel("Used_for"), Metric("translation accuracy"))])
pred: [['statistical translation model', 'Used_for', 'phrases'], ['training method', 'Used_for', 'phrases'], ['translation accuracy', 'Evaluate_for', 'training method'], ['NIST evaluation metric', 'Used_for', 'translation accuracy']]
trues: [['statistical translation model', 'Used_for', 'phrases'], ['maximization of translation accuracy', 'Used_for', 'training method'], ['NIST evaluation metric', 'Evaluate_for', 'statistical translation model']]
metrics_sample: [0.25, 0.3333333333333333, 0.28571428571428575]
metrics_current: [0.38153846153846155, 0.3961661341853035, 0.3887147335423197]
text: We present a novel approach for automatically acquiring English topic signatures .
result: extract = Extract([Triple(Generic("approach"), Rel("Used_for"), Task("automatically acquiring English topic signatures"))])
pred: [['approach', 'Used_for', 'automatically acquiring English topic signatures']]
trues: [['approach', 'Used_for', 'automatically acquiring English topic signatures']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3834355828220859, 0.3980891719745223, 0.390625]
text: We propose a novel metric learning formulation called Weighted Approximate Rank Component Analysis ( WARCA ) .
result: extract = Extract([Triple(Method("Weighted Approximate Rank Component Analysis ( WARCA )"), Rel("Hyponym_of"), Method("metric learning formulation"))])
pred: [['Weighted Approximate Rank Component Analysis ( WARCA )', 'Hyponym_of', 'metric learning formulation']]
trues: [['Weighted Approximate Rank Component Analysis ( WARCA )', 'Hyponym_of', 'metric learning formulation']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3853211009174312, 0.4, 0.3925233644859813]
text: In the second half of the paper , we report a laboratory study using the Wizard of Oz technique to identify NL requirements for carrying out this task .
result: extract = Extract([Triple(Method("Wizard of Oz technique"), Rel("Used_for"), Other_scientific_term("NL requirements")), Triple(Other_scientific_term("NL requirements"), Rel("Used_for"), Generic("task"))])
pred: [['Wizard of Oz technique', 'Used_for', 'NL requirements'], ['NL requirements', 'Used_for', 'task']]
trues: [['Wizard of Oz technique', 'Used_for', 'NL requirements'], ['Wizard of Oz technique', 'Used_for', 'task']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3860182370820669, 0.40063091482649843, 0.3931888544891641]
text: The applicability of many current information extraction techniques is severely limited by the need for supervised training data .
result: extract = Extract([Triple(Material("supervised training data"), Rel("Used_for"), Method("information extraction techniques"))])
pred: [['supervised training data', 'Used_for', 'information extraction techniques']]
trues: [['supervised training data', 'Used_for', 'information extraction techniques']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3878787878787879, 0.4025157232704403, 0.39506172839506176]
text: This paper describes a method for incorporating priming into an incremental probabilistic parser .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Task("priming")), Triple(Method("incremental probabilistic parser"), Rel("Used_for"), Generic("method"))])
pred: [['method', 'Used_for', 'priming'], ['incremental probabilistic parser', 'Used_for', 'method']]
trues: [['method', 'Used_for', 'priming'], ['priming', 'Used_for', 'incremental probabilistic parser']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3885542168674699, 0.403125, 0.3957055214723927]
text: Machine transliteration/back-transliteration plays an important role in many multilingual speech and language applications .
result: extract = Extract([Triple(Task("Machine translliteration/back-transliteration"), Rel("Used_for"), Task("multilingual speech and language applications"))])
pred: [['Machine translliteration/back-transliteration', 'Used_for', 'multilingual speech and language applications']]
trues: [['Machine transliteration/back-transliteration', 'Used_for', 'multilingual speech and language applications']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38738738738738737, 0.40186915887850466, 0.3944954128440367]
text: We compare our results to state of the art methods such as mi-cro phase shifting and modulated phase shifting .
result: extract = Extract([Triple(Generic("state of the art methods"), Rel("Compare"), Generic("our results")), Triple(Method("mi-cro phase shifting"), Rel("Hyponym_of"), Generic("state of the art methods")), Triple(Method("mi-cro phase shifting"), Rel("Conjunction"), Method("modulated phase shifting")), Triple(Method("modulated phase shifting"), Rel("Hyponym_of"), Generic("state of the art methods"))])
pred: [['state of the art methods', 'Compare', 'our results'], ['mi-cro phase shifting', 'Hyponym_of', 'state of the art methods'], ['mi-cro phase shifting', 'Conjunction', 'modulated phase shifting'], ['modulated phase shifting', 'Hyponym_of', 'state of the art methods']]
trues: [['mi-cro phase shifting', 'Hyponym_of', 'state of the art methods'], ['mi-cro phase shifting', 'Conjunction', 'modulated phase shifting'], ['modulated phase shifting', 'Hyponym_of', 'state of the art methods']]
metrics_sample: [0.75, 1.0, 0.8571428571428571]
metrics_current: [0.3916913946587537, 0.4074074074074074, 0.3993948562783661]
text: We demonstrate our approach with multiple PTZ camera sequences in typical outdoor surveillance settings and show a comparison with state-of-the-art approaches .
result: extract = Extract([Triple(Material("PTZ camera sequences"), Rel("Used_for"), Generic("approach")), Triple(Material("PTZ camera sequences"), Rel("Used_for"), Task("outdoor surveillance settings")), Triple(Generic("approach"), Rel("Compare"), Generic("state-of-the-art approaches"))])
pred: [['PTZ camera sequences', 'Used_for', 'approach'], ['PTZ camera sequences', 'Used_for', 'outdoor surveillance settings'], ['approach', 'Compare', 'state-of-the-art approaches']]
trues: [['approach', 'Used_for', 'outdoor surveillance settings'], ['approach', 'Compare', 'state-of-the-art approaches'], ['multiple PTZ camera sequences', 'Used_for', 'approach']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.3911764705882353, 0.40672782874617736, 0.39880059970014997]
text: We show how this can efficiently be combined with random-projection based methods for 2 NN search , such as locality-sensitive hashing ( LSH ) or random projection trees .
result: extract = Extract([Triple(Method("random-projection based methods"), Rel("Used_for"), Task("2 NN search")), Triple(Method("locality-sensitive hashing ( LSH )"), Rel("Hyponym_of"), Method("random-projection methods")), Triple(Method("locality-sensitive hashing ( LSH )"), Rel("Conjunction"), Method("random projection trees")), Triple(Method("random projection trees"), Rel("Hyponym_of"), Method("random-projection methods"))])
pred: [['random-projection based methods', 'Used_for', '2 NN search'], ['locality-sensitive hashing ( LSH )', 'Hyponym_of', 'random-projection methods'], ['locality-sensitive hashing ( LSH )', 'Conjunction', 'random projection trees'], ['random projection trees', 'Hyponym_of', 'random-projection methods']]
trues: [['this', 'Conjunction', 'random-projection based methods'], ['random-projection based methods', 'Used_for', 'NN search'], ['locality-sensitive hashing ( LSH )', 'Hyponym_of', 'random-projection based methods'], ['locality-sensitive hashing ( LSH )', 'Conjunction', 'random projection trees'], ['random projection trees', 'Hyponym_of', 'random-projection based methods']]
metrics_sample: [0.25, 0.2, 0.22222222222222224]
metrics_current: [0.38953488372093026, 0.4036144578313253, 0.39644970414201186]
text: The goal of this work is the enrichment of human-machine interactions in a natural language environment .
result: extract = Extract([Triple(Material("natural language environment"), Rel("Feature_of"), Task("human-machine interactions"))])
pred: [['natural language environment', 'Feature_of', 'human-machine interactions']]
trues: [['natural language environment', 'Feature_of', 'human-machine interactions']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.391304347826087, 0.40540540540540543, 0.39823008849557523]
text: Branch and bound strategies have previously attempted to curb this complexity whilst maintaining global optimality .
result: extract = Extract([Triple(Method("Branch and bound strategies"), Rel("Used_for"), Other_scientific_term("global optimality"))])
pred: [['Branch and bound strategies', 'Used_for', 'global optimality']]
trues: [['Branch and bound strategies', 'Used_for', 'complexity'], ['global optimality', 'Feature_of', 'Branch and bound strategies']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3901734104046243, 0.40298507462686567, 0.39647577092511016]
text: The objective of this work is to recognize faces using video sequences both for training and recognition input , in a realistic , unconstrained setup in which lighting , pose and user motion pattern have a wide variability and face images are of low resolution .
result: extract = Extract([Triple(Material("video sequences"), Rel("Used_for"), Task("recognition")), Triple(Material("low resolution"), Rel("Feature_of"), Material("face images"))])
pred: [['video sequences', 'Used_for', 'recognition'], ['low resolution', 'Feature_of', 'face images']]
trues: [['lighting', 'Conjunction', 'pose'], ['pose', 'Conjunction', 'user motion pattern'], ['resolution', 'Feature_of', 'face images']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3879310344827586, 0.3994082840236686, 0.3935860058309038]
text: In addition to the high accuracy of the model , the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results .
result: extract = Extract([Triple(Method("smoothing"), Rel("Used_for"), Method("unlexicalized parser")), Triple(Method("smoothing"), Rel("Conjunction"), Generic("parsing results"))])
pred: [['smoothing', 'Used_for', 'unlexicalized parser'], ['smoothing', 'Conjunction', 'parsing results']]
trues: [['accuracy', 'Evaluate_for', 'model'], ['smoothing', 'Used_for', 'unlexicalized parser']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.38857142857142857, 0.4, 0.39420289855072466]
text: The combined approach is formulated in the framework of Bayesian statistics to account for the robust-ness requirement in image understanding .
result: extract = Extract([Triple(Method("Bayesian statistics"), Rel("Used_for"), Generic("approach")), Triple(Task("image understanding"), Rel("Used_for"), Method("Bayesian statistics"))])
pred: [['Bayesian statistics', 'Used_for', 'approach'], ['image understanding', 'Used_for', 'Bayesian statistics']]
trues: [['approach', 'Used_for', 'robust-ness requirement in image understanding'], ['Bayesian statistics', 'Used_for', 'approach']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.38920454545454547, 0.40058479532163743, 0.3948126801152738]
text: Topical blog post retrieval is the task of ranking blog posts with respect to their relevance for a given topic .
result: extract = Extract([Triple(Task("ranking blog posts"), Rel("Used_for"), Task("Topical blog post retrieval")), Triple(Other_scientific_term("topic"), Rel("Used_for"), Task("ranking blog posts"))])
pred: [['ranking blog posts', 'Used_for', 'Topical blog post retrieval'], ['topic', 'Used_for', 'ranking blog posts']]
trues: [['Topical blog post retrieval', 'Hyponym_of', 'ranking blog posts'], ['relevance', 'Feature_of', 'blog posts']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3870056497175141, 0.39825581395348836, 0.39255014326647564]
text: Extended CF grammars ( grammars with regular expressions at the right hand side ) can be parsed with a simple modification of the LR-parser for normal CF grammars .
result: extract = Extract([Triple(Method("Extended CF grammars"), Rel("Used_for"), Method("LR-parser")), Triple(Method("LR-parser"), Rel("Used_for"), Method("normal CF grammars"))])
pred: [['Extended CF grammars', 'Used_for', 'LR-parser'], ['LR-parser', 'Used_for', 'normal CF grammars']]
trues: [['regular expressions', 'Feature_of', 'grammars'], ['LR-parser', 'Used_for', 'Extended CF grammars'], ['LR-parser', 'Used_for', 'CF grammars']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3848314606741573, 0.39481268011527376, 0.3897581792318635]
text: Our main result is a first-order upper bound on the distance between any approximate joint triangularizer of the matrices in M ' and any exact joint triangularizer of the matrices in M .
result: extract = Extract([Triple(Metric("first-order upper bound"), Rel("Evaluate_for"), Other_scientific_term("distance")), Triple(Other_scientific_term("approximate joint triangularizer"), Rel("Conjunction"), Other_scientific_term("exact joint triangularizer")), Triple(Other_scientific_term("matrices"), Rel("Part_of"), Other_scientific_term("approximate joint triangularizer")), Triple(Other_scientific_term("matrices"), Rel("Part_of"), Other_scientific_term("exact joint triangularizer"))])
pred: [['first-order upper bound', 'Evaluate_for', 'distance'], ['approximate joint triangularizer', 'Conjunction', 'exact joint triangularizer'], ['matrices', 'Part_of', 'approximate joint triangularizer'], ['matrices', 'Part_of', 'exact joint triangularizer']]
trues: [['approximate joint triangularizer', 'Conjunction', 'exact joint triangularizer']]
metrics_sample: [0.25, 1.0, 0.4]
metrics_current: [0.38333333333333336, 0.39655172413793105, 0.38983050847457634]
text: In the case of natural speech signals , the method is shown to have separation accuracy better than the state-of-the-art methods .
result: extract = Extract([Triple(Generic("method"), Rel("Compare"), Generic("state-of-the-art methods"))])
pred: [['method', 'Compare', 'state-of-the-art methods']]
trues: [['method', 'Used_for', 'natural speech signals'], ['method', 'Compare', 'methods'], ['separation accuracy', 'Evaluate_for', 'method'], ['separation accuracy', 'Evaluate_for', 'methods'], ['methods', 'Used_for', 'natural speech signals']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38227146814404434, 0.3909348441926346, 0.3865546218487395]
text: Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns .
result: extract = Extract([Triple(Other_scientific_term("zero-crossings"), Rel("Used_for"), Other_scientific_term("Subpixel accuracy")), Triple(Other_scientific_term("unstructured patterns"), Rel("Used_for"), Other_scientific_term("zero-crossings"))])
pred: [['zero-crossings', 'Used_for', 'Subpixel accuracy'], ['unstructured patterns', 'Used_for', 'zero-crossings']]
trues: [['zero-crossings', 'Used_for', 'Subpixel accuracy']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.38292011019283745, 0.3926553672316384, 0.3877266387726639]
text: In order to build robust automatic abstracting systems , there is a need for better training resources than are currently available .
result: extract = Extract([Triple(Material("training resources"), Rel("Used_for"), Task("automatic abstracting systems"))])
pred: [['training resources', 'Used_for', 'automatic abstracting systems']]
trues: [['training resources', 'Used_for', 'automatic abstracting systems']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.38461538461538464, 0.39436619718309857, 0.3894297635605007]
text: Experimental results from a real telephone application on a natural number recognition task show an 50 % reduction in recognition errors with a moderate 12 % rejection rate of correct utterances and a low 1.5 % rate of false acceptance .
result: extract = Extract([Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Other_scientific_term("telephone application")), Triple(Metric("recognition errors"), Rel("Evaluate_for"), Task("natural number recognition task")), Triple(
pred: [['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task'], ['recognition errors', 'Evaluate_for', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task']]
trues: [['natural number recognition task', 'Feature_of', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.38524590163934425, 0.3949579831932773, 0.39004149377593356]
text: Thus the ASR system is designed to deal with what we refer to as Word Spotting and Noise Spotting capabilities .
result: extract = Extract([Triple(Method("ASR system"), Rel("Used_for"), Task("Word Spotting and Noise Spotting capabilities"))])
pred: [['ASR system', 'Used_for', 'Word Spotting and Noise Spotting capabilities']]
trues: [['ASR system', 'Used_for', 'Word Spotting'], ['ASR system', 'Used_for', 'Noise Spotting capabilities']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38419618528610355, 0.39275766016713093, 0.3884297520661157]
text: We also introduce a novel classification method based on PER which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence .
result: extract = Extract([Triple(Method("classification method"), Rel("Used_for"), Other_scientific_term("PER")), Triple(Other_scientific_term("PER"), Rel("Used_for"), Other_scientific_term("part of speech information")), Triple(Other_scientific_term("part of speech information"), Rel("Feature_of"), Other_scientific_term("word matches")), Triple(Other_scientific_term("part of speech information"), Rel("Feature_of"), Other_scientific_term("non-matches"))])
pred: [['classification method', 'Used_for', 'PER'], ['PER', 'Used_for', 'part of speech information'], ['part of speech information', 'Feature_of', 'word matches'], ['part of speech information', 'Feature_of', 'non-matches']]
trues: [['PER', 'Used_for', 'classification method'], ['PER', 'Used_for', 'part of speech information'], ['part of speech information', 'Used_for', 'word matches and non-matches']]
metrics_sample: [0.25, 0.3333333333333333, 0.28571428571428575]
metrics_current: [0.38274932614555257, 0.39226519337016574, 0.38744884038199184]
text: We evaluate the proposed methods through several transliteration/backtransliteration experiments for English/Chinese and English/Japanese language pairs .
result: extract = Extract([Triple(Generic("methods"), Rel("Used_for"), Task("transliteration/backtransliteration experiments")), Triple(Material("English/Chinese"), Rel("Evaluate_for"), Generic("methods")), Triple(Material("English/Chinese"), Rel("Conjunction"), Material("English/Japanese language pairs")), Triple(Material("English/Japanese language pairs"), Rel("Evaluate_for"), Generic("methods"))])
pred: [['methods', 'Used_for', 'transliteration/backtransliteration experiments'], ['English/Chinese', 'Evaluate_for', 'methods'], ['English/Chinese', 'Conjunction', 'English/Japanese language pairs'], ['English/Japanese language pairs', 'Evaluate_for', 'methods']]
trues: [['transliteration/backtransliteration', 'Evaluate_for', 'methods'], ['transliteration/backtransliteration', 'Used_for', 'English/Chinese and English/Japanese language pairs']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37866666666666665, 0.3901098901098901, 0.38430311231393777]
text: In this paper we present a new UV procedure with two major features : a ) Confidence tests are applied to decoded string hypotheses obtained from using word and garbage models that represent OOV words and noises .
result: extract = Extract([Triple(Method("UV procedure"), Rel("Used_for"), Other_scientific_term("Confidence tests")), Triple(Other_scientific_term("Confidence tests"), Rel("Used_for"), Other_scientific_term("decoded string hypotheses")), Triple(Method("word and garbage models"), Rel("Used_for"), Other_scientific_term("OOV words")), Triple(Method("word and garbage models"), Rel("Used_for"), Other_scientific_term("noises"))])
pred: [['UV procedure', 'Used_for', 'Confidence tests'], ['Confidence tests', 'Used_for', 'decoded string hypotheses'], ['word and garbage models', 'Used_for', 'OOV words'], ['word and garbage models', 'Used_for', 'noises']]
trues: [['Confidence tests', 'Used_for', 'decoded string hypotheses'], ['noises', 'Conjunction', 'OOV words']]
metrics_sample: [0.25, 0.5, 0.3333333333333333]
metrics_current: [0.37730870712401055, 0.3907103825136612, 0.3838926174496644]
text: Establishing a `` best '' correspondence between the '' UNL-tree + L0 '' and the '' MS-L0 structure '' , a lattice , may be done using the dictionary and trying to align the tree and the selected trajectory with as few crossing liaisons as possible .
result: extract = Extract([Triple(Other_scientific_term("UNL-tree + L0"), Rel("Conjunction"), Other_scientific_term("MS-L0 structure")), Triple(Other_scientific_term("UNL-tree + L0"), Rel("Used_for"), Other_scientific_term("lattice")), Triple(Other_scientific_term("MS-L0 structure"), Rel("Used_for"), Other_scientific_term("lattice"))])
pred: [['UNL-tree + L0', 'Conjunction', 'MS-L0 structure'], ['UNL-tree + L0', 'Used_for', 'lattice'], ['MS-L0 structure', 'Used_for', 'lattice']]
trues: [['UNL-tree + L0', 'Conjunction', 'MS-L0 structure'], ['dictionary', 'Used_for', 'lattice']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.3769633507853403, 0.391304347826087, 0.38400000000000006]
text: Given an object model and a black-box measure of similarity between the model and candidate targets , we consider visual object tracking as a numerical optimization problem .
result: extract = Extract([Triple(Method("object model"), Rel("Used_for"), Task("visual object tracking")), Triple(Other_scientific_term("black-box measure of similarity"), Rel("Used_for"), Task("visual object tracking")), Triple(Task("numerical optimization problem"), Rel("Used_for"), Task("visual object tracking"))])
pred: [['object model', 'Used_for', 'visual object tracking'], ['black-box measure of similarity', 'Used_for', 'visual object tracking'], ['numerical optimization problem', 'Used_for', 'visual object tracking']]
trues: [['numerical optimization problem', 'Used_for', 'visual object tracking']]
metrics_sample: [0.3333333333333333, 1.0, 0.5]
metrics_current: [0.37662337662337664, 0.39295392953929537, 0.3846153846153846]
text: Based on this , MINPRAN uses random sampling to search for the t and the number of inliers to the t that are least likely to have occurred randomly .
result: extract = Extract([Triple(Method("random sampling"), Rel("Used_for"), Method("MINPRAN"))])
pred: [['random sampling', 'Used_for', 'MINPRAN']]
trues: [['random sampling', 'Used_for', 'MINPRAN']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.37823834196891193, 0.3945945945945946, 0.3862433862433863]
text: The results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis for a useful disambiguation tool .
result: extract = Extract([Triple(Other_scientific_term("cooccurrence statistics"), Rel("Used_for"), Task("semantic constraints")), Triple(Other_scientific_term("cooccurrence statistics"), Rel("Used_for"), Method("disambiguation tool"))])
pred: [['cooccurrence statistics', 'Used_for', 'semantic constraints'], ['cooccurrence statistics', 'Used_for', 'disambiguation tool']]
trues: [['cooccurrence statistics', 'Used_for', 'disambiguation tool']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3788659793814433, 0.39622641509433965, 0.38735177865612647]
text: We have previously introduced a statistical model describing the generating process of speech F0 contours , based on the discrete-time version of the Fujisaki model .
result: extract = Extract([Triple(Method("statistical model"), Rel("Used_for"), Other_scientific_term("speech F0 contours")), Triple(Method("discrete-time version of the Fujisaki model"), Rel("Used_for"), Method("statistical model"))])
pred: [['statistical model', 'Used_for', 'speech F0 contours'], ['discrete-time version of the Fujisaki model', 'Used_for', 'statistical model']]
trues: [['statistical model', 'Used_for', 'speech F0 contours'], ['Fujisaki model', 'Used_for', 'statistical model']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.37948717948717947, 0.3967828418230563, 0.3879423328964613]
text: We consider the problem of computing the Kullback-Leibler distance , also called the relative entropy , between a probabilistic context-free grammar and a probabilistic finite automaton .
result: extract = Extract([Triple(Metric("Kullback-Leibler distance"), Rel("Used_for"), Other_scientific_term("relative entropy")), Triple(Method("probabilistic context-free grammar"), Rel("Conjunction"), Method("probabilistic finite automaton"))])
pred: [['Kullback-Leibler distance', 'Used_for', 'relative entropy'], ['probabilistic context-free grammar', 'Conjunction', 'probabilistic finite automaton']]
trues: [['probabilistic context-free grammar', 'Compare', 'probabilistic finite automaton']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37755102040816324, 0.39572192513368987, 0.3864229765013055]
text: This paper solves a specialized regression problem to obtain sampling probabilities for records in databases .
result: extract = Extract([Triple(Task("regression problem"), Rel("Used_for"), Other_scientific_term("sampling probabilities")), Triple(Other_scientific_term("records"), Rel("Part_of"), Other_scientific_term("databases"))])
pred: [['regression problem', 'Used_for', 'sampling probabilities'], ['records', 'Part_of', 'databases']]
trues: [['specialized regression problem', 'Used_for', 'sampling probabilities'], ['sampling probabilities', 'Used_for', 'records'], ['records', 'Part_of', 'databases']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.37817258883248733, 0.3952254641909814, 0.3865110246433204]
text: A random modulation scheme produces near-Bernoulli statistics even for highly correlated inputs .
result: extract = Extract([Triple(Method("random modulation scheme"), Rel("Used_for"), Other_scientific_term("near-Bernoulli statistics")), Triple(Other_scientific_term("highly correlated inputs"), Rel("Feature_of"), Other_scientific_term("near-Bernoulli statistics"))])
pred: [['random modulation scheme', 'Used_for', 'near-Bernoulli statistics'], ['highly correlated inputs', 'Feature_of', 'near-Bernoulli statistics']]
trues: [['random modulation scheme', 'Used_for', 'near-Bernoulli statistics'], ['highly correlated inputs', 'Used_for', 'random modulation scheme']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3787878787878788, 0.39577836411609496, 0.38709677419354843]
text: Labeled data is replaced by a few hand-crafted rules that encode basic syntactic knowledge .
result: extract = Extract([Triple(Other_scientific_term("hand-crafted rules"), Rel("Used_for"), Other_scientific_term("syntactic knowledge"))])
pred: [['hand-crafted rules', 'Used_for', 'syntactic knowledge']]
trues: [['hand-crafted rules', 'Used_for', 'syntactic knowledge']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.380352644836272, 0.3973684210526316, 0.3886743886743887]
text: We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word-to-word alignments from an MT system , and syntactic structure from parse-trees of source and target language sentences .
result: extract = Extract([Triple(Other_scientific_term("word strings"), Rel("Part_of"), Other_scientific_term("linguistic information")), Triple(Other_scientific_term("word strings"), Rel("Conjunction"), Other_scientific_term("word-to-word alignments")), Triple(Method("MT system"), Rel("Used_for"), Other_scientific_term("word-to-word alignments")), Triple(Other_scientific_term("word-to-word alignments"), Rel("Part_of"), Other_scientific_term("linguistic information")), Triple(Other_scientific_term("word-to-word alignments"), Rel("Conjunction"), Other_scientific_term("syntactic structure")), Triple(Other_scientific_term("syntactic structure"), Rel("Part_of"), Other_scientific_term("linguistic information")), Triple(Other_scientific_term("syntactic structure"), Rel("Conjunction"), Other_scientific_term("parse-trees")), Triple(Other_scientific_term("parse-trees"), Rel("Part_of"), Other_scientific_term("linguistic information")), Triple(Other_scientific_term("source and target language sentences"), Rel("Used_for"), Other_scientific_term("parse-trees"))])
pred: [['word strings', 'Part_of', 'linguistic information'], ['word strings', 'Conjunction', 'word-to-word alignments'], ['MT system', 'Used_for', 'word-to-word alignments'], ['word-to-word alignments', 'Part_of', 'linguistic information'], ['word-to-word alignments', 'Conjunction', 'syntactic structure'], ['syntactic structure', 'Part_of', 'linguistic information'], ['syntactic structure', 'Conjunction', 'parse-trees'], ['parse-trees', 'Part_of', 'linguistic information'], ['source and target language sentences', 'Used_for', 'parse-trees']]
trues: [['linguistic information', 'Used_for', 'loss functions'], ['word-to-word alignments', 'Used_for', 'loss functions'], ['word-to-word alignments', 'Part_of', 'MT system'], ['syntactic structure', 'Used_for', 'loss functions'], ['parse-trees', 'Part_of', 'syntactic structure']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37192118226600984, 0.3922077922077922, 0.3817951959544881]
text: This paper examines the benefits of system combination for unsupervised WSD .
result: extract = Extract([Triple(Method("system combination"), Rel("Used_for"), Task("unsupervised WSD"))])
pred: [['system combination', 'Used_for', 'unsupervised WSD']]
trues: [['system combination', 'Used_for', 'unsupervised WSD']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.37346437346437344, 0.39378238341968913, 0.3833543505674653]
text: In this paper , a novel framework for machine transliteration/backtransliteration that allows us to carry out direct orthographical mapping ( DOM ) between two different languages is presented .
result: extract = Extract([Triple(Generic("framework"), Rel("Used_for"), Task("machine transliteration/backtransliteration")), Triple(Task("direct orthographical mapping ( DOM )"), Rel("Used_for"), Generic("framework"))])
pred: [['framework', 'Used_for', 'machine transliteration/backtransliteration'], ['direct orthographical mapping ( DOM )', 'Used_for', 'framework']]
trues: [['framework', 'Used_for', 'machine transliteration/backtransliteration'], ['machine transliteration/backtransliteration', 'Used_for', 'direct orthographical mapping ( DOM )']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3740831295843521, 0.3943298969072165, 0.38393977415307406]
text: Our core motivation is that minimizing a weighted rank loss is a natural criterion for many problems in computer vision such as person re-identification .
result: extract = Extract([Triple(Metric("weighted rank loss"), Rel("Used_for"), Task("computer vision")), Triple(Task("computer vision"), Rel("Used_for"), Task("person re-identification"))])
pred: [['weighted rank loss', 'Used_for', 'computer vision'], ['computer vision', 'Used_for', 'person re-identification']]
trues: [['weighted rank loss', 'Used_for', 'computer vision'], ['weighted rank loss', 'Used_for', 'person re-identification'], ['person re-identification', 'Hyponym_of', 'computer vision']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.3746958637469586, 0.3938618925831202, 0.38403990024937656]
text: Furthermore , this paper presents a novel algorithm for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts ( flicker ) ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space .
result: extract = Extract([Triple(Generic("algorithm"), Rel("Used_for"), Other_scientific_term("background model")), Triple(Generic("algorithm"), Rel("Used_for"), Other_scientific_term("occlusions")), Triple(Generic("algorithm"), Rel("Used_for"), Other_scientific_term("temporal artefacts ( flicker )")), Triple(Method("cost aggregation algorithm"), Rel("Used_for"), Other_scientific_term("matching cost space"))])
pred: [['algorithm', 'Used_for', 'background model'], ['algorithm', 'Used_for', 'occlusions'], ['algorithm', 'Used_for', 'temporal artefacts ( flicker )'], ['cost aggregation algorithm', 'Used_for', 'matching cost space']]
trues: [['algorithm', 'Used_for', 'temporal maintenance of a background model'], ['algorithm', 'Used_for', 'rendering of occlusions'], ['algorithm', 'Used_for', 'temporal artefacts ( flicker )'], ['cost aggregation algorithm', 'Conjunction', 'algorithm'], ['cost aggregation algorithm', 'Used_for', 'three-dimensional matching cost space']]
metrics_sample: [0.25, 0.2, 0.22222222222222224]
metrics_current: [0.37349397590361444, 0.39141414141414144, 0.3822441430332922]
text: To our knowledge , this is the first a posteriori bound for joint matrix decomposition .
result: extract = Extract([Triple(Metric("a posteriori bound"), Rel("Evaluate_for"), Method("joint matrix decomposition"))])
pred: [['a posteriori bound', 'Evaluate_for', 'joint matrix decomposition']]
trues: [['posteriori bound', 'Used_for', 'joint matrix decomposition']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37259615384615385, 0.3904282115869018, 0.3813038130381304]
text: We propose a convex problem to incorporate side information in robust PCA and show that the low rank matrix can be exactly recovered via the proposed method under certain conditions .
result: extract = Extract([Triple(Other_scientific_term("side information"), Rel("Used_for"), Task("robust PCA")), Triple(Generic("convex problem"), Rel("Used_for"), Task("robust PCA")), Triple(Generic("convex problem"), Rel("Used_for"), Task("low rank matrix"))])
pred: [['side information', 'Used_for', 'robust PCA'], ['convex problem', 'Used_for', 'robust PCA'], ['convex problem', 'Used_for', 'low rank matrix']]
trues: [['convex problem', 'Used_for', 'side information'], ['side information', 'Part_of', 'robust PCA'], ['method', 'Used_for', 'low rank matrix']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.36992840095465396, 0.3875, 0.3785103785103785]
text: This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .
result: extract = Extract([Triple(Generic("grammar formalisms"), Rel("Used_for"), Generic("formalism")), Triple(Method("rewriting systems"), Rel("Hyponym_of"), Generic("grammar formalisms")), Triple(Method("rewriting systems"), Rel("Conjunction"), Method("dependency grammars")), Triple(Method("dependency grammars"), Rel("Hyponym_of"), Generic("grammar formalisms")), Triple(Method("dependency grammars"), Rel("Conjunction"), Method("TAG")), Triple(Method("TAG"), Rel("Hyponym_of"), Generic("grammar formalisms")), Triple(Method("TAG"), Rel("Conjunction"), Method("HPSG")), Triple(Method("HPSG"), Rel("Hyponym_of"), Generic("grammar formalisms")), Triple(Method("HPSG"), Rel("Conjunction"), Method("LFG")), Triple(Method("LFG"), Rel("Hyponym_of"), Generic("grammar formalisms"))])
pred: [['grammar formalisms', 'Used_for', 'formalism'], ['rewriting systems', 'Hyponym_of', 'grammar formalisms'], ['rewriting systems', 'Conjunction', 'dependency grammars'], ['dependency grammars', 'Hyponym_of', 'grammar formalisms'], ['dependency grammars', 'Conjunction', 'TAG'], ['TAG', 'Hyponym_of', 'grammar formalisms'], ['TAG', 'Conjunction', 'HPSG'], ['HPSG', 'Hyponym_of', 'grammar formalisms'], ['HPSG', 'Conjunction', 'LFG'], ['LFG', 'Hyponym_of', 'grammar formalisms']]
trues: [['formalism', 'Used_for', 'grammar formalisms'], ['rewriting systems', 'Hyponym_of', 'grammar formalisms'], ['rewriting systems', 'Conjunction', 'dependency grammars'], ['dependency grammars', 'Hyponym_of', 'grammar formalisms'], ['dependency grammars', 'Conjunction', 'TAG'], ['TAG', 'Hyponym_of', 'grammar formalisms'], ['TAG', 'Conjunction', 'HPSG'], ['HPSG', 'Hyponym_of', 'grammar formalisms'], ['HPSG', 'Conjunction', 'LFG'], ['LFG', 'Hyponym_of', 'grammar formalisms']]
metrics_sample: [0.9, 0.9, 0.9]
metrics_current: [0.3822843822843823, 0.4, 0.3909415971394517]
text: The automated segmentation of images into semantically meaningful parts requires shape information since low-level feature analysis alone often fails to reach this goal .
result: extract = Extract([Triple(Other_scientific_term("shape information"), Rel("Used_for"), Task("automated segmentation of images"))])
pred: [['shape information', 'Used_for', 'automated segmentation of images']]
trues: [['images', 'Used_for', 'automated segmentation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3813953488372093, 0.39902676399026765, 0.39001189060642094]
text: The goal is to sample a small set of records over which evaluating aggregate queries can be done both efficiently and accurately .
result: extract = Extract([Triple(Other_scientific_term("aggregate queries"), Rel("Evaluate_for"), Other_scientific_term("records"))])
pred: [['aggregate queries', 'Evaluate_for', 'records']]
trues: [['aggregate queries', 'Evaluate_for', 'records']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.382830626450116, 0.40048543689320387, 0.39145907473309616]
text: Online action detection goes one step further and is more challenging , which identifies the action type and localizes the action positions on the fly from the untrimmed stream .
result: extract = Extract([Triple(Task("action detection"), Rel("Used_for"), Task("action type")), Triple(Task("action detection"), Rel("Used_for"), Other_scientific_term("action positions")), Triple(Task("action detection"), Rel("Used_for"), Other_scientific_term("untrimmed stream"))])
pred: [['action detection', 'Used_for', 'action type'], ['action detection', 'Used_for', 'action positions'], ['action detection', 'Used_for', 'untrimmed stream']]
trues: [['Online action detection', 'Used_for', 'action type'], ['Online action detection', 'Used_for', 'action positions'], ['action type', 'Conjunction', 'action positions'], ['untrimmed stream', 'Used_for', 'Online action detection']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38018433179723504, 0.39663461538461536, 0.38823529411764707]
text: To a large extent , these statistics reflect semantic constraints and thus are used to disambiguate anaphora references and syntactic ambiguities .
result: extract = Extract([Triple(Other_scientific_term("semantic constraints"), Rel("Used_for"), Other_scientific_term("anaphora references")), Triple(Other_scientific_term("semantic constraints"), Rel("Used_for"), Other_scientific_term("syntactic ambiguities")), Triple(Other_scientific_term("anaphora references"), Rel("Conjunction"), Other_scientific_term("syntactic ambiguities"))])
pred: [['semantic constraints', 'Used_for', 'anaphora references'], ['semantic constraints', 'Used_for', 'syntactic ambiguities'], ['anaphora references', 'Conjunction', 'syntactic ambiguities']]
trues: [['semantic constraints', 'Used_for', 'anaphora references'], ['semantic constraints', 'Used_for', 'syntactic ambiguities'], ['anaphora references', 'Conjunction', 'syntactic ambiguities']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.38443935926773454, 0.4009546539379475, 0.3925233644859813]
text: We investigate several voting - and arbiter-based combination strategies over a diverse pool of unsupervised WSD systems .
result: extract = Extract([Triple(Method("voting - and arbiter-based combination strategies"), Rel("Used_for"), Method("unsupervised WSD systems"))])
pred: [['voting - and arbiter-based combination strategies', 'Used_for', 'unsupervised WSD systems']]
trues: [['voting - and arbiter-based combination strategies', 'Used_for', 'unsupervised WSD systems']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3858447488584475, 0.4023809523809524, 0.3939393939393939]
text: Furthermore , we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems .
result: extract = Extract([Triple(Metric("parser evaluation methods"), Rel("Used_for"), Task("automatically evaluating the summarization quality of sentence condensation systems"))])
pred: [['parser evaluation methods', 'Used_for', 'automatically evaluating the summarization quality of sentence condensation systems']]
trues: [['parser evaluation methods', 'Evaluate_for', 'summarization quality'], ['summarization quality', 'Evaluate_for', 'sentence condensation systems']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38496583143507973, 0.4004739336492891, 0.3925667828106853]
text: It is particularly valuable to empirical MT research .
result: extract = Extract([Triple(Generic("It"), Rel("Used_for"), Task("empirical MT research"))])
pred: [['It', 'Used_for', 'empirical MT research']]
trues: [['It', 'Used_for', 'empirical MT research']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.38636363636363635, 0.40189125295508277, 0.39397450753186564]
text: Under this framework , a joint source-channel transliteration model , also called n-gram transliteration model ( n-gram TM ) , is further proposed to model the transliteration process .
result: extract = Extract([Triple(Method("joint source-channel transliteration model"), Rel("Hyponym_of"), Method("n-gram transliteration model ( n-gram TM )"))])
pred: [['joint source-channel transliteration model', 'Hyponym_of', 'n-gram transliteration model ( n-gram TM )']]
trues: [['framework', 'Used_for', 'joint source-channel transliteration model'], ['n-gram transliteration model ( n-gram TM )', 'Used_for', 'transliteration process']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3854875283446712, 0.4, 0.3926096997690532]
text: It is based on a weakly supervised dependency parser that can model speech syntax without relying on any annotated training corpus .
result: extract = Extract([Triple(Method("weakly supervised dependency parser"), Rel("Used_for"), Generic("It")), Triple(Material("annotated training corpus"), Rel("Used_for"), Method("weakly supervised dependency parser"))])
pred: [['weakly supervised dependency parser', 'Used_for', 'It'], ['annotated training corpus', 'Used_for', 'weakly supervised dependency parser']]
trues: [['weakly supervised dependency parser', 'Used_for', 'speech syntax']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3837471783295711, 0.39906103286384975, 0.3912543153049482]
text: A mixed-signal paradigm is presented for high-resolution parallel inner-product computation in very high dimensions , suitable for efficient implementation of kernels in image processing .
result: extract = Extract([Triple(Method("mixed-signal paradigm"), Rel("Used_for"), Task("high-resolution parallel inner-product computation")), Triple(Task("high-resolution parallel inner-product computation"), Rel("Used_for"), Task("kernels")), Triple(Task("kernels"), Rel("Used_for"), Task("image processing"))])
pred: [['mixed-signal paradigm', 'Used_for', 'high-resolution parallel inner-product computation'], ['high-resolution parallel inner-product computation', 'Used_for', 'kernels'], ['kernels', 'Used_for', 'image processing']]
trues: [['mixed-signal paradigm', 'Used_for', 'high-resolution parallel inner-product computation'], ['mixed-signal paradigm', 'Used_for', 'kernels'], ['kernels', 'Used_for', 'image processing']]
metrics_sample: [0.6666666666666666, 0.6666666666666666, 0.6666666666666666]
metrics_current: [0.38565022421524664, 0.40093240093240096, 0.3931428571428572]
text: We present a new part-of-speech tagger that demonstrates the following ideas : ( i ) explicit use of both preceding and following tag contexts via a dependency network representation , ( ii ) broad use of lexical features , including jointly conditioning on multiple consecutive words , ( iii ) effective use of priors in conditional loglinear models , and ( iv ) fine-grained modeling of unknown word features .
result: extract = Extract([Triple(Method("dependency network representation"), Rel("Used_for"), Other_scientific_term("explicit use of both preceding and following tag contexts")), Triple(Other_scientific_term("lexical features"), Rel("Used_for"), Generic("ideas")), Triple(Other_scientific_term("lexical features"), Rel("Conjunction"), Other_scientific_term("unknown word features")), Triple(Other_scientific_term("unknown word features"), Rel("Used_for"), Generic("ideas"))])
pred: [['dependency network representation', 'Used_for', 'explicit use of both preceding and following tag contexts'], ['lexical features', 'Used_for', 'ideas'], ['lexical features', 'Conjunction', 'unknown word features'], ['unknown word features', 'Used_for', 'ideas']]
trues: [['tag contexts', 'Used_for', 'part-of-speech tagger'], ['dependency network representation', 'Used_for', 'tag contexts'], ['lexical features', 'Used_for', 'part-of-speech tagger'], ['priors in conditional loglinear models', 'Used_for', 'part-of-speech tagger'], ['fine-grained modeling of unknown word features', 'Used_for', 'part-of-speech tagger']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38222222222222224, 0.39631336405529954, 0.3891402714932126]
text: Moreover , a cost zero solution always exists and can only be excluded by hard budget constraints .
result: extract = Extract([Triple(Other_scientific_term("cost zero solution"), Rel("Used_for"), Other_scientific_term("hard budget constraints"))])
pred: [['cost zero solution', 'Used_for', 'hard budget constraints']]
trues: [['hard budget constraints', 'Used_for', 'cost zero solution']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38137472283813745, 0.3954022988505747, 0.38826185101580135]
text: Amorph recognizes NE items in two stages : dictionary lookup and rule application .
result: extract = Extract([Triple(Method("dictionary lookup"), Rel("Part_of"), Method("Amorph recognizes NE items")), Triple(Method("dictionary lookup"), Rel("Conjunction"), Method("rule application")), Triple(Method("rule application"), Rel("Part_of"), Method("Amorph recognizes NE items"))])
pred: [['dictionary lookup', 'Part_of', 'Amorph recognizes NE items'], ['dictionary lookup', 'Conjunction', 'rule application'], ['rule application', 'Part_of', 'Amorph recognizes NE items']]
trues: [['Amorph', 'Used_for', 'NE items'], ['dictionary lookup', 'Part_of', 'Amorph'], ['dictionary lookup', 'Conjunction', 'rule application'], ['rule application', 'Part_of', 'Amorph']]
metrics_sample: [0.3333333333333333, 0.25, 0.28571428571428575]
metrics_current: [0.381057268722467, 0.3940774487471526, 0.38745800671892494]
text: Compared to the exhaustive procedure that thus far is the state-of-the-art for cascade evaluation , the proposed method requires fewer evaluations of the classifier functions , thereby speeding up the search .
result: extract = Extract([Triple(Method("exhaustive procedure"), Rel("Compare"), Generic("method")), Triple(Method("exhaustive procedure"), Rel("Used_for"), Task("cascade evaluation")), Triple(Generic("method"), Rel("Used_for"), Task("cascade evaluation"))])
pred: [['exhaustive procedure', 'Compare', 'method'], ['exhaustive procedure', 'Used_for', 'cascade evaluation'], ['method', 'Used_for', 'cascade evaluation']]
trues: [['exhaustive procedure', 'Used_for', 'cascade evaluation'], ['exhaustive procedure', 'Compare', 'method'], ['method', 'Used_for', 'search']]
metrics_sample: [0.6666666666666666, 0.6666666666666666, 0.6666666666666666]
metrics_current: [0.38293216630196936, 0.39592760180995473, 0.38932146829810904]
text: Multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide matching and reconstruction , allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .
result: extract = Extract([Triple(Other_scientific_term("normalized representation of their appearance"), Rel("Used_for"), Other_scientific_term("Multi-view constraints")), Triple(Other_scientific_term("normalized representation of their appearance"), Rel("Conjunction"), Other_scientific_term("affine and Euclidean models")), Triple(Other_scientific_term("affine and Euclidean models"), Rel("Used_for"), Other_scientific_term("recognition"))])
pred: [['normalized representation of their appearance', 'Used_for', 'Multi-view constraints'], ['normalized representation of their appearance', 'Conjunction', 'affine and Euclidean models'], ['affine and Euclidean models', 'Used_for', 'recognition']]
trues: [['Multi-view constraints', 'Conjunction', 'normalized representation'], ['Multi-view constraints', 'Used_for', 'matching'], ['Multi-view constraints', 'Used_for', 'reconstruction'], ['normalized representation', 'Used_for', 'matching'], ['normalized representation', 'Used_for', 'reconstruction'], ['matching', 'Conjunction', 'reconstruction'], ['images', 'Used_for', 'acquisition of true three-dimensional affine and Euclidean models']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3804347826086957, 0.3897550111358575, 0.38503850385038507]
text: This paper investigates the utility of applying standard MT evaluation methods ( BLEU , NIST , WER and PER ) to building classifiers to predict semantic equivalence and entailment .
result: extract = Extract([Triple(Metric("BLEU"), Rel("Hyponym_of"), Method("MT evaluation methods")), Triple(Metric("BLEU"), Rel("Conjunction"), Metric("NIST")), Triple(Metric("NIST"), Rel("Hyponym_of"), Method("MT evaluation methods")), Triple(Metric("NIST"), Rel("Conjunction"), Metric("WER")), Triple(Metric("WER"), Rel("Hyponym_of"), Method("MT evaluation methods")), Triple(Metric("WER"), Rel("Conjunction"), Metric("PER")), Triple(Metric("PER"), Rel("Hyponym_of"), Method("MT evaluation methods")), Triple(Task("building classifiers"), Rel("Used_for"), Other_scientific_term("semantic equivalence")), Triple(Task("building classifiers"), Rel("Used_for"), Other_scientific_term("entailment")), Triple(Other_scientific_term("semantic equivalence"), Rel("Conjunction"), Other_scientific_term("entailment"))])
pred: [['BLEU', 'Hyponym_of', 'MT evaluation methods'], ['BLEU', 'Conjunction', 'NIST'], ['NIST', 'Hyponym_of', 'MT evaluation methods'], ['NIST', 'Conjunction', 'WER'], ['WER', 'Hyponym_of', 'MT evaluation methods'], ['WER', 'Conjunction', 'PER'], ['PER', 'Hyponym_of', 'MT evaluation methods'], ['building classifiers', 'Used_for', 'semantic equivalence'], ['building classifiers', 'Used_for', 'entailment'], ['semantic equivalence', 'Conjunction', 'entailment']]
trues: [['MT evaluation methods', 'Used_for', 'classifiers'], ['BLEU', 'Hyponym_of', 'MT evaluation methods'], ['BLEU', 'Conjunction', 'NIST'], ['NIST', 'Hyponym_of', 'MT evaluation methods'], ['NIST', 'Conjunction', 'WER'], ['WER', 'Hyponym_of', 'MT evaluation methods'], ['WER', 'Conjunction', 'PER'], ['PER', 'Hyponym_of', 'MT evaluation methods'], ['classifiers', 'Used_for', 'semantic equivalence'], ['classifiers', 'Used_for', 'entailment'], ['semantic equivalence', 'Conjunction', 'entailment']]
metrics_sample: [0.8, 0.7272727272727273, 0.761904761904762]
metrics_current: [0.3893617021276596, 0.3978260869565217, 0.3935483870967742]
text: In order to perform an exhaustive comparison , we also evaluate a hand-crafted template-based generation component , two rule-based sentence planners , and two baseline sentence planners .
result: extract = Extract([Triple(Method("hand-crafted template-based generation component"), Rel("Hyponym_of"), Generic("baseline sentence planners")), Triple(Method("rule-based sentence planners"), Rel("Conjunction"), Method("baseline sentence planners")), Triple(Method("rule-based sentence planners"), Rel("Hyponym_of"), Generic("baseline sentence planners"))])
pred: [['hand-crafted template-based generation component', 'Hyponym_of', 'baseline sentence planners'], ['rule-based sentence planners', 'Conjunction', 'baseline sentence planners'], ['rule-based sentence planners', 'Hyponym_of', 'baseline sentence planners']]
trues: [['hand-crafted template-based generation component', 'Conjunction', 'rule-based sentence planners'], ['rule-based sentence planners', 'Conjunction', 'baseline sentence planners']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.3890063424947146, 0.39826839826839827, 0.3935828877005348]
text: On this challenging data set our system consistently demonstrated a nearly perfect recognition rate ( over 99.7 % on all three databases ) , significantly out-performing state-of-the-art commercial software and methods from the literature .
result: extract = Extract([Triple(Generic("system"), Rel("Compare"), Method("commercial software")), Triple(Generic("system"), Rel("Compare"), Method("methods")), Triple(Method("commercial software"), Rel("Conjunction"), Method("methods"))])
pred: [['system', 'Compare', 'commercial software'], ['system', 'Compare', 'methods'], ['commercial software', 'Conjunction', 'methods']]
trues: [['data set', 'Evaluate_for', 'system'], ['system', 'Compare', 'commercial software'], ['system', 'Compare', 'methods'], ['recognition rate', 'Evaluate_for', 'system'], ['commercial software', 'Conjunction', 'methods']]
metrics_sample: [1.0, 0.6, 0.7499999999999999]
metrics_current: [0.39285714285714285, 0.4004282655246253, 0.3966065747613998]
text: This poster paper describes a full scale two-level morphological description ( Karttunen , 1983 ; Koskenniemi , 1983 ) of Turkish word structures .
result: extract = Extract([Triple(Method("two-level morphological description"), Rel("Used_for"), Other_scientific_term("Turkish word structures")), Triple(Other_scientific_term("Karttunen"), Rel("Hyponym_of"), Method("two-level morphological description")), Triple(Other_scientific_term("Karttunen"), Rel("Conjunction"), Other_scientific_term("Koskenniemi")), Triple(Other_scientific_term("Koskenniemi"), Rel("Hyponym_of"), Method("two-level morphological description"))])
pred: [['two-level morphological description', 'Used_for', 'Turkish word structures'], ['Karttunen', 'Hyponym_of', 'two-level morphological description'], ['Karttunen', 'Conjunction', 'Koskenniemi'], ['Koskenniemi', 'Hyponym_of', 'two-level morphological description']]
trues: [['full scale two-level morphological description', 'Used_for', 'Turkish word structures']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38958333333333334, 0.3995726495726496, 0.3945147679324895]
text: Given the video streams acquired by two cameras placed on either side of a computer monitor , the proposed algorithm synthesises images from a virtual camera in arbitrary position ( typically located within the monitor ) to facilitate eye contact .
result: extract = Extract([Triple(Material("video streams"), Rel("Used_for"), Generic("algorithm")), Triple(Other_scientific_term("virtual camera"), Rel("Used_for"), Generic("algorithm")), Triple(Other_scientific_term("eye contact"), Rel("Feature_of"), Other_scientific_term("virtual camera"))])
pred: [['video streams', 'Used_for', 'algorithm'], ['virtual camera', 'Used_for', 'algorithm'], ['eye contact', 'Feature_of', 'virtual camera']]
trues: [['cameras', 'Used_for', 'video streams'], ['algorithm', 'Used_for', 'eye contact'], ['virtual camera', 'Used_for', 'images'], ['arbitrary position', 'Feature_of', 'virtual camera']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38716356107660455, 0.3961864406779661, 0.39162303664921466]
text: Our approach is based on the iterative deformation of a 3 -- D surface mesh to minimize an objective function .
result: extract = Extract([Triple(Method("iterative deformation"), Rel("Used_for"), Other_scientific_term("3 -- D surface mesh")), Triple(Other_scientific_term("3 -- D surface mesh"), Rel("Used_for"), Generic("approach")), Triple(Other_scientific_term("3 -- D surface mesh"), Rel("Used_for"), Other_scientific_term("objective function"))])
pred: [['iterative deformation', 'Used_for', '3 -- D surface mesh'], ['3 -- D surface mesh', 'Used_for', 'approach'], ['3 -- D surface mesh', 'Used_for', 'objective function']]
trues: [['iterative deformation of a 3 -- D surface mesh', 'Used_for', 'approach'], ['iterative deformation of a 3 -- D surface mesh', 'Used_for', 'objective function']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38477366255144035, 0.39451476793248946, 0.3895833333333334]
text: The system incorporates a decision-tree classifier for 30 scf types which tests for the presence of grammatical relations ( grs ) in the output of a robust statistical parser .
result: extract = Extract([Triple(Method("decision-tree classifier"), Rel("Used_for"), Generic("system")), Triple(Other_scientific_term("scf types"), Rel("Used_for"), Method("decision-tree classifier")), Triple(Other_scientific_term("scf types"), Rel("Conjunction"), Other_scientific_term("grammatical relations ( grs )")), Triple(Other_scientific_term("grammatical relations ( grs )"), Rel("Used_for"), Method("decision-tree classifier")), Triple(Method("robust statistical parser"), Rel("Used_for"), Other_scientific_term("grammatical relations ( grs )"))])
pred: [['decision-tree classifier', 'Used_for', 'system'], ['scf types', 'Used_for', 'decision-tree classifier'], ['scf types', 'Conjunction', 'grammatical relations ( grs )'], ['grammatical relations ( grs )', 'Used_for', 'decision-tree classifier'], ['robust statistical parser', 'Used_for', 'grammatical relations ( grs )']]
trues: [['decision-tree classifier', 'Part_of', 'system'], ['decision-tree classifier', 'Used_for', 'grammatical relations ( grs )']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38085539714867617, 0.39285714285714285, 0.3867631851085832]
text: Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield significantly better results when compared with state-of-the-art .
result: extract = Extract([Triple(Material("SemCor and Senseval-3 data sets"), Rel("Evaluate_for"), Generic("ensembles")), Triple(Generic("ensembles"), Rel("Compare"), Generic("state-of-the-art"))])
pred: [['SemCor and Senseval-3 data sets', 'Evaluate_for', 'ensembles'], ['ensembles', 'Compare', 'state-of-the-art']]
trues: [['SemCor and Senseval-3 data sets', 'Evaluate_for', 'ensembles'], ['SemCor and Senseval-3 data sets', 'Evaluate_for', 'state-of-the-art']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3813387423935091, 0.39330543933054396, 0.3872296601441813]
text: In previous work , we have developed hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers that integrate textual and prosodic knowledge sources for detecting sentence boundaries .
result: extract = Extract([Triple(Method("hidden Markov model ( HMM )"), Rel("Conjunction"), Method("maximum entropy ( Maxent ) classifiers")), Triple(Other_scientific_term("textual and prosodic knowledge sources"), Rel("Used_for"), Method("hidden Markov model ( HMM )")), Triple(Other_scientific_term("textual and prosodic knowledge sources"), Rel("Used_for"), Method("maximum entropy ( Maxent ) classifiers")), Triple(Method("hidden Markov model ( HMM )"), Rel("Used_for"), Task("detecting sentence boundaries")), Triple(Method("maximum entropy ( Maxent ) classifiers"), Rel("Used_for"), Task("detecting sentence boundaries"))])
pred: [['hidden Markov model ( HMM )', 'Conjunction', 'maximum entropy ( Maxent ) classifiers'], ['textual and prosodic knowledge sources', 'Used_for', 'hidden Markov model ( HMM )'], ['textual and prosodic knowledge sources', 'Used_for', 'maximum entropy ( Maxent ) classifiers'], ['hidden Markov model ( HMM )', 'Used_for', 'detecting sentence boundaries'], ['maximum entropy ( Maxent ) classifiers', 'Used_for', 'detecting sentence boundaries']]
trues: [['hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers', 'Used_for', 'detecting sentence boundaries'], ['textual and prosodic knowledge sources', 'Used_for', 'hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37751004016064255, 0.39166666666666666, 0.38445807770961143]
text: Our system incorporates a linguistic parser/generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .
result: extract = Extract([Triple(Method("maximum-entropy model"), Rel("Used_for"), Other_scientific_term("stochastic output selection"))])
pred: [['maximum-entropy model', 'Used_for', 'stochastic output selection']]
trues: [['linguistic parser/generator', 'Part_of', 'system'], ['linguistic parser/generator', 'Used_for', 'LFG'], ['linguistic parser/generator', 'Conjunction', 'transfer component'], ['transfer component', 'Part_of', 'system'], ['transfer component', 'Used_for', 'parse reduction'], ['transfer component', 'Conjunction', 'maximum-entropy model'], ['packed parse forests', 'Used_for', 'parse reduction'], ['maximum-entropy model', 'Part_of', 'system'], ['maximum-entropy model', 'Used_for', 'stochastic output selection']]
metrics_sample: [1.0, 0.1111111111111111, 0.19999999999999998]
metrics_current: [0.3787575150300601, 0.38650306748466257, 0.38259109311740885]
text: The objects can be complex in that they may be composed of multiple layers with different refractive indices .
result: extract = Extract([Triple(Other_scientific_term("layers"), Rel("Part_of"), Other_scientific_term("objects"))])
pred: [['layers', 'Part_of', 'objects']]
trues: [['multiple layers', 'Part_of', 'they'], ['refractive indices', 'Feature_of', 'multiple layers']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.378, 0.384928716904277, 0.3814328960645812]
text: The subjects were given three minutes per extract to determine whether they believed the sample output to be an expert human translation or a machine translation .
result: extract = Extract([Triple(Other_scientific_term("expert human translation"), Rel("Hyponym_of"), Other_scientific_term("sample output")), Triple(Other_scientific_term("expert human translation"), Rel("Conjunction"), Other_scientific_term("machine translation")), Triple(Other_scientific_term("machine translation"), Rel("Hyponym_of"), Other_scientific_term("sample output"))])
pred: [['expert human translation', 'Hyponym_of', 'sample output'], ['expert human translation', 'Conjunction', 'machine translation'], ['machine translation', 'Hyponym_of', 'sample output']]
trues: [['expert human translation', 'Compare', 'machine translation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3757455268389662, 0.38414634146341464, 0.37989949748743723]
text: Bayesian inference then samples the rules , disambiguating and combining them to create complex tree structures that maximize a discriminative model 's posterior on a target unlabeled corpus .
result: extract = Extract([Triple(Method("Bayesian inference"), Rel("Used_for"), Other_scientific_term("rules")), Triple(Other_scientific_term("rules"), Rel("Used_for"), Other_scientific_term("tree structures")), Triple(Other_scientific_term("discriminative model"), Rel("Used_for"), Other_scientific_term("tree structures"))])
pred: [['Bayesian inference', 'Used_for', 'rules'], ['rules', 'Used_for', 'tree structures'], ['discriminative model', 'Used_for', 'tree structures']]
trues: [['Bayesian inference', 'Used_for', 'rules'], ['them', 'Used_for', 'complex tree structures'], ['complex tree structures', 'Used_for', "discriminative model 's posterior"], ['unlabeled corpus', 'Used_for', "discriminative model 's posterior"]]
metrics_sample: [0.3333333333333333, 0.25, 0.28571428571428575]
metrics_current: [0.37549407114624506, 0.38306451612903225, 0.37924151696606784]
text: Our combination methods rely on predominant senses which are derived automatically from raw text .
result: extract = Extract([Triple(Other_scientific_term("predominant senses"), Rel("Used_for"), Generic("combination methods")), Triple(Material("raw text"), Rel("Used_for"), Other_scientific_term("predominant senses"))])
pred: [['predominant senses', 'Used_for', 'combination methods'], ['raw text', 'Used_for', 'predominant senses']]
trues: [['predominant senses', 'Used_for', 'combination methods'], ['raw text', 'Used_for', 'predominant senses']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3779527559055118, 0.3855421686746988, 0.3817097415506958]
text: A model-based approach to on-line cursive handwriting analysis and recognition is presented and evaluated .
result: extract = Extract([Triple(Method("model-based approach"), Rel("Used_for"), Task("on-line cursive handwriting analysis and recognition"))])
pred: [['model-based approach', 'Used_for', 'on-line cursive handwriting analysis and recognition']]
trues: [['model-based approach', 'Used_for', 'on-line cursive handwriting analysis and recognition']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3791748526522593, 0.3867735470941884, 0.38293650793650796]
text: While previous approaches relied on geometric , appearance , or correlation-based information for establishing correspondences between static cameras , they each have well-known limitations and are not extendable to wide-area settings with PTZ cameras .
result: extract = Extract([Triple(Other_scientific_term("PTZ cameras"), Rel("Used_for"), Other_scientific_term("wide-area settings"))])
pred: [['PTZ cameras', 'Used_for', 'wide-area settings']]
trues: [['geometric , appearance , or correlation-based information', 'Used_for', 'approaches']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3784313725490196, 0.386, 0.38217821782178213]
text: Thus , the limitations of the automatic metrics used within MT are also discussed in this regard .
result: extract = Extract([Triple(Metric("automatic metrics"), Rel("Used_for"), Method("MT"))])
pred: [['automatic metrics', 'Used_for', 'MT']]
trues: [['automatic metrics', 'Evaluate_for', 'MT']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3776908023483366, 0.3852295409181637, 0.3814229249011858]
text: We propose a corpus-based method ( Biber ,1993 ; Nagao ,1993 ; Smadja ,1993 ) which generates Noun Classifier Associations ( NCA ) to overcome the problems in classifier assignment and semantic construction of noun phrase .
result: extract = Extract([Triple(Method("corpus-based method"), Rel("Used_for"), Other_scientific_term("Noun Classifier Associations ( NCA )")), Triple(Other_scientific_term("Noun Classifier Associations ( NCA )"), Rel("Used_for"), Task("classifier assignment")), Triple(Other_scientific_term("Noun Classifier Associations ( NCA )"), Rel("Used_for"), Task("semantic construction of noun phrase"))])
pred: [['corpus-based method', 'Used_for', 'Noun Classifier Associations ( NCA )'], ['Noun Classifier Associations ( NCA )', 'Used_for', 'classifier assignment'], ['Noun Classifier Associations ( NCA )', 'Used_for', 'semantic construction of noun phrase']]
trues: [['corpus-based method', 'Used_for', 'Noun Classifier Associations ( NCA )'], ['corpus-based method', 'Used_for', 'classifier assignment'], ['corpus-based method', 'Used_for', 'semantic construction of noun phrase'], ['Noun Classifier Associations ( NCA )', 'Used_for', 'classifier assignment'], ['Noun Classifier Associations ( NCA )', 'Used_for', 'semantic construction of noun phrase'], ['classifier assignment', 'Conjunction', 'semantic construction of noun phrase']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.38132295719844356, 0.3865877712031558, 0.3839373163565132]
text: Overall summarization quality of the proposed system is state-of-the-art , with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator .
result: extract = Extract([Triple(Metric("summarization quality"), Rel("Evaluate_for"), Generic("system")), Triple(Method("constraint-based parser/generator"), Rel("Used_for"), Generic("system"))])
pred: [['summarization quality', 'Evaluate_for', 'system'], ['constraint-based parser/generator', 'Used_for', 'system']]
trues: [['summarization quality', 'Evaluate_for', 'system'], ['grammaticality', 'Evaluate_for', 'system'], ['constraint-based parser/generator', 'Used_for', 'system']]
metrics_sample: [1.0, 0.6666666666666666, 0.8]
metrics_current: [0.38372093023255816, 0.38823529411764707, 0.38596491228070184]
text: In this work , we study how Convolutional Neural Networks ( CNN ) architectures can be adapted to the task of simultaneous object recognition and pose estimation .
result: extract = Extract([Triple(Method("Convolutional Neural Networks ( CNN ) architectures"), Rel("Used_for"), Task("simultaneous object recognition and pose estimation"))])
pred: [['Convolutional Neural Networks ( CNN ) architectures', 'Used_for', 'simultaneous object recognition and pose estimation']]
trues: [['Convolutional Neural Networks ( CNN ) architectures', 'Used_for', 'object recognition'], ['Convolutional Neural Networks ( CNN ) architectures', 'Used_for', 'pose estimation'], ['object recognition', 'Conjunction', 'pose estimation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3829787234042553, 0.38596491228070173, 0.3844660194174757]
text: One remarkable feature of this model is that it has allowed us to derive an efficient algorithm based on powerful statistical methods for estimating the Fujisaki-model parameters from raw F0 contours .
result: extract = Extract([Triple(Method("statistical methods"), Rel("Used_for"), Generic("algorithm")), Triple(Method("statistical methods"), Rel("Used_for"), Other_scientific_term("Fujisaki-model parameters")), Triple(Other_scientific_term("raw F0 contours"), Rel("Used_for"), Method("statistical methods"))])
pred: [['statistical methods', 'Used_for', 'algorithm'], ['statistical methods', 'Used_for', 'Fujisaki-model parameters'], ['raw F0 contours', 'Used_for', 'statistical methods']]
trues: [['remarkable feature', 'Feature_of', 'model'], ['algorithm', 'Used_for', 'Fujisaki-model parameters'], ['statistical methods', 'Used_for', 'algorithm'], ['raw F0 contours', 'Used_for', 'Fujisaki-model parameters']]
metrics_sample: [0.3333333333333333, 0.25, 0.28571428571428575]
metrics_current: [0.38269230769230766, 0.3849129593810445, 0.3837994214079074]
text: Furthermore , we show how the recently developed efficient subwindow search ( ESS ) procedure [ 11 ] can be integrated into the last stage of our method .
result: extract = Extract([Triple(Method("efficient subwindow search ( ESS ) procedure"), Rel("Used_for"), Generic("method"))])
pred: [['efficient subwindow search ( ESS ) procedure', 'Used_for', 'method']]
trues: [['subwindow search ( ESS ) procedure', 'Part_of', 'method']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.381957773512476, 0.3841698841698842, 0.38306063522617906]
text: In particular there are three areas of novelty : ( i ) we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; ( ii ) we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and ( iii ) we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .
result: extract = Extract([Triple(Method("photometric model of image formation"), Rel("Conjunction"), Method("statistical model of generic face appearance variation")), Triple(Method("statistical model of generic face appearance variation"), Rel("Used_for"), Other_scientific_term("unseen head poses")), Triple(Method("robust same-identity likelihood"), Rel("Used_for"), Other_scientific_term("invariance to unseen head poses"))])
pred: [['photometric model of image formation', 'Conjunction', 'statistical model of generic face appearance variation'], ['statistical model of generic face appearance variation', 'Used_for', 'unseen head poses'], ['robust same-identity likelihood', 'Used_for', 'invariance to unseen head poses']]
trues: [['photometric model', 'Used_for', 'image formation'], ['photometric model', 'Conjunction', 'statistical model'], ['statistical model', 'Used_for', 'generic face appearance variation'], ['statistical model', 'Used_for', 'extreme illumination changes'], ['smoothness', 'Feature_of', 'geodesically local appearance manifold structure'], ['geodesically local appearance manifold structure', 'Conjunction', 'robust same-identity likelihood'], ['robustness', 'Evaluate_for', "video sequence '' reillumination '' algorithm"], ['face motion patterns', 'Feature_of', 'robustness'], ['face motion patterns', 'Part_of', 'video']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3797709923664122, 0.3776091081593928, 0.37868696479543296]
text: Towards deep analysis of compositional classes of paraphrases , we have examined a class-oriented framework for collecting paraphrase examples , in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement .
result: extract = Extract([Triple(Method("class-oriented framework"), Rel("Used_for"), Task("deep analysis of compositional classes of paraphrases")), Triple(Method("automatic candidate generation"), Rel("Used_for"), Method("class-oriented framework")), Triple(Method("automatic candidate generation"), Rel("Conjunction"), Other_scientific_term("manual judgement")), Triple(Other_scientific_term("manual judgement"), Rel("Used_for"), Method("class-oriented framework"))])
pred: [['class-oriented framework', 'Used_for', 'deep analysis of compositional classes of paraphrases'], ['automatic candidate generation', 'Used_for', 'class-oriented framework'], ['automatic candidate generation', 'Conjunction', 'manual judgement'], ['manual judgement', 'Used_for', 'class-oriented framework']]
trues: [['class-oriented framework', 'Used_for', 'compositional classes of paraphrases'], ['class-oriented framework', 'Used_for', 'paraphrase examples'], ['automatic candidate generation', 'Used_for', 'sentential paraphrases'], ['automatic candidate generation', 'Conjunction', 'manual judgement'], ['manual judgement', 'Used_for', 'sentential paraphrases']]
metrics_sample: [0.25, 0.2, 0.22222222222222224]
metrics_current: [0.3787878787878788, 0.37593984962406013, 0.3773584905660377]
text: The resulting logical expression is then transformed by a planning algorithm into efficient Prolog , cf. query optimisation in a relational database .
result: extract = Extract([Triple(Method("planning algorithm"), Rel("Used_for"), Other_scientific_term("logical expression")), Triple(Method("Prolog"), Rel("Used_for"), Other_scientific_term("logical expression")), Triple(Task("query optimisation"), Rel("Used_for"), Method("Prolog")), Triple(Material("relational database"), Rel("Used_for"), Task("query optimisation"))])
pred: [['planning algorithm', 'Used_for', 'logical expression'], ['Prolog', 'Used_for', 'logical expression'], ['query optimisation', 'Used_for', 'Prolog'], ['relational database', 'Used_for', 'query optimisation']]
trues: [['planning algorithm', 'Used_for', 'logical expression'], ['relational database', 'Used_for', 'query optimisation']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.37969924812030076, 0.3782771535580524, 0.3789868667917448]
text: The NCA is created statistically from a large corpus and recomposed under concept hierarchy constraints and frequency of occurrences .
result: extract = Extract([Triple(Other_scientific_term("concept hierarchy constraints"), Rel("Conjunction"), Other_scientific_term("frequency of occurrences"))])
pred: [['concept hierarchy constraints', 'Conjunction', 'frequency of occurrences']]
trues: [['concept hierarchy constraints', 'Used_for', 'NCA'], ['frequency of occurrences', 'Used_for', 'NCA']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3789868667917448, 0.376865671641791, 0.37792329279700654]
text: The psycholinguistic literature provides evidence for syntactic priming , i.e. , the tendency to repeat structures .
result: extract = Extract([Triple(Method("psycholinguistic literature"), Rel("Used_for"), Other_scientific_term("syntactic priming"))])
pred: [['psycholinguistic literature', 'Used_for', 'syntactic priming']]
trues: [['psycholinguistic literature', 'Used_for', 'syntactic priming']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3801498127340824, 0.3780260707635009, 0.3790849673202614]
text: Our results show that MT evaluation techniques are able to produce useful features for paraphrase classification and to a lesser extent entailment .
result: extract = Extract([Triple(Metric("MT evaluation techniques"), Rel("Used_for"), Task("paraphrase classification")), Triple(Metric("MT evaluation techniques"), Rel("Used_for"), Task("entailment")), Triple(Task("paraphrase classification"), Rel("Conjunction"), Task("entailment"))])
pred: [['MT evaluation techniques', 'Used_for', 'paraphrase classification'], ['MT evaluation techniques', 'Used_for', 'entailment'], ['paraphrase classification', 'Conjunction', 'entailment']]
trues: [['MT evaluation techniques', 'Used_for', 'features'], ['MT evaluation techniques', 'Used_for', 'paraphrase classification'], ['MT evaluation techniques', 'Used_for', 'entailment'], ['paraphrase classification', 'Conjunction', 'entailment']]
metrics_sample: [1.0, 0.75, 0.8571428571428571]
metrics_current: [0.38361266294227186, 0.3807763401109057, 0.3821892393320965]
text: We suggest a new goal and evaluation criterion for word similarity measures .
result: extract = Extract([Triple(Metric("goal and evaluation criterion"), Rel("Used_for"), Method("word similarity measures"))])
pred: [['goal and evaluation criterion', 'Used_for', 'word similarity measures']]
trues: [['evaluation criterion', 'Used_for', 'word similarity measures']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3828996282527881, 0.3800738007380074, 0.38148148148148153]
text: We have implemented a restricted domain parser called Plume .
result: extract = Extract([Triple(Method("restricted domain parser"), Rel("Used_for"), Method("Plume"))])
pred: [['restricted domain parser', 'Used_for', 'Plume']]
trues: [['Plume', 'Hyponym_of', 'restricted domain parser']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3821892393320965, 0.37937384898710863, 0.3807763401109057]
text: We test our algorithm on the detection task and the viewpoint classification task by using '' car '' category from both the Savarese et al. 2007 and PASCAL VOC 2006 datasets .
result: extract = Extract([Triple(Generic("algorithm"), Rel("Used_for"), Task("detection task")), Triple(Generic("algorithm"), Rel("Used_for"), Task("viewpoint classification task")), Triple(Task("detection task"), Rel("Conjunction"), Task("viewpoint classification task")), Triple(Material("Savarese et al. 2007 and PASCAL VOC 2006 datasets"), Rel("Used_for"), Other_scientific_term("'' car '' category"))])
pred: [['algorithm', 'Used_for', 'detection task'], ['algorithm', 'Used_for', 'viewpoint classification task'], ['detection task', 'Conjunction', 'viewpoint classification task'], ['Savarese et al. 2007 and PASCAL VOC 2006 datasets', 'Used_for', "'' car '' category"]]
trues: [['algorithm', 'Used_for', 'detection task'], ['algorithm', 'Used_for', 'viewpoint classification task'], ['detection task', 'Conjunction', 'viewpoint classification task'], ['PASCAL VOC 2006 datasets', 'Evaluate_for', 'algorithm']]
metrics_sample: [0.75, 0.75, 0.75]
metrics_current: [0.3848987108655617, 0.3820840950639854, 0.38348623853211006]
text: For example , nucleotides in a DNA sequence , children 's names in a given state and year , and text documents are all commonly modeled with multinomial distributions .
result: extract = Extract([Triple(Other_scientific_term("multinomial distributions"), Rel("Used_for"), Other_scientific_term("nucleotides in a DNA sequence")), Triple(Other_scientific_term("multinomial distributions"), Rel("Used_for"), Other_scientific_term("children 's names")), Triple(Other_scientific_term("multinomial distributions"), Rel("Used_for"), Other_scientific_term("text documents")), Triple(Other_scientific_term("nucleotides in a DNA sequence"), Rel("Conjunction"), Other_scientific_term("children 's names")), Triple(Other_scientific_term("children 's names"), Rel("Conjunction"), Other_scientific_term("year")), Triple(Other_scientific_term("year"), Rel("Conjunction"), Other_scientific_term("text documents"))])
pred: [['multinomial distributions', 'Used_for', 'nucleotides in a DNA sequence'], ['multinomial distributions', 'Used_for', "children 's names"], ['multinomial distributions', 'Used_for', 'text documents'], ['nucleotides in a DNA sequence', 'Conjunction', "children 's names"], ["children 's names", 'Conjunction', 'year'], ['year', 'Conjunction', 'text documents']]
trues: [['multinomial distributions', 'Used_for', 'nucleotides in a DNA sequence'], ['multinomial distributions', 'Used_for', 'text documents']]
metrics_sample: [0.3333333333333333, 1.0, 0.5]
metrics_current: [0.3843351548269581, 0.3843351548269581, 0.3843351548269581]
text: This paper considers the problem of reconstructing the motion of a 3D articulated tree from 2D point correspondences subject to some temporal prior .
result: extract = Extract([Triple(Generic("problem"), Rel("Used_for"), Task("reconstructing the motion of a 3D articulated tree")), Triple(Other_scientific_term("2D point correspondences"), Rel("Used_for"), Generic("problem")), Triple(Other_scientific_term("temporal prior"), Rel("Feature_of"), Other_scientific_term("2D point correspondences"))])
pred: [['problem', 'Used_for', 'reconstructing the motion of a 3D articulated tree'], ['2D point correspondences', 'Used_for', 'problem'], ['temporal prior', 'Feature_of', '2D point correspondences']]
trues: [['2D point correspondences', 'Used_for', 'reconstructing the motion of a 3D articulated tree']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3822463768115942, 0.3836363636363636, 0.38294010889292196]
text: In comparison with previous works , in this paper it is assumed that the signals are not i.i.d. in each epoch , but obey a first-order autoregressive model .
result: extract = Extract([Triple(Other_scientific_term("first-order autoregressive model"), Rel("Used_for"), Other_scientific_term("signals"))])
pred: [['first-order autoregressive model', 'Used_for', 'signals']]
trues: [['first-order autoregressive model', 'Used_for', 'signals']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3833634719710669, 0.38475499092558985, 0.38405797101449274]
text: First , we investigate how well the addressee of a dialogue act can be predicted based on gaze , utterance and conversational context features .
result: extract = Extract([Triple(Other_scientific_term("gaze"), Rel("Conjunction"), Other_scientific_term("utterance and conversational context features"))])
pred: [['gaze', 'Conjunction', 'utterance and conversational context features']]
trues: [['gaze', 'Used_for', 'addressee of a dialogue act'], ['gaze', 'Conjunction', 'utterance'], ['utterance', 'Used_for', 'addressee of a dialogue act'], ['utterance', 'Conjunction', 'conversational context features'], ['conversational context features', 'Used_for', 'addressee of a dialogue act']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38267148014440433, 0.381294964028777, 0.38198198198198197]
text: Our morphological analyzer has done all the necessary work for the recognition and classification of proper names , numerical and temporal expressions , i.e. Named Entity ( NE ) items in the Japanese text .
result: extract = Extract([Triple(Method("morphological analyzer"), Rel("Used_for"), Other_scientific_term("proper names")), Triple(Method("morphological analyzer"), Rel("Used_for"), Other_scientific_term("numerical and temporal expressions")), Triple(Other_scientific_term("proper names"), Rel("Conjunction"), Other_scientific_term("numerical and temporal expressions")), Triple(Other_scientific_term("Named Entity ( NE ) items"), Rel("Feature_of"), Material("Japanese text"))])
pred: [['morphological analyzer', 'Used_for', 'proper names'], ['morphological analyzer', 'Used_for', 'numerical and temporal expressions'], ['proper names', 'Conjunction', 'numerical and temporal expressions'], ['Named Entity ( NE ) items', 'Feature_of', 'Japanese text']]
trues: [['morphological analyzer', 'Used_for', 'recognition and classification of proper names , numerical and temporal expressions'], ['Named Entity ( NE ) items', 'Hyponym_of', 'proper names , numerical and temporal expressions'], ['Named Entity ( NE ) items', 'Part_of', 'Japanese text']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37992831541218636, 0.37924865831842575, 0.37958818263205013]
text: The problem of blind separation of underdetermined instantaneous mixtures of independent signals is addressed through a method relying on nonstationarity of the original signals .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Task("blind separation of underdetermined instantaneous mixtures of independent signals")), Triple(Other_scientific_term("nonstationarity"), Rel("Used_for"), Generic("method")), Triple(Other_scientific_term("nonstationarity"), Rel("Feature_of"), Other_scientific_term("original signals"))])
pred: [['method', 'Used_for', 'blind separation of underdetermined instantaneous mixtures of independent signals'], ['nonstationarity', 'Used_for', 'method'], ['nonstationarity', 'Feature_of', 'original signals']]
trues: [['method', 'Used_for', 'blind separation of underdetermined instantaneous mixtures of independent signals'], ['nonstationarity', 'Used_for', 'method']]
metrics_sample: [0.6666666666666666, 1.0, 0.8]
metrics_current: [0.38146167557932265, 0.38146167557932265, 0.38146167557932265]
text: We also derive an efficient non-linear extension of WARCA by using the kernel trick .
result: extract = Extract([Triple(Method("kernel trick"), Rel("Used_for"), Method("non-linear extension of WARCA"))])
pred: [['kernel trick', 'Used_for', 'non-linear extension of WARCA']]
trues: [['kernel trick', 'Used_for', 'non-linear extension of WARCA']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.38256227758007116, 0.38256227758007116, 0.38256227758007116]
text: The system is implemented entirely in Prolog , a programming language based on logic .
result: extract = Extract([Triple(Method("Prolog"), Rel("Used_for"), Generic("system")), Triple(Method("Prolog"), Rel("Used_for"), Method("programming language")), Triple(Method("logic"), Rel("Used_for"), Method("Prolog"))])
pred: [['Prolog', 'Used_for', 'system'], ['Prolog', 'Used_for', 'programming language'], ['logic', 'Used_for', 'Prolog']]
trues: [['Prolog', 'Used_for', 'system'], ['Prolog', 'Hyponym_of', 'programming language'], ['logic', 'Used_for', 'programming language']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.3823008849557522, 0.3823008849557522, 0.38230088495575215]
text: The result theoretically justifies the effectiveness of features in robust PCA .
result: extract = Extract([Triple(Other_scientific_term("features"), Rel("Used_for"), Method("robust PCA"))])
pred: [['features', 'Used_for', 'robust PCA']]
trues: [['features', 'Feature_of', 'robust PCA']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38162544169611307, 0.38162544169611307, 0.38162544169611307]
text: Thus , in this paper , we study the problem of robust PCA with side information , where both prior structure and features of entities are exploited for recovery .
result: extract = Extract([Triple(Other_scientific_term("prior structure"), Rel("Conjunction"), Other_scientific_term("features of entities")), Triple(Other_scientific_term("prior structure"), Rel("Used_for"), Task("robust PCA")), Triple(Other_scientific_term("features of entities"), Rel("Used_for"), Task("robust PCA"))])
pred: [['prior structure', 'Conjunction', 'features of entities'], ['prior structure', 'Used_for', 'robust PCA'], ['features of entities', 'Used_for', 'robust PCA']]
trues: [['side information', 'Used_for', 'robust PCA'], ['prior structure', 'Conjunction', 'features of entities'], ['prior structure', 'Used_for', 'recovery'], ['features of entities', 'Used_for', 'recovery']]
metrics_sample: [0.3333333333333333, 0.25, 0.28571428571428575]
metrics_current: [0.38137082601054484, 0.38070175438596493, 0.3810359964881475]
text: We propose a novel probabilistic framework for learning visual models of 3D object categories by combining appearance information and geometric constraints .
result: extract = Extract([Triple(Method("probabilistic framework"), Rel("Used_for"), Method("visual models of 3D object categories")), Triple(Other_scientific_term("appearance information"), Rel("Used_for"), Method("probabilistic framework")), Triple(Other_scientific_term("appearance information"), Rel("Conjunction"), Other_scientific_term("geometric constraints")), Triple(Other_scientific_term("geometric constraints"), Rel("Used_for"), Method("probabilistic framework"))])
pred: [['probabilistic framework', 'Used_for', 'visual models of 3D object categories'], ['appearance information', 'Used_for', 'probabilistic framework'], ['appearance information', 'Conjunction', 'geometric constraints'], ['geometric constraints', 'Used_for', 'probabilistic framework']]
trues: [['probabilistic framework', 'Used_for', 'visual models of 3D object categories'], ['appearance information', 'Used_for', 'probabilistic framework'], ['appearance information', 'Conjunction', 'geometric constraints'], ['geometric constraints', 'Used_for', 'probabilistic framework']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3856893542757417, 0.38501742160278746, 0.3853530950305144]
text: We present a strong detector based on combining a set of weak detectors .
result: extract = Extract([Triple(Method("weak detectors"), Rel("Used_for"), Method("detector"))])
pred: [['weak detectors', 'Used_for', 'detector']]
trues: [['detectors', 'Used_for', 'detector']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38501742160278746, 0.3843478260869565, 0.38468233246301137]
text: This paper presents a novel representation for three-dimensional objects in terms of affine-invariant image patches and their spatial relationships .
result: extract = Extract([Triple(Other_scientific_term("affine-invariant image patches"), Rel("Used_for"), Other_scientific_term("three-dimensional objects")), Triple(Other_scientific_term("affine-invariant image patches"), Rel("Conjunction"), Other_scientific_term("spatial relationships"))])
pred: [['affine-invariant image patches', 'Used_for', 'three-dimensional objects'], ['affine-invariant image patches', 'Conjunction', 'spatial relationships']]
trues: [['representation', 'Used_for', 'three-dimensional objects'], ['affine-invariant image patches', 'Feature_of', 'three-dimensional objects'], ['spatial relationships', 'Feature_of', 'affine-invariant image patches']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3836805555555556, 0.38235294117647056, 0.38301559792027734]
text: We have applied it to real scenes that include transparent objects and recovered the shapes of the objects with high accuracy .
result: extract = Extract([Triple(Generic("it"), Rel("Used_for"), Other_scientific_term("transparent objects")), Triple(Generic("it"), Rel("Used_for"), Other_scientific_term("shapes of the objects"))])
pred: [['it', 'Used_for', 'transparent objects'], ['it', 'Used_for', 'shapes of the objects']]
trues: [['it', 'Used_for', 'real scenes'], ['it', 'Used_for', 'shapes of the objects'], ['transparent objects', 'Part_of', 'real scenes'], ['accuracy', 'Evaluate_for', 'shapes of the objects']]
metrics_sample: [0.5, 0.25, 0.3333333333333333]
metrics_current: [0.38408304498269896, 0.38144329896907214, 0.38275862068965516]
text: This distribution has numerous applications in computer Go , including serving as an efficient stand-alone Go player .
result: extract = Extract([Triple(Generic("distribution"), Rel("Used_for"), Task("computer Go")), Triple(Generic("distribution"), Rel("Used_for"), Method("stand-alone Go player")), Triple(Task("computer Go"), Rel("Used_for"), Method("stand-alone Go player"))])
pred: [['distribution', 'Used_for', 'computer Go'], ['distribution', 'Used_for', 'stand-alone Go player'], ['computer Go', 'Used_for', 'stand-alone Go player']]
trues: [['distribution', 'Used_for', 'computer Go']]
metrics_sample: [0.3333333333333333, 1.0, 0.5]
metrics_current: [0.3838209982788296, 0.38250428816466553, 0.38316151202749144]
text: In particular , our guarantee suggests that a substantial amount of low rank matrices , which can not be recovered by standard robust PCA , become re-coverable by our proposed method .
result: extract = Extract([Triple(Other_scientific_term("low rank matrices"), Rel("Used_for"), Method("robust PCA")), Triple(Generic("guarantee"), Rel("Used_for"), Method("robust PCA")), Triple(Generic("guarantee"), Rel("Used_for"), Generic("method"))])
pred: [['low rank matrices', 'Used_for', 'robust PCA'], ['guarantee', 'Used_for', 'robust PCA'], ['guarantee', 'Used_for', 'method']]
trues: [['method', 'Used_for', 'low rank matrices']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3818493150684932, 0.3818493150684932, 0.3818493150684932]
text: With the rise of deep archi-tectures , the prime focus has been on object category recognition .
result: extract = Extract([Triple(Method("deep archi-tectures"), Rel("Used_for"), Task("object category recognition"))])
pred: [['deep archi-tectures', 'Used_for', 'object category recognition']]
trues: [['deep archi-tectures', 'Used_for', 'object category recognition']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.38290598290598293, 0.38290598290598293, 0.382905982905983]
text: Recent progress in computer vision has been driven by high-capacity models trained on large datasets .
result: extract = Extract([Triple(Method("high-capacity models"), Rel("Used_for"), Task("computer vision")), Triple(Material("large datasets"), Rel("Used_for"), Method("high-capacity models"))])
pred: [['high-capacity models', 'Used_for', 'computer vision'], ['large datasets', 'Used_for', 'high-capacity models']]
trues: [['high-capacity models', 'Used_for', 'computer vision'], ['large datasets', 'Used_for', 'high-capacity models']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3850085178875639, 0.3850085178875639, 0.3850085178875639]
text: This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance .
result: extract = Extract([Triple(Method("statistical approach"), Rel("Used_for"), Other_scientific_term("translation errors")), Triple(Other_scientific_term("loss functions"), Rel("Evaluate_for"), Other_scientific_term("translation performance"))])
pred: [['statistical approach', 'Used_for', 'translation errors'], ['loss functions', 'Evaluate_for', 'translation performance']]
trues: [['loss functions', 'Evaluate_for', 'translation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3837011884550085, 0.3843537414965986, 0.38402718776550554]
text: Extension to affine projection enables reconstruction without estimating cameras .
result: extract = Extract([Triple(Other_scientific_term("affine projection"), Rel("Used_for"), Task("reconstruction")), Triple(Other_scientific_term("affine projection"), Rel("Used_for"), Other_scientific_term("cameras"))])
pred: [['affine projection', 'Used_for', 'reconstruction'], ['affine projection', 'Used_for', 'cameras']]
trues: [['affine projection', 'Used_for', 'reconstruction']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3840947546531303, 0.3853989813242784, 0.3847457627118644]
text: In this paper , we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2 , higher than previously reported results on the NEGRA corpus .
result: extract = Extract([Triple(Method("unlexicalized parser"), Rel("Used_for"), Material("German")), Triple(Method("smoothing and suffix analysis"), Rel("Used_for"), Method("unlexicalized parser")), Triple(Metric("labelled bracket F-score"), Rel("Evaluate_for"), Method("unlexicalized parser")), Triple(Material("NEGRA corpus"), Rel("Evaluate_for"), Method("unlexicalized parser"))])
pred: [['unlexicalized parser', 'Used_for', 'German'], ['smoothing and suffix analysis', 'Used_for', 'unlexicalized parser'], ['labelled bracket F-score', 'Evaluate_for', 'unlexicalized parser'], ['NEGRA corpus', 'Evaluate_for', 'unlexicalized parser']]
trues: [['unlexicalized parser', 'Used_for', 'German'], ['smoothing', 'Used_for', 'unlexicalized parser'], ['smoothing', 'Conjunction', 'suffix analysis'], ['suffix analysis', 'Used_for', 'unlexicalized parser'], ['labelled bracket F-score', 'Evaluate_for', 'unlexicalized parser'], ['NEGRA corpus', 'Evaluate_for', 'unlexicalized parser']]
metrics_sample: [0.75, 0.5, 0.6]
metrics_current: [0.3865546218487395, 0.3865546218487395, 0.38655462184873957]
text: The seven categories of the scheme are based on rhetorical moves of argumentation .
result: extract = Extract([Triple(Other_scientific_term("rhetorical moves of argumentation"), Rel("Used_for"), Generic("categories"))])
pred: [['rhetorical moves of argumentation', 'Used_for', 'categories']]
trues: [['rhetorical moves of argumentation', 'Used_for', 'scheme']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3859060402684564, 0.3859060402684564, 0.3859060402684564]
text: To evaluate our proposed model , we build a large streaming video dataset with annotations .
result: extract = Extract([Triple(Generic("large streaming video dataset"), Rel("Used_for"), Generic("model")), Triple(Other_scientific_term("annotations"), Rel("Feature_of"), Generic("large streaming video dataset"))])
pred: [['large streaming video dataset', 'Used_for', 'model'], ['annotations', 'Feature_of', 'large streaming video dataset']]
trues: [['streaming video dataset', 'Evaluate_for', 'model']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38461538461538464, 0.3852596314907873, 0.38493723849372385]
text: Metagrammatical formalisms that combine context-free phrase structure rules and metarules ( MPS grammars ) allow concise statement of generalizations about the syntax of natural languages .
result: extract = Extract([Triple(Method("Metagrammatical formalisms"), Rel("Used_for"), Other_scientific_term("syntax of natural languages")), Triple(Method("context-free phrase structure rules"), Rel("Part_of"), Method("Metagrammatical formalisms")), Triple(Method("context-free phrase structure rules"), Rel("Conjunction"), Method("metarules ( MPS grammars")), Triple(Method("metarules ( MPS grammars"), Rel("Part_of"), Method("Metagrammatical formalisms"))])
pred: [['Metagrammatical formalisms', 'Used_for', 'syntax of natural languages'], ['context-free phrase structure rules', 'Part_of', 'Metagrammatical formalisms'], ['context-free phrase structure rules', 'Conjunction', 'metarules ( MPS grammars'], ['metarules ( MPS grammars', 'Part_of', 'Metagrammatical formalisms']]
trues: [['context-free phrase structure rules', 'Part_of', 'Metagrammatical formalisms'], ['context-free phrase structure rules', 'Conjunction', 'metarules ( MPS grammars )'], ['metarules ( MPS grammars )', 'Part_of', 'Metagrammatical formalisms']]
metrics_sample: [0.25, 0.3333333333333333, 0.28571428571428575]
metrics_current: [0.38372093023255816, 0.385, 0.38435940099833615]
text: Thus , our method can be applied with great benefit to language pairs for which only scarce resources are available .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Material("language pairs")), Triple(Material("scarce resources"), Rel("Used_for"), Generic("method"))])
pred: [['method', 'Used_for', 'language pairs'], ['scarce resources', 'Used_for', 'method']]
trues: [['scarce resources', 'Used_for', 'method']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3841059602649007, 0.3860232945091514, 0.3850622406639004]
text: We present a novel method for discovering parallel sentences in comparable , non-parallel corpora .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Task("discovering parallel sentences")), Triple(Material("non-parallel corpora"), Rel("Used_for"), Task("discovering parallel sentences"))])
pred: [['method', 'Used_for', 'discovering parallel sentences'], ['non-parallel corpora', 'Used_for', 'discovering parallel sentences']]
trues: [['method', 'Used_for', 'discovering parallel sentences'], ['comparable , non-parallel corpora', 'Used_for', 'discovering parallel sentences']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3844884488448845, 0.3864013266998342, 0.38544251447477257]
text: We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities .
result: extract = Extract([Triple(Other_scientific_term("gray-level band-pass white noise patterns"), Rel("Used_for"), Other_scientific_term("robustness")), Triple(Other_scientific_term("indirect lighting"), Rel("Conjunction"), Other_scientific_term("scene discontinuities"))])
pred: [['gray-level band-pass white noise patterns', 'Used_for', 'robustness'], ['indirect lighting', 'Conjunction', 'scene discontinuities']]
trues: [['robustness', 'Evaluate_for', 'gray-level band-pass white noise patterns'], ['indirect lighting', 'Feature_of', 'robustness'], ['indirect lighting', 'Conjunction', 'scene discontinuities'], ['scene discontinuities', 'Feature_of', 'robustness']]
metrics_sample: [0.5, 0.25, 0.3333333333333333]
metrics_current: [0.3848684210526316, 0.385502471169687, 0.3851851851851852]
text: FERRET utilizes a novel approach to Q/A known as predictive questioning which attempts to identify the questions ( and answers ) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario .
result: extract = Extract([Triple(Method("FERRET"), Rel("Used_for"), Task("Q/A")), Triple(Method("predictive questioning"), Rel("Used_for"), Method("FERRET")), Triple(Method("predictive questioning"), Rel("Used_for"), Task("Q/A"))])
pred: [['FERRET', 'Used_for', 'Q/A'], ['predictive questioning', 'Used_for', 'FERRET'], ['predictive questioning', 'Used_for', 'Q/A']]
trues: [['approach', 'Used_for', 'FERRET'], ['approach', 'Used_for', 'Q/A']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3829787234042553, 0.3842364532019704, 0.3836065573770492]
text: We then derive a scalable stochastic gradient descent algorithm for the resulting learning problem .
result: extract = Extract([Triple(Method("stochastic gradient descent algorithm"), Rel("Used_for"), Generic("learning problem"))])
pred: [['stochastic gradient descent algorithm', 'Used_for', 'learning problem']]
trues: [['stochastic gradient descent algorithm', 'Used_for', 'learning problem']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3839869281045752, 0.38524590163934425, 0.3846153846153846]
text: Multi-layer perceptrons ( MLPs ) performed much better than standard Gaussian mixture models ( GMMs ) for both our ANF-based and ON-based auditory features .
result: extract = Extract([Triple(Method("Multi-layer perceptrons ( MLPs )"), Rel("Compare"), Method("Gaussian mixture models ( GMMs )")), Triple(Other_scientific_term("ANF-based and ON-based auditory features"), Rel("Used_for"), Method("Multi-layer perceptrons ( MLPs )")), Triple(Other_scientific_term("ANF-based and ON-based auditory features"), Rel("Used_for"), Method("Gaussian mixture models ( GMMs )"))])
pred: [['Multi-layer perceptrons ( MLPs )', 'Compare', 'Gaussian mixture models ( GMMs )'], ['ANF-based and ON-based auditory features', 'Used_for', 'Multi-layer perceptrons ( MLPs )'], ['ANF-based and ON-based auditory features', 'Used_for', 'Gaussian mixture models ( GMMs )']]
trues: [['Multi-layer perceptrons ( MLPs )', 'Compare', 'Gaussian mixture models ( GMMs )'], ['Multi-layer perceptrons ( MLPs )', 'Used_for', 'ANF-based and ON-based auditory features'], ['Gaussian mixture models ( GMMs )', 'Used_for', 'ANF-based and ON-based auditory features']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.383739837398374, 0.38499184339314846, 0.38436482084690554]
text: In this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances ( called the linguistic structure ) , a structure of purposes ( called the intentional structure ) , and the state of focus of attention ( called the attentional state ) .
result: extract = Extract([Triple(Other_scientific_term("structure of purposes"), Rel("Part_of"), Generic("components")), Triple(Other_scientific_term("structure of purposes"), Rel("Conjunction"), Other_scientific_term("state of focus of attention")), Triple(Other_scientific_term("state of focus of attention"), Rel("Part_of"), Generic("components"))])
pred: [['structure of purposes', 'Part_of', 'components'], ['structure of purposes', 'Conjunction', 'state of focus of attention'], ['state of focus of attention', 'Part_of', 'components']]
trues: [['components', 'Part_of', 'discourse structure'], ['linguistic structure', 'Part_of', 'components'], ['linguistic structure', 'Conjunction', 'intentional structure'], ['intentional structure', 'Part_of', 'components'], ['intentional structure', 'Conjunction', 'attentional state'], ['attentional state', 'Part_of', 'components']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3818770226537217, 0.38126009693053314, 0.38156831042845596]
text: We investigate the problem of learning to predict moves in the board game of Go from game records of expert players .
result: extract = Extract([Triple(Material("game records of expert players"), Rel("Used_for"), Task("learning to predict moves in the board game of Go"))])
pred: [['game records of expert players', 'Used_for', 'learning to predict moves in the board game of Go']]
trues: [['game records of expert players', 'Used_for', 'board game of Go']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38126009693053314, 0.38064516129032255, 0.380952380952381]
text: In this situation , the general method is to segment the raw corpus automatically using a word list , correct the output sentences by hand , and build a model from the segmented corpus .
result: extract = Extract([Triple(Method("word list"), Rel("Used_for"), Method("segment the raw corpus")), Triple(Other_scientific_term("output sentences"), Rel("Used_for"), Generic("method")), Triple(Material("segmented corpus"), Rel("Used_for"), Generic("method"))])
pred: [['word list', 'Used_for', 'segment the raw corpus'], ['output sentences', 'Used_for', 'method'], ['segmented corpus', 'Used_for', 'method']]
trues: [['method', 'Used_for', 'raw corpus'], ['word list', 'Used_for', 'method'], ['segmented corpus', 'Used_for', 'model']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37942122186495175, 0.37881219903691815, 0.37911646586345377]
text: We report the performance of the MBR decoders on a Chinese-to-English translation task .
result: extract = Extract([Triple(Generic("MBR decoders"), Rel("Used_for"), Task("Chinese-to-English translation task"))])
pred: [['MBR decoders', 'Used_for', 'Chinese-to-English translation task']]
trues: [['MBR decoders', 'Used_for', 'Chinese-to-English translation task']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.38041733547351525, 0.3798076923076923, 0.3801122694466721]
text: We extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic Horn clauses .
result: extract = Extract([Triple(Material("corpus-based sample"), Rel("Used_for"), Other_scientific_term("heuristic principles")), Triple(Other_scientific_term("heuristic principles"), Rel("Used_for"), Other_scientific_term("probabilistic Horn clauses"))])
pred: [['corpus-based sample', 'Used_for', 'heuristic principles'], ['heuristic principles', 'Used_for', 'probabilistic Horn clauses']]
trues: [['corpus-based sample', 'Used_for', 'heuristic principles'], ['probabilistic Horn clauses', 'Feature_of', 'heuristic principles']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3808, 0.3801916932907348, 0.3804956035171863]
text: Both classifiers perform the best when conversational context and utterance features are combined with speaker 's gaze information .
result: extract = Extract([Triple(Other_scientific_term("conversational context"), Rel("Conjunction"), Other_scientific_term("utterance features")), Triple(Other_scientific_term("conversational context"), Rel("Used_for"), Generic("classifiers")), Triple(Other_scientific_term("utterance features"), Rel("Used_for"), Generic("classifiers")), Triple(Other_scientific_term("speaker 's gaze information"), Rel("Used_for"), Generic("classifiers"))])
pred: [['conversational context', 'Conjunction', 'utterance features'], ['conversational context', 'Used_for', 'classifiers'], ['utterance features', 'Used_for', 'classifiers'], ["speaker 's gaze information", 'Used_for', 'classifiers']]
trues: [['conversational context', 'Used_for', 'classifiers'], ['conversational context', 'Conjunction', 'utterance features'], ['utterance features', 'Used_for', 'classifiers'], ["speaker 's gaze information", 'Used_for', 'classifiers'], ["speaker 's gaze information", 'Conjunction', 'utterance features']]
metrics_sample: [1.0, 0.8, 0.888888888888889]
metrics_current: [0.3847376788553259, 0.38351822503961963, 0.38412698412698415]
text: For LR ( 0 ) grammars , our algorithm is closely related to the recursive ascent parsers recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] .
result: extract = Extract([Triple(Method("LR ( 0 ) grammars"), Rel("Used_for"), Generic("algorithm")), Triple(Method("LR ( 0 ) grammars"), Rel("Conjunction"), Method("recursive ascent parsers")), Triple(Method("recursive ascent parsers"), Rel("Used_for"), Generic("algorithm")), Triple(Method("Kruse-man Aretz"), Rel("Hyponym_of"), Method("recursive ascent parsers")), Triple(Method("Kruse-man Aretz"), Rel("Conjunction"), Method("Roberts")), Triple(Method("Roberts"), Rel("Hyponym_of"), Method("recursive ascent parsers"))])
pred: [['LR ( 0 ) grammars', 'Used_for', 'algorithm'], ['LR ( 0 ) grammars', 'Conjunction', 'recursive ascent parsers'], ['recursive ascent parsers', 'Used_for', 'algorithm'], ['Kruse-man Aretz', 'Hyponym_of', 'recursive ascent parsers'], ['Kruse-man Aretz', 'Conjunction', 'Roberts'], ['Roberts', 'Hyponym_of', 'recursive ascent parsers']]
trues: [['algorithm', 'Used_for', 'LR ( 0 ) grammars'], ['algorithm', 'Conjunction', 'recursive ascent parsers']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38110236220472443, 0.382306477093207, 0.38170347003154576]
text: We present a scanning method that recovers dense sub-pixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry .
result: extract = Extract([Triple(Method("scanning method"), Rel("Used_for"), Other_scientific_term("dense sub-pixel camera-projector correspondence")), Triple(Other_scientific_term("photometric calibration"), Rel("Used_for"), Method("scanning method")), Triple(Other_scientific_term("relative geometry"), Rel("Used_for"), Method("scanning method"))])
pred: [['scanning method', 'Used_for', 'dense sub-pixel camera-projector correspondence'], ['photometric calibration', 'Used_for', 'scanning method'], ['relative geometry', 'Used_for', 'scanning method']]
trues: [['scanning method', 'Used_for', 'dense sub-pixel camera-projector correspondence']]
metrics_sample: [0.3333333333333333, 1.0, 0.5]
metrics_current: [0.38087774294670845, 0.3832807570977918, 0.3820754716981132]
text: Evaluation on the ACE 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous best-reported feature-based methods on the 24 ACE relation subtypes .
result: extract = Extract([Triple(Other_scientific_term("ACE 2003 corpus"), Rel("Evaluate_for"), Method("convolution kernel over parse trees")), Triple(Other_scientific_term("ACE 2003 corpus"), Rel("Evaluate_for"), Method("feature-based methods")), Triple(Method("convolution kernel over parse trees"), Rel("Compare"), Method("feature-based methods")), Triple(Other_scientific_term("ACE relation subtypes"), Rel("Feature_of"), Method("feature-based methods"))])
pred: [['ACE 2003 corpus', 'Evaluate_for', 'convolution kernel over parse trees'], ['ACE 2003 corpus', 'Evaluate_for', 'feature-based methods'], ['convolution kernel over parse trees', 'Compare', 'feature-based methods'], ['ACE relation subtypes', 'Feature_of', 'feature-based methods']]
trues: [['ACE 2003 corpus', 'Evaluate_for', 'convolution kernel over parse trees'], ['feature-based methods', 'Compare', 'convolution kernel over parse trees']]
metrics_sample: [0.25, 0.5, 0.3333333333333333]
metrics_current: [0.38006230529595014, 0.3836477987421384, 0.3818466353677622]
text: Building on previous work at Carnegie-Mellon University e.g. [ 4 , 5 , 8 ] , Plume 's approach to parsing is based on semantic caseframe instantiation .
result: extract = Extract([Triple(Method("semantic caseframe instantiation"), Rel("Used_for"), Method("Plume 's approach"))])
pred: [['semantic caseframe instantiation', 'Used_for', "Plume 's approach"]]
trues: [["Plume 's approach", 'Used_for', 'parsing'], ['semantic caseframe instantiation', 'Used_for', "Plume 's approach"]]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3810264385692068, 0.384012539184953, 0.38251366120218583]
text: We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 video sequences with extreme illumination , pose and head motion variation .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Task("recognition system"))])
pred: [['method', 'Used_for', 'recognition system']]
trues: [['method', 'Used_for', 'fully automatic recognition system'], ['video sequences', 'Evaluate_for', 'fully automatic recognition system'], ['illumination', 'Feature_of', 'video sequences'], ['illumination', 'Conjunction', 'pose'], ['pose', 'Feature_of', 'video sequences'], ['pose', 'Conjunction', 'head motion variation'], ['head motion variation', 'Feature_of', 'video sequences']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3804347826086957, 0.3798449612403101, 0.38013964313421256]
text: The proposed approach does not require a separate segmentation stage and is applicable to cluttered scenes .
result: extract = Extract([Triple(Generic("approach"), Rel("Used_for"), Material("cluttered scenes"))])
pred: [['approach', 'Used_for', 'cluttered scenes']]
trues: [['approach', 'Used_for', 'cluttered scenes']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3813953488372093, 0.38080495356037153, 0.38109992254066616]
text: On the internal side , liaisons are established between elements of the text and the graph by using broadly available resources such as a LO-English or better a L0-UNL dictionary , a morphosyntactic parser of L0 , and a canonical graph2tree transformation .
result: extract = Extract([Triple(Other_scientific_term("LO-English or better a L0-UNL dictionary"), Rel("Used_for"), Other_scientific_term("liaisons")), Triple(Other_scientific_term("LO-English or better a L0-UNL dictionary"), Rel("Conjunction"), Method("morphosyntactic parser of L0")), Triple(Method("morphosyntactic parser of L0"), Rel("Used_for"), Other_scientific_term("liaisons")), Triple(Method("morphosyntactic parser of L0"), Rel("Conjunction"), Method("canonical graph2tree transformation")), Triple(Method("canonical graph2tree transformation"), Rel("Used_for"), Other_scientific_term("liaisons"))])
pred: [['LO-English or better a L0-UNL dictionary', 'Used_for', 'liaisons'], ['LO-English or better a L0-UNL dictionary', 'Conjunction', 'morphosyntactic parser of L0'], ['morphosyntactic parser of L0', 'Used_for', 'liaisons'], ['morphosyntactic parser of L0', 'Conjunction', 'canonical graph2tree transformation'], ['canonical graph2tree transformation', 'Used_for', 'liaisons']]
trues: [['resources', 'Used_for', 'liaisons'], ['LO-English or better a L0-UNL dictionary', 'Hyponym_of', 'resources'], ['LO-English or better a L0-UNL dictionary', 'Conjunction', 'morphosyntactic parser of L0'], ['morphosyntactic parser of L0', 'Hyponym_of', 'resources'], ['morphosyntactic parser of L0', 'Conjunction', 'canonical graph2tree transformation'], ['canonical graph2tree transformation', 'Hyponym_of', 'resources']]
metrics_sample: [0.4, 0.3333333333333333, 0.3636363636363636]
metrics_current: [0.38153846153846155, 0.3803680981595092, 0.38095238095238093]
text: We provide a principled and provable solution for this problem ; it is parameterless and requires no data insights .
result: extract = Extract([Triple(Generic("it"), Rel("Used_for"), Generic("problem"))])
pred: [['it', 'Used_for', 'problem']]
trues: [['principled and provable solution', 'Used_for', 'problem']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.38095238095238093, 0.37978560490045943, 0.38036809815950917]
text: The key idea of our approach is to use different view points for reasoning about contradictions and consistencies between multiple depth maps generated with the same stereo algorithm .
result: extract = Extract([Triple(Other_scientific_term("view points"), Rel("Used_for"), Task("reasoning about contradictions and consistencies")), Triple(Method("stereo algorithm"), Rel("Used_for"), Other_scientific_term("depth maps"))])
pred: [['view points', 'Used_for', 'reasoning about contradictions and consistencies'], ['stereo algorithm', 'Used_for', 'depth maps']]
trues: [['view points', 'Used_for', 'approach']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37978560490045943, 0.37920489296636084, 0.37949502677888297]
text: Unfortunately , creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required .
result: extract = Extract([Triple(Material("pixel-level labels"), Rel("Used_for"), Material("large datasets"))])
pred: [['pixel-level labels', 'Used_for', 'large datasets']]
trues: [['pixel-level labels', 'Feature_of', 'large datasets']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37920489296636084, 0.3786259541984733, 0.37891520244461424]
text: A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition .
result: extract = Extract([Triple(Material("scfs"), Rel("Used_for"), Other_scientific_term("linguistic annotation")), Triple(Material("corpus data"), Rel("Used_for"), Task("subcategorization acquisition"))])
pred: [['scfs', 'Used_for', 'linguistic annotation'], ['corpus data', 'Used_for', 'subcategorization acquisition']]
trues: [['tool', 'Used_for', 'linguistic annotation of scfs'], ['training and test data', 'Used_for', 'subcategorization acquisition']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3780487804878049, 0.3774733637747336, 0.37776085300837775]
text: We validate this new method on nine standard person re-identification datasets including two large scale Market-1501 and CUHK03 datasets and show that we improve upon the current state-of-the-art methods on all of them .
result: extract = Extract([Triple(Generic("datasets"), Rel("Evaluate_for"), Generic("method")), Triple(Material("Market-1501 and CUHK03 datasets"), Rel("Hyponym_of"), Generic("datasets"))])
pred: [['datasets', 'Evaluate_for', 'method'], ['Market-1501 and CUHK03 datasets', 'Hyponym_of', 'datasets']]
trues: [['person re-identification datasets', 'Evaluate_for', 'method'], ['scale Market-1501', 'Hyponym_of', 'person re-identification datasets'], ['CUHK03 datasets', 'Hyponym_of', 'person re-identification datasets'], ['CUHK03 datasets', 'Conjunction', 'scale Market-1501']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3768996960486322, 0.3751891074130106, 0.3760424564063684]
text: Experiments on the TREC Blog track test set show that both groups of credibility indicators significantly improve retrieval effectiveness ; the best performance is achieved when combining them .
result: extract = Extract([Triple(Material("TREC Blog track test set"), Rel("Evaluate_for"), Other_scientific_term("credibility indicators")), Triple(Other_scientific_term("credibility indicators"), Rel("Evaluate_for"), Metric("retrieval effectiveness"))])
pred: [['TREC Blog track test set', 'Evaluate_for', 'credibility indicators'], ['credibility indicators', 'Evaluate_for', 'retrieval effectiveness']]
trues: [['TREC Blog track test set', 'Evaluate_for', 'credibility indicators'], ['retrieval effectiveness', 'Evaluate_for', 'credibility indicators']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.37727272727272726, 0.3755656108597285, 0.3764172335600907]
text: Kernel space embedding decouples the training and prediction costs from the data dimension and enables us to plug inarbitrary distance measures which are more natural for the features .
result: extract = Extract([Triple(Other_scientific_term("distance measures"), Rel("Used_for"), Other_scientific_term("features"))])
pred: [['distance measures', 'Used_for', 'features']]
trues: [['Kernel space embedding', 'Used_for', 'inarbitrary distance measures']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3767019667170953, 0.375, 0.37584905660377355]
text: The two main contributions of this paper are : i ) a new type of three-plane graph for dense-stereo dynamic-programming , that encourages correct occlusion labeling ; ii ) a compact geometric derivation for novel-view synthesis by direct projection of the minimum-cost surface .
result: extract = Extract([Triple(Method("three-plane graph"), Rel("Used_for"), Method("dense-stereo dynamic-programming")), Triple(Method("three-plane graph"), Rel("Used_for"), Task("correct occlusion labeling")), Triple(Method("compact geometric derivation"), Rel("Used_for"), Task("novel-view synthesis")), Triple(Other_scientific_term("minimum-cost surface"), Rel("Used_for"), Method("direct projection"))])
pred: [['three-plane graph', 'Used_for', 'dense-stereo dynamic-programming'], ['three-plane graph', 'Used_for', 'correct occlusion labeling'], ['compact geometric derivation', 'Used_for', 'novel-view synthesis'], ['minimum-cost surface', 'Used_for', 'direct projection']]
trues: [['three-plane graph', 'Used_for', 'dense-stereo dynamic-programming'], ['dense-stereo dynamic-programming', 'Used_for', 'occlusion labeling'], ['compact geometric derivation', 'Used_for', 'novel-view synthesis'], ['direct projection of the minimum-cost surface', 'Used_for', 'compact geometric derivation']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3774436090225564, 0.37574850299401197, 0.37659414853713435]
text: Hitherto , smooth motion has been encouraged using a trajectory basis , yielding a hard combinatorial problem with time complexity growing exponentially in the number of frames .
result: extract = Extract([Triple(Other_scientific_term("trajectory basis"), Rel("Used_for"), Other_scientific_term("smooth motion")), Triple(Other_scientific_term("trajectory basis"), Rel("Used_for"), Other_scientific_term("hard combinatorial problem"))])
pred: [['trajectory basis', 'Used_for', 'smooth motion'], ['trajectory basis', 'Used_for', 'hard combinatorial problem']]
trues: [['trajectory basis', 'Used_for', 'smooth motion'], ['time complexity', 'Evaluate_for', 'hard combinatorial problem']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3778110944527736, 0.3761194029850746, 0.37696335078534027]
text: Experiments on synthetic and real data show that both our triangulation and LAGC algorithms outperform state-of-the-art solutions in accuracy and visual quality .
result: extract = Extract([Triple(Generic("synthetic and real data"), Rel("Evaluate_for"), Generic("our triangulation and LAGC algorithms")), Triple(Generic("our triangulation and LAGC algorithms"), Rel("Compare"), Generic("state-of-the-art solutions")), Triple(Metric("accuracy"), Rel("Evaluate_for"), Generic("our triangulation and LAGC algorithms")), Triple(Metric("accuracy"), Rel("Evaluate_for"), Generic("state-of-the-art solutions")), Triple(Metric("accuracy"), Rel("Conjunction"), Metric("visual quality")), Triple(Metric("visual quality"), Rel("Evaluate_for"), Generic("our triangulation and LAGC algorithms")), Triple(Metric("visual quality"), Rel("Evaluate_for"), Generic("state-of-the-art solutions"))])
pred: [['synthetic and real data', 'Evaluate_for', 'our triangulation and LAGC algorithms'], ['our triangulation and LAGC algorithms', 'Compare', 'state-of-the-art solutions'], ['accuracy', 'Evaluate_for', 'our triangulation and LAGC algorithms'], ['accuracy', 'Evaluate_for', 'state-of-the-art solutions'], ['accuracy', 'Conjunction', 'visual quality'], ['visual quality', 'Evaluate_for', 'our triangulation and LAGC algorithms'], ['visual quality', 'Evaluate_for', 'state-of-the-art solutions']]
trues: [['synthetic and real data', 'Evaluate_for', 'triangulation and LAGC algorithms'], ['synthetic and real data', 'Evaluate_for', 'state-of-the-art solutions'], ['triangulation and LAGC algorithms', 'Compare', 'state-of-the-art solutions'], ['accuracy', 'Evaluate_for', 'triangulation and LAGC algorithms'], ['accuracy', 'Evaluate_for', 'state-of-the-art solutions'], ['visual quality', 'Evaluate_for', 'triangulation and LAGC algorithms'], ['visual quality', 'Evaluate_for', 'state-of-the-art solutions']]
metrics_sample: [0.2857142857142857, 0.2857142857142857, 0.2857142857142857]
metrics_current: [0.3768545994065282, 0.37518463810930575, 0.3760177646188008]
text: A separation method is proposed that is nearly statistically efficient ( approaching the corresponding Cramér-Rao lower bound ) , if the separated signals obey the assumed model .
result: extract = Extract([Triple(Method("separation method"), Rel("Compare"), Other_scientific_term("Cramér-Rao lower bound")), Triple(Metric("statistically efficient"), Rel("Evaluate_for"), Method("separation method")), Triple(Metric("statistically efficient"), Rel("Evaluate_for"), Other_scientific_term("Cramér-Rao lower bound"))])
pred: [['separation method', 'Compare', 'Cramér-Rao lower bound'], ['statistically efficient', 'Evaluate_for', 'separation method'], ['statistically efficient', 'Evaluate_for', 'Cramér-Rao lower bound']]
trues: [['Cramér-Rao lower bound )', 'Feature_of', 'separation method']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37518463810930575, 0.3746312684365782, 0.37490774907749075]
text: Our study reveals that the proposed method not only reduces an extensive system development effort but also improves the transliteration accuracy significantly .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Task("system development")), Triple(Metric("transliteration accuracy"), Rel("Evaluate_for"), Generic("method"))])
pred: [['method', 'Used_for', 'system development'], ['transliteration accuracy', 'Evaluate_for', 'method']]
trues: [['transliteration accuracy', 'Evaluate_for', 'method']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3755522827687776, 0.3755522827687776, 0.3755522827687776]
text: In the experiments , we used a variety of methods for preparing a segmented corpus and compared the language models by their speech recognition accuracies .
result: extract = Extract([Triple(Material("segmented corpus"), Rel("Used_for"), Method("language models")), Triple(Method("language models"), Rel("Compare"), Metric("speech recognition accuracies"))])
pred: [['segmented corpus', 'Used_for', 'language models'], ['language models', 'Compare', 'speech recognition accuracies']]
trues: [['methods', 'Used_for', 'preparing a segmented corpus'], ['speech recognition accuracies', 'Evaluate_for', 'language models']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3744493392070485, 0.3744493392070485, 0.3744493392070485]
text: MINPRAN , a new robust operator , nds good ts in data sets where more than 50 % of the points are outliers .
result: extract = Extract([Triple(Method("MINPRAN"), Rel("Hyponym_of"), Method("robust operator")), Triple(Method("MINPRAN"), Rel("Used_for"), Other_scientific_term("ts")), Triple(Other_scientific_term("data sets"), Rel("Used_for"), Method("MINPRAN"))])
pred: [['MINPRAN', 'Hyponym_of', 'robust operator'], ['MINPRAN', 'Used_for', 'ts'], ['data sets', 'Used_for', 'MINPRAN']]
trues: [['MINPRAN', 'Hyponym_of', 'robust operator']]
metrics_sample: [0.3333333333333333, 1.0, 0.5]
metrics_current: [0.3742690058479532, 0.375366568914956, 0.3748169838945827]
text: We present results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers .
result: extract = Extract([Triple(Method("Bayesian Network"), Rel("Conjunction"), Method("Naive Bayes classifiers")), Triple(Method("Bayesian Network"), Rel("Used_for"), Task("addressee identification")), Triple(Method("Naive Bayes classifiers"), Rel("Used_for"), Task("addressee identification"))])
pred: [['Bayesian Network', 'Conjunction', 'Naive Bayes classifiers'], ['Bayesian Network', 'Used_for', 'addressee identification'], ['Naive Bayes classifiers', 'Used_for', 'addressee identification']]
trues: [['Bayesian Network', 'Used_for', 'addressee identification in four-participants face-to-face meetings'], ['Naive Bayes classifiers', 'Used_for', 'addressee identification in four-participants face-to-face meetings'], ['Naive Bayes classifiers', 'Conjunction', 'Bayesian Network']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3726346433770015, 0.37372262773722625, 0.37317784256559766]
text: We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system .
result: extract = Extract([Triple(Material("extracted data"), Rel("Evaluate_for"), Generic("it")), Triple(Generic("it"), Rel("Evaluate_for"), Method("statistical machine translation system"))])
pred: [['extracted data', 'Evaluate_for', 'it'], ['it', 'Evaluate_for', 'statistical machine translation system']]
trues: [['it', 'Used_for', 'statistical machine translation system']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37155297532656023, 0.37317784256559766, 0.37236363636363634]
text: The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases , referring expressions , and interruptions .
result: extract = Extract([Triple(Generic("components"), Rel("Used_for"), Other_scientific_term("discourse phenomena")), Triple(Other_scientific_term("cue phrases"), Rel("Hyponym_of"), Other_scientific_term("discourse phenomena")), Triple(Other_scientific_term("cue phrases"), Rel("Conjunction"), Other_scientific_term("referring expressions")), Triple(Other_scientific_term("referring expressions"), Rel("Hyponym_of"), Other_scientific_term("discourse phenomena")), Triple(Other_scientific_term("referring expressions"), Rel("Conjunction"), Other_scientific_term("interruptions")), Triple(Other_scientific_term("interruptions"), Rel("Hyponym_of"), Other_scientific_term("discourse phenomena"))])
pred: [['components', 'Used_for', 'discourse phenomena'], ['cue phrases', 'Hyponym_of', 'discourse phenomena'], ['cue phrases', 'Conjunction', 'referring expressions'], ['referring expressions', 'Hyponym_of', 'discourse phenomena'], ['referring expressions', 'Conjunction', 'interruptions'], ['interruptions', 'Hyponym_of', 'discourse phenomena']]
trues: [['cue phrases', 'Hyponym_of', 'discourse phenomena'], ['cue phrases', 'Conjunction', 'referring expressions'], ['referring expressions', 'Hyponym_of', 'discourse phenomena'], ['referring expressions', 'Conjunction', 'interruptions'], ['interruptions', 'Hyponym_of', 'discourse phenomena']]
metrics_sample: [0.8333333333333334, 1.0, 0.9090909090909091]
metrics_current: [0.37553956834532376, 0.37771345875542695, 0.37662337662337664]
text: This research is part of a larger study of anaphoric expressions , the results of which will be incorporated into a natural language generation system .
result: extract = Extract([Triple(Generic("research"), Rel("Part_of"), Task("study of anaphoric expressions")), Triple(Generic("research"), Rel("Used_for"), Method("natural language generation system"))])
pred: [['research', 'Part_of', 'study of anaphoric expressions'], ['research', 'Used_for', 'natural language generation system']]
trues: [['anaphoric expressions', 'Used_for', 'natural language generation system']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3744619799139168, 0.37716763005780346, 0.3758099352051836]
text: An extension of our method to space-time interest point detection for action classification is also presented .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Task("space-time interest point detection")), Triple(Task("space-time interest point detection"), Rel("Used_for"), Task("action classification"))])
pred: [['method', 'Used_for', 'space-time interest point detection'], ['space-time interest point detection', 'Used_for', 'action classification']]
trues: [['method', 'Used_for', 'space-time interest point detection'], ['space-time interest point detection', 'Used_for', 'action classification']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.37625178826895567, 0.37896253602305474, 0.37760229720028715]
text: Examples are given that demonstrate the robustness of the new algorithm to spatial and temporal artefacts for long stereo video streams .
result: extract = Extract([Triple(Other_scientific_term("spatial and temporal artefacts"), Rel("Feature_of"), Other_scientific_term("stereo video streams"))])
pred: [['spatial and temporal artefacts', 'Feature_of', 'stereo video streams']]
trues: [['robustness', 'Evaluate_for', 'algorithm'], ['algorithm', 'Used_for', 'spatial and temporal artefacts'], ['spatial and temporal artefacts', 'Used_for', 'long stereo video streams']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3757142857142857, 0.37733142037302725, 0.376521116678597]
text: Using these ideas together , the resulting tagger gives a 97.24 % accuracy on the Penn Treebank WSJ , an error reduction of 4.4 % on the best previous single automatically learned tagging result .
result: extract = Extract([Triple(Metric("accuracy"), Rel("Evaluate_for"), Method("tagger")), Triple(Metric("accuracy"), Rel("Evaluate_for"), Material("Penn Treebank WSJ")), Triple(Metric("accuracy"), Rel("Evaluate_for"), Method("single automatically learned tagging result")), Triple(Material("Penn Treebank WSJ"), Rel("Conjunction"), Method("single automatically learned tagging result"))])
pred: [['accuracy', 'Evaluate_for', 'tagger'], ['accuracy', 'Evaluate_for', 'Penn Treebank WSJ'], ['accuracy', 'Evaluate_for', 'single automatically learned tagging result'], ['Penn Treebank WSJ', 'Conjunction', 'single automatically learned tagging result']]
trues: [['accuracy', 'Evaluate_for', 'tagger'], ['Penn Treebank WSJ', 'Evaluate_for', 'tagger'], ['error', 'Evaluate_for', 'tagger']]
metrics_sample: [0.25, 0.3333333333333333, 0.28571428571428575]
metrics_current: [0.375, 0.37714285714285717, 0.376068376068376]
text: Many of the resources used are derived from data created by human beings out of an NLP context , especially regarding MT and reference translations .
result: extract = Extract([Triple(Other_scientific_term("NLP context"), Rel("Feature_of"), Material("data")), Triple(Material("data"), Rel("Used_for"), Other_scientific_term("MT")), Triple(Material("data"), Rel("Used_for"), Other_scientific_term("reference translations")), Triple(Other_scientific_term("MT"), Rel("Conjunction"), Other_scientific_term("reference translations"))])
pred: [['NLP context', 'Feature_of', 'data'], ['data', 'Used_for', 'MT'], ['data', 'Used_for', 'reference translations'], ['MT', 'Conjunction', 'reference translations']]
trues: [['MT', 'Hyponym_of', 'NLP'], ['MT', 'Conjunction', 'reference translations'], ['reference translations', 'Hyponym_of', 'NLP']]
metrics_sample: [0.25, 0.3333333333333333, 0.28571428571428575]
metrics_current: [0.3742937853107345, 0.37695590327169276, 0.37562012756909996]
text: Unlike other techniques that handle large outlier percentages , MINPRAN does not rely on a known error bound for the good data .
result: extract = Extract([Triple(Generic("techniques"), Rel("Compare"), Method("MINPRAN"))])
pred: [['techniques', 'Compare', 'MINPRAN']]
trues: [['techniques', 'Used_for', 'large outlier percentages'], ['techniques', 'Compare', 'MINPRAN']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3751763046544429, 0.3773049645390071, 0.37623762376237624]
text: It uses a powerful pattern-matching language to classify grs into frames hierarchically in a way that mirrors inheritance-based lexica .
result: extract = Extract([Triple(Method("pattern-matching language"), Rel("Used_for"), Other_scientific_term("grs")), Triple(Other_scientific_term("frames"), Rel("Used_for"), Other_scientific_term("grs")), Triple(Other_scientific_term("inheritance-based lexica"), Rel("Used_for"), Other_scientific_term("frames"))])
pred: [['pattern-matching language', 'Used_for', 'grs'], ['frames', 'Used_for', 'grs'], ['inheritance-based lexica', 'Used_for', 'frames']]
trues: [['pattern-matching language', 'Used_for', 'It'], ['pattern-matching language', 'Used_for', 'grs']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.375, 0.37765205091937765, 0.37632135306553904]
text: The result is a discrete motor control representation of the continuous pen motion , via the quantized levels of the model parameters .
result: extract = Extract([Triple(Other_scientific_term("quantized levels of the model parameters"), Rel("Feature_of"), Method("discrete motor control representation"))])
pred: [['quantized levels of the model parameters', 'Feature_of', 'discrete motor control representation']]
trues: [['discrete motor control representation', 'Used_for', 'continuous pen motion']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3744740532959327, 0.3771186440677966, 0.37579169598874035]
text: In spite of over two decades of intense research , illumination and pose invariance remain prohibitively challenging aspects of face recognition for most practical applications .
result: extract = Extract([Triple(Other_scientific_term("illumination and pose invariance"), Rel("Feature_of"), Task("face recognition"))])
pred: [['illumination and pose invariance', 'Feature_of', 'face recognition']]
trues: [['illumination', 'Conjunction', 'pose invariance'], ['illumination', 'Part_of', 'face recognition'], ['pose invariance', 'Part_of', 'face recognition']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3739495798319328, 0.3755274261603376, 0.37473684210526315]
text: The modified graph is then sent to the UNL-L0 deconverter and the result shown .
result: extract = Extract([Triple(Other_scientific_term("UNL-L0 deconverter"), Rel("Used_for"), Generic("result"))])
pred: [['UNL-L0 deconverter', 'Used_for', 'result']]
trues: [['graph', 'Used_for', 'UNL-L0 deconverter']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3734265734265734, 0.375, 0.3742116327960757]
text: On the other hand , the manifold learning methods suggest to use the Euclidean distance in the local range , combining with the graphical relationship between samples , for approximating the geodesic distance .
result: extract = Extract([Triple(Other_scientific_term("Euclidean distance"), Rel("Used_for"), Other_scientific_term("local range")), Triple(Other_scientific_term("Euclidean distance"), Rel("Conjunction"), Other_scientific_term("graphical relationship")), Triple(Other_scientific_term("graphical relationship"), Rel("Used_for"), Other_scientific_term("geodesic distance"))])
pred: [['Euclidean distance', 'Used_for', 'local range'], ['Euclidean distance', 'Conjunction', 'graphical relationship'], ['graphical relationship', 'Used_for', 'geodesic distance']]
trues: [['Euclidean distance', 'Used_for', 'manifold learning methods'], ['Euclidean distance', 'Conjunction', 'graphical relationship'], ['Euclidean distance', 'Used_for', 'geodesic distance'], ['local range', 'Feature_of', 'Euclidean distance'], ['graphical relationship', 'Used_for', 'geodesic distance']]
metrics_sample: [0.6666666666666666, 0.4, 0.5]
metrics_current: [0.3746518105849582, 0.37517433751743373, 0.3749128919860627]
text: We analyzed eye gaze , head nods and attentional focus in the context of a direction-giving task .
result: extract = Extract([Triple(Other_scientific_term("eye gaze"), Rel("Conjunction"), Other_scientific_term("head nods")), Triple(Other_scientific_term("eye gaze"), Rel("Feature_of"), Task("direction-giving task")), Triple(Other_scientific_term("head nods"), Rel("Feature_of"), Task("direction-giving task")), Triple(Other_scientific_term("attentional focus"), Rel("Feature_of"), Task("direction-giving task"))])
pred: [['eye gaze', 'Conjunction', 'head nods'], ['eye gaze', 'Feature_of', 'direction-giving task'], ['head nods', 'Feature_of', 'direction-giving task'], ['attentional focus', 'Feature_of', 'direction-giving task']]
trues: [['eye gaze', 'Conjunction', 'head nods'], ['eye gaze', 'Part_of', 'direction-giving task'], ['head nods', 'Conjunction', 'attentional focus'], ['head nods', 'Part_of', 'direction-giving task'], ['attentional focus', 'Part_of', 'direction-giving task']]
metrics_sample: [0.25, 0.2, 0.22222222222222224]
metrics_current: [0.3739612188365651, 0.3739612188365651, 0.3739612188365651]
text: Our approach outperforms state-of-the-art trackers on the VIVID benchmark datasets .
result: extract = Extract([Triple(Generic("approach"), Rel("Compare"), Generic("trackers")), Triple(Material("VIVID benchmark datasets"), Rel("Evaluate_for"), Generic("approach")), Triple(Material("VIVID benchmark datasets"), Rel("Evaluate_for"), Generic("trackers"))])
pred: [['approach', 'Compare', 'trackers'], ['VIVID benchmark datasets', 'Evaluate_for', 'approach'], ['VIVID benchmark datasets', 'Evaluate_for', 'trackers']]
trues: [['state-of-the-art trackers', 'Compare', 'approach'], ['VIVID benchmark datasets', 'Evaluate_for', 'approach'], ['VIVID benchmark datasets', 'Evaluate_for', 'state-of-the-art trackers']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.3737931034482759, 0.3737931034482759, 0.37379310344827593]
text: However , they provide no guarantee of being more efficient than exhaustive search .
result: extract = Extract([Triple(Generic("they"), Rel("Compare"), Method("exhaustive search"))])
pred: [['they', 'Compare', 'exhaustive search']]
trues: [['they', 'Compare', 'exhaustive search']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3746556473829201, 0.3746556473829201, 0.3746556473829201]
text: First , it uses several kinds of dictionaries to segment and tag Japanese character strings .
result: extract = Extract([Triple(Material("dictionaries"), Rel("Used_for"), Other_scientific_term("Japanese character strings"))])
pred: [['dictionaries', 'Used_for', 'Japanese character strings']]
trues: [['dictionaries', 'Used_for', 'it'], ['dictionaries', 'Used_for', 'Japanese character strings']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.37551581843191195, 0.375, 0.37525773195876283]
text: Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results , proposing an objective measure for evaluating feature vector quality .
result: extract = Extract([Triple(Other_scientific_term("semantic criterion"), Rel("Used_for"), Other_scientific_term("distributional word feature vectors")), Triple(Metric("objective measure"), Rel("Used_for"), Other_scientific_term("feature vector quality"))])
pred: [['semantic criterion', 'Used_for', 'distributional word feature vectors'], ['objective measure', 'Used_for', 'feature vector quality']]
trues: [['semantic criterion', 'Evaluate_for', 'distributional word feature vectors'], ['distributional word feature vectors', 'Used_for', 'word similarity'], ['measure', 'Evaluate_for', 'feature vector quality']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37448559670781895, 0.3734610123119015, 0.373972602739726]
text: We integrate a spoken language understanding system with intelligent mobile agents that mediate between users and information sources .
result: extract = Extract([Triple(Method("spoken language understanding system"), Rel("Conjunction"), Other_scientific_term("intelligent mobile agents")), Triple(Other_scientific_term("intelligent mobile agents"), Rel("Used_for"), Other_scientific_term("users")), Triple(Other_scientific_term("intelligent mobile agents"), Rel("Used_for"), Other_scientific_term("information sources"))])
pred: [['spoken language understanding system', 'Conjunction', 'intelligent mobile agents'], ['intelligent mobile agents', 'Used_for', 'users'], ['intelligent mobile agents', 'Used_for', 'information sources']]
trues: [['intelligent mobile agents', 'Part_of', 'spoken language understanding system']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3729508196721312, 0.3729508196721312, 0.3729508196721312]
text: We discuss several applications of the result to the problem of distributional approximation of probabilistic context-free grammars by means of probabilistic finite automata .
result: extract = Extract([Triple(Method("distributional approximation"), Rel("Used_for"), Method("probabilistic context-free grammars")), Triple(Method("probabilistic finite automata"), Rel("Used_for"), Method("distributional approximation"))])
pred: [['distributional approximation', 'Used_for', 'probabilistic context-free grammars'], ['probabilistic finite automata', 'Used_for', 'distributional approximation']]
trues: [['distributional approximation', 'Feature_of', 'probabilistic context-free grammars'], ['probabilistic finite automata', 'Used_for', 'distributional approximation']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.37329700272479566, 0.37329700272479566, 0.3732970027247957]
text: We present a novel model-based approach to recover the shapes and the poses of transparent objects from known motion .
result: extract = Extract([Triple(Method("model-based approach"), Rel("Used_for"), Other_scientific_term("transparent objects")), Triple(Other_scientific_term("shapes"), Rel("Feature_of"), Other_scientific_term("transparent objects")), Triple(Other_scientific_term("shapes"), Rel("Conjunction"), Other_scientific_term("poses")), Triple(Other_scientific_term("poses"), Rel("Feature_of"), Other_scientific_term("transparent objects"))])
pred: [['model-based approach', 'Used_for', 'transparent objects'], ['shapes', 'Feature_of', 'transparent objects'], ['shapes', 'Conjunction', 'poses'], ['poses', 'Feature_of', 'transparent objects']]
trues: [['model-based approach', 'Used_for', 'shapes and the poses of transparent objects'], ['known motion', 'Used_for', 'shapes and the poses of transparent objects']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3712737127371274, 0.37228260869565216, 0.37177747625508817]
text: We show that combining anisotropic meshing with a non-quadratic approach to regularization enables us to obtain satisfactory reconstruction results using triangulations with few vertices .
result: extract = Extract([Triple(Method("anisotropic meshing"), Rel("Conjunction"), Method("non-quadratic approach to regularization")), Triple(Method("non-quadratic approach to regularization"), Rel("Used_for"), Other_scientific_term("reconstruction results")), Triple(Other_scientific_term("triangulations"), Rel("Used_for"), Other_scientific_term("reconstruction results")), Triple(Other_scientific_term("triangulations"), Rel("Feature_of"), Other_scientific_term("vertices"))])
pred: [['anisotropic meshing', 'Conjunction', 'non-quadratic approach to regularization'], ['non-quadratic approach to regularization', 'Used_for', 'reconstruction results'], ['triangulations', 'Used_for', 'reconstruction results'], ['triangulations', 'Feature_of', 'vertices']]
trues: [['anisotropic meshing', 'Conjunction', 'non-quadratic approach'], ['anisotropic meshing', 'Used_for', 'reconstruction'], ['non-quadratic approach', 'Used_for', 'regularization'], ['non-quadratic approach', 'Used_for', 'reconstruction'], ['triangulations', 'Used_for', 'reconstruction']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3692722371967655, 0.3697705802968961, 0.3695212407282535]
text: In the Object Recognition task , there exists a di-chotomy between the categorization of objects and estimating object pose , where the former necessitates a view-invariant representation , while the latter requires a representation capable of capturing pose information over different categories of objects .
result: extract = Extract([Triple(Task("categorization of objects"), Rel("Conjunction"), Task("estimating object pose")), Triple(Other_scientific_term("view-invariant representation"), Rel("Used_for"), Task("categorization of objects")), Triple(Other_scientific_term("view-invariant representation"), Rel("Used_for"), Other_scientific_term("pose information")), Triple(Other_scientific_term("pose information"), Rel("Used_for"), Task("estimating object pose"))])
pred: [['categorization of objects', 'Conjunction', 'estimating object pose'], ['view-invariant representation', 'Used_for', 'categorization of objects'], ['view-invariant representation', 'Used_for', 'pose information'], ['pose information', 'Used_for', 'estimating object pose']]
trues: [['categorization of objects', 'Part_of', 'Object Recognition task'], ['categorization of objects', 'Conjunction', 'estimating object pose'], ['estimating object pose', 'Part_of', 'Object Recognition task'], ['view-invariant representation', 'Used_for', 'former'], ['representation', 'Used_for', 'latter'], ['representation', 'Used_for', 'pose information']]
metrics_sample: [0.25, 0.16666666666666666, 0.2]
metrics_current: [0.36863270777479895, 0.36813922356091033, 0.3683858004018754]
text: A bio-inspired model for an analog programmable array processor ( APAP ) , based on studies on the vertebrate retina , has permitted the realization of complex programmable spatio-temporal dynamics in VLSI .
result: extract = Extract([Triple(Method("bio-inspired model"), Rel("Used_for"), Method("analog programmable array processor ( APAP )")), Triple(Other_scientific_term("vertebrate retina"), Rel("Used_for"), Method("bio-inspired model")), Triple(Other_scientific_term("spatio-temporal dynamics"), Rel("Feature_of"), Method("VLSI"))])
pred: [['bio-inspired model', 'Used_for', 'analog programmable array processor ( APAP )'], ['vertebrate retina', 'Used_for', 'bio-inspired model'], ['spatio-temporal dynamics', 'Feature_of', 'VLSI']]
trues: [['bio-inspired model', 'Used_for', 'analog programmable array processor ( APAP )'], ['bio-inspired model', 'Used_for', 'complex programmable spatio-temporal dynamics'], ['vertebrate retina', 'Used_for', 'bio-inspired model'], ['complex programmable spatio-temporal dynamics', 'Feature_of', 'VLSI']]
metrics_sample: [0.6666666666666666, 0.5, 0.5714285714285715]
metrics_current: [0.369826435246996, 0.36884154460719043, 0.36933333333333335]
text: Our numerical hybrid local and global mode-seeking tracker is validated on challenging airborne videos with heavy occlusion and large camera motions .
result: extract = Extract([Triple(Material("airborne videos"), Rel("Feature_of"), Other_scientific_term("occlusion")), Triple(Material("airborne videos"), Rel("Feature_of"), Other_scientific_term("camera motions"))])
pred: [['airborne videos', 'Feature_of', 'occlusion'], ['airborne videos', 'Feature_of', 'camera motions']]
trues: [['airborne videos', 'Evaluate_for', 'numerical hybrid local and global mode-seeking tracker'], ['heavy occlusion', 'Feature_of', 'airborne videos'], ['heavy occlusion', 'Conjunction', 'camera motions'], ['camera motions', 'Feature_of', 'airborne videos']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.36884154460719043, 0.36688741721854307, 0.36786188579017265]
text: Our approach to the Multi-lingual Evaluation Task ( MET ) for Japanese text is to consider the given task as a morphological analysis problem in Japanese .
result: extract = Extract([Triple(Task("Multi-lingual Evaluation Task ( MET )"), Rel("Used_for"), Task("morphological analysis problem")), Triple(Material("Japanese text"), Rel("Used_for"), Task("Multi-lingual Evaluation Task ( MET )"))])
pred: [['Multi-lingual Evaluation Task ( MET )', 'Used_for', 'morphological analysis problem'], ['Japanese text', 'Used_for', 'Multi-lingual Evaluation Task ( MET )']]
trues: [['approach', 'Used_for', 'Multi-lingual Evaluation Task ( MET )'], ['Multi-lingual Evaluation Task ( MET )', 'Used_for', 'Japanese text'], ['morphological analysis problem', 'Used_for', 'task'], ['Japanese', 'Used_for', 'morphological analysis problem']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.36786188579017265, 0.36495388669301715, 0.3664021164021164]
text: We introduce a novel method of shape constrained image segmentation which is based on mixtures of feature distributions for color and texture as well as probabilistic shape knowledge .
result: extract = Extract([Triple(Method("mixtures of feature distributions"), Rel("Used_for"), Task("shape constrained image segmentation")), Triple(Other_scientific_term("color"), Rel("Part_of"), Method("mixtures of feature distributions")), Triple(Other_scientific_term("color"), Rel("Conjunction"), Other_scientific_term("texture")), Triple(Other_scientific_term("texture"), Rel("Part_of"), Method("mixtures of feature distributions")), Triple(Other_scientific_term("probabilistic shape knowledge"), Rel("Part_of"), Method("mixtures of feature distributions"))])
pred: [['mixtures of feature distributions', 'Used_for', 'shape constrained image segmentation'], ['color', 'Part_of', 'mixtures of feature distributions'], ['color', 'Conjunction', 'texture'], ['texture', 'Part_of', 'mixtures of feature distributions'], ['probabilistic shape knowledge', 'Part_of', 'mixtures of feature distributions']]
trues: [['method', 'Used_for', 'shape constrained image segmentation'], ['mixtures of feature distributions', 'Used_for', 'method'], ['mixtures of feature distributions', 'Used_for', 'color'], ['mixtures of feature distributions', 'Used_for', 'texture'], ['mixtures of feature distributions', 'Used_for', 'probabilistic shape knowledge'], ['color', 'Conjunction', 'texture'], ['texture', 'Conjunction', 'probabilistic shape knowledge']]
metrics_sample: [0.2, 0.14285714285714285, 0.16666666666666666]
metrics_current: [0.36675461741424803, 0.3629242819843342, 0.36482939632545935]
text: In this paper , we propose a novel moderate positive sample mining method to train robust CNN for person re-identification , dealing with the problem of large variation .
result: extract = Extract([Triple(Method("moderate positive sample mining method"), Rel("Used_for"), Method("CNN")), Triple(Method("CNN"), Rel("Used_for"), Task("person re-identification")), Triple(Other_scientific_term("large variation"), Rel("Feature_of"), Task("person re-identification"))])
pred: [['moderate positive sample mining method', 'Used_for', 'CNN'], ['CNN', 'Used_for', 'person re-identification'], ['large variation', 'Feature_of', 'person re-identification']]
trues: [['moderate positive sample mining method', 'Used_for', 'robust CNN'], ['robust CNN', 'Used_for', 'person re-identification']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.36530880420499345, 0.3619791666666667, 0.3636363636363636]
text: In this paper , we present our work on the detection of question-answer pairs in an email conversation for the task of email summarization .
result: extract = Extract([Triple(Task("detection of question-answer pairs"), Rel("Used_for"), Task("email summarization"))])
pred: [['detection of question-answer pairs', 'Used_for', 'email summarization']]
trues: [['detection of question-answer pairs', 'Used_for', 'email summarization'], ['email conversation', 'Used_for', 'detection of question-answer pairs']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3661417322834646, 0.36233766233766235, 0.36422976501305476]
text: We investigate the verbal and nonverbal means for grounding , and propose a design for embodied conversational agents that relies on both kinds of signals to establish common ground in human-computer interaction .
result: extract = Extract([Triple(Other_scientific_term("verbal and nonverbal means"), Rel("Used_for"), Task("grounding")), Triple(Method("embodied conversational agents"), Rel("Used_for"), Task("grounding")), Triple(Other_scientific_term("verbal and nonverbal means"), Rel("Conjunction"), Other_scientific_term("signals")), Triple(Other_scientific_term("signals"), Rel("Used_for"), Task("human-computer interaction"))])
pred: [['verbal and nonverbal means', 'Used_for', 'grounding'], ['embodied conversational agents', 'Used_for', 'grounding'], ['verbal and nonverbal means', 'Conjunction', 'signals'], ['signals', 'Used_for', 'human-computer interaction']]
trues: [['verbal and nonverbal means', 'Used_for', 'grounding'], ['design', 'Used_for', 'embodied conversational agents'], ['common ground', 'Used_for', 'human-computer interaction']]
metrics_sample: [0.25, 0.3333333333333333, 0.28571428571428575]
metrics_current: [0.36553524804177545, 0.36222509702457956, 0.363872644574399]
text: Related work applies MINPRAN to complex range and intensity data 23 ] .
result: extract = Extract([Triple(Method("MINPRAN"), Rel("Used_for"), Other_scientific_term("complex range and intensity data"))])
pred: [['MINPRAN', 'Used_for', 'complex range and intensity data']]
trues: [['MINPRAN', 'Used_for', 'complex range'], ['MINPRAN', 'Used_for', 'intensity data']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3650586701434159, 0.36129032258064514, 0.3631647211413748]
text: We explain this distinction and we show how this model may be applied to the interpretation of compounds in real texts , provided that complementary semantic information are retrieved .
result: extract = Extract([Triple(Generic("model"), Rel("Used_for"), Task("interpretation of compounds")), Triple(Other_scientific_term("complementary semantic information"), Rel("Used_for"), Generic("model"))])
pred: [['model', 'Used_for', 'interpretation of compounds'], ['complementary semantic information', 'Used_for', 'model']]
trues: [['model', 'Used_for', 'interpretation of compounds']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.36540962288686607, 0.36211340206185566, 0.36375404530744343]
text: They improve the reconstruction results and enforce their consistency with a priori knowledge about object shape .
result: extract = Extract([Triple(Generic("They"), Rel("Used_for"), Other_scientific_term("reconstruction results")), Triple(Generic("They"), Rel("Used_for"), Other_scientific_term("object shape")), Triple(Other_scientific_term("apriori knowledge"), Rel("Feature_of"), Other_scientific_term("object shape"))])
pred: [['They', 'Used_for', 'reconstruction results'], ['They', 'Used_for', 'object shape'], ['apriori knowledge', 'Feature_of', 'object shape']]
trues: [['They', 'Used_for', 'reconstruction'], ['object shape', 'Feature_of', 'priori knowledge']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3639896373056995, 0.36118251928020567, 0.3625806451612903]
text: This reflects contemporary methods for unsupervised object discovery from groups of images , and it allows us to define intuitive evaluation met-rics for our sets of segmentations based on the accurate and parsimonious delineation of scene objects .
result: extract = Extract([Triple(Metric("evaluation met-rics"), Rel("Evaluate_for"), Method("sets of segmentations")), Triple(Other_scientific_term("scene objects"), Rel("Used_for"), Metric("evaluation met-rics"))])
pred: [['evaluation met-rics', 'Evaluate_for', 'sets of segmentations'], ['scene objects', 'Used_for', 'evaluation met-rics']]
trues: [['contemporary methods', 'Used_for', 'unsupervised object discovery']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.36304909560723514, 0.3607188703465982, 0.3618802318094011]
text: Language resource quality is crucial in NLP .
result: extract = Extract([Triple(Metric("Language resource quality"), Rel("Evaluate_for"), Task("NLP"))])
pred: [['Language resource quality', 'Evaluate_for', 'NLP']]
trues: [['Language resource quality', 'Feature_of', 'NLP']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.36258064516129035, 0.36025641025641025, 0.36141479099678453]
text: As a Monte Carlo approach , ASA stochastically samples the parameter space , in contrast to local deterministic search .
result: extract = Extract([Triple(Method("Monte Carlo approach"), Rel("Used_for"), Other_scientific_term("parameter space")), Triple(Method("ASA"), Rel("Hyponym_of"), Method("Monte Carlo approach")), Triple(Method("local deterministic search"), Rel("Compare"), Method("ASA"))])
pred: [['Monte Carlo approach', 'Used_for', 'parameter space'], ['ASA', 'Hyponym_of', 'Monte Carlo approach'], ['local deterministic search', 'Compare', 'ASA']]
trues: [['ASA', 'Hyponym_of', 'Monte Carlo approach'], ['ASA', 'Compare', 'local deterministic search']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.36246786632390743, 0.36061381074168797, 0.36153846153846153]
text: To improve topical blog post retrieval we incorporate textual credibility indicators in the retrieval process .
result: extract = Extract([Triple(Other_scientific_term("textual credibility indicators"), Rel("Used_for"), Task("topical blog post retrieval"))])
pred: [['textual credibility indicators', 'Used_for', 'topical blog post retrieval']]
trues: [['textual credibility indicators', 'Used_for', 'topical blog post retrieval'], ['textual credibility indicators', 'Part_of', 'retrieval process']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3632862644415918, 0.36096938775510207, 0.36212412028151]
text: We further demonstrate synthesis from a freely translating virtual camera .
result: extract = Extract([Triple(Other_scientific_term("freely translating virtual camera"), Rel("Used_for"), Task("synthesis"))])
pred: [['freely translating virtual camera', 'Used_for', 'synthesis']]
trues: [['translating virtual camera', 'Used_for', 'synthesis']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3628205128205128, 0.36050955414012736, 0.3616613418530351]
text: Memo-functions also facilitate a simple way to construct a very compact representation of the parse forest .
result: extract = Extract([Triple(Other_scientific_term("Memo-functions"), Rel("Used_for"), Other_scientific_term("compact representation")), Triple(Other_scientific_term("parse forest"), Rel("Feature_of"), Other_scientific_term("compact representation"))])
pred: [['Memo-functions', 'Used_for', 'compact representation'], ['parse forest', 'Feature_of', 'compact representation']]
trues: [['Memo-functions', 'Used_for', 'parse forest']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3618925831202046, 0.36005089058524176, 0.36096938775510207]
text: This model was shown to be more appropriate for blind separation of natural speech signals .
result: extract = Extract([Triple(Generic("model"), Rel("Used_for"), Task("blind separation of natural speech signals"))])
pred: [['model', 'Used_for', 'blind separation of natural speech signals']]
trues: [['model', 'Used_for', 'blind separation of natural speech signals .']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3614303959131545, 0.3595933926302414, 0.36050955414012736]
text: This has the advantages of efficiency on grammatical input , and robustness in the face of ungrammatical input .
result: extract = Extract([Triple(Generic("This"), Rel("Used_for"), Material("grammatical input")), Triple(Generic("This"), Rel("Used_for"), Material("ungrammatical input")), Triple(Material("ungrammatical input"), Rel("Feature_of"), Material("grammatical input"))])
pred: [['This', 'Used_for', 'grammatical input'], ['This', 'Used_for', 'ungrammatical input'], ['ungrammatical input', 'Feature_of', 'grammatical input']]
trues: [['ungrammatical input', 'Feature_of', 'robustness']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.36005089058524176, 0.35913705583756345, 0.3595933926302414]
text: Indeed , automatic evaluations need high-quality data that allow the comparison of both automatic and human translations .
result: extract = Extract([Triple(Material("high-quality data"), Rel("Used_for"), Metric("automatic evaluations")), Triple(Material("high-quality data"), Rel("Used_for"), Other_scientific_term("automatic and human translations"))])
pred: [['high-quality data', 'Used_for', 'automatic evaluations'], ['high-quality data', 'Used_for', 'automatic and human translations']]
trues: [['high-quality data', 'Evaluate_for', 'automatic evaluations']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35913705583756345, 0.35868187579214195, 0.35890932149651233]
text: During normal tracking conditions when the object is visible from frame to frame , local optimization is used to track the local mode of the similarity measure in a parameter space of translation , rotation and scale .
result: extract = Extract([Triple(Method("local optimization"), Rel("Used_for"), Task("tracking")), Triple(Other_scientific_term("translation"), Rel("Part_of"), Other_scientific_term("parameter space")), Triple(Other_scientific_term("translation"), Rel("Conjunction"), Other_scientific_term("rotation")), Triple(Other_scientific_term("rotation"), Rel("Part_of"), Other_scientific_term("parameter space")), Triple(Other_scientific_term("rotation"), Rel("Conjunction"), Other_scientific_term("scale")), Triple(Other_scientific_term("scale"), Rel("Part_of"), Other_scientific_term("parameter space"))])
pred: [['local optimization', 'Used_for', 'tracking'], ['translation', 'Part_of', 'parameter space'], ['translation', 'Conjunction', 'rotation'], ['rotation', 'Part_of', 'parameter space'], ['rotation', 'Conjunction', 'scale'], ['scale', 'Part_of', 'parameter space']]
trues: [['local optimization', 'Used_for', 'local mode of the similarity measure'], ['parameter space of translation , rotation and scale', 'Used_for', 'local mode of the similarity measure']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35642317380352645, 0.3577749683944374, 0.35709779179810724]
text: To associate a sequence of the Fujisaki-model parameters with a text input based on statistical learning , this paper proposes extending this model to a context-dependent one .
result: extract = Extract([Triple(Method("Fujisaki-model"), Rel("Used_for"), Other_scientific_term("sequence of the Fujisaki-model parameters")), Triple(Method("Fujisaki-model"), Rel("Used_for"), Other_scientific_term("text input")), Triple(Method("Fujisaki-model"), Rel("Used_for"), Method("context-dependent one")), Triple(Method("statistical learning"), Rel("Used_for"), Method("Fujisaki-model")), Triple(Method("statistical learning"), Rel("Used_for"), Method("context-dependent one"))])
pred: [['Fujisaki-model', 'Used_for', 'sequence of the Fujisaki-model parameters'], ['Fujisaki-model', 'Used_for', 'text input'], ['Fujisaki-model', 'Used_for', 'context-dependent one'], ['statistical learning', 'Used_for', 'Fujisaki-model'], ['statistical learning', 'Used_for', 'context-dependent one']]
trues: [['text input', 'Used_for', 'Fujisaki-model parameters'], ['statistical learning', 'Used_for', 'Fujisaki-model parameters']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3541927409261577, 0.35687263556116017, 0.35552763819095484]
text: Specifically , we show how to incorporate a simple prior on the distribution of natural images into support vector machines .
result: extract = Extract([Triple(Other_scientific_term("prior"), Rel("Used_for"), Method("support vector machines")), Triple(Other_scientific_term("prior"), Rel("Used_for"), Other_scientific_term("distribution of natural images")), Triple(Other_scientific_term("distribution of natural images"), Rel("Feature_of"), Other_scientific_term("distribution of natural images"))])
pred: [['prior', 'Used_for', 'support vector machines'], ['prior', 'Used_for', 'distribution of natural images'], ['distribution of natural images', 'Feature_of', 'distribution of natural images']]
trues: [['prior on the distribution of natural images', 'Used_for', 'support vector machines']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35286783042394015, 0.35642317380352645, 0.3546365914786967]
text: Finally , a novel feature weighting and selection function is presented , which yields superior feature vectors and better word similarity performance .
result: extract = Extract([Triple(Method("feature weighting and selection function"), Rel("Used_for"), Other_scientific_term("feature vectors")), Triple(Method("feature weighting and selection function"), Rel("Used_for"), Metric("word similarity")), Triple(Other_scientific_term("feature vectors"), Rel("Conjunction"), Metric("word similarity"))])
pred: [['feature weighting and selection function', 'Used_for', 'feature vectors'], ['feature weighting and selection function', 'Used_for', 'word similarity'], ['feature vectors', 'Conjunction', 'word similarity']]
trues: [['feature weighting and selection function', 'Used_for', 'feature vectors'], ['feature weighting and selection function', 'Used_for', 'word similarity'], ['feature vectors', 'Conjunction', 'word similarity']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3552795031055901, 0.3588456712672522, 0.3570536828963795]
text: These models simulate the reading time advantage for parallel structures found in human data , and also yield a small increase in overall parsing accuracy .
result: extract = Extract([Triple(Generic("models"), Rel("Used_for"), Other_scientific_term("reading time advantage")), Triple(Other_scientific_term("parallel structures"), Rel("Feature_of"), Material("human data"))])
pred: [['models', 'Used_for', 'reading time advantage'], ['parallel structures', 'Feature_of', 'human data']]
trues: [['parallel structures', 'Part_of', 'human data']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3543990086741016, 0.3583959899749373, 0.356386292834891]
text: A critical step in encoding sound for neuronal processing occurs when the analog pressure wave is coded into discrete nerve-action potentials .
result: extract = Extract([Triple(Other_scientific_term("analog pressure wave"), Rel("Used_for"), Other_scientific_term("nerve-action potentials"))])
pred: [['analog pressure wave', 'Used_for', 'nerve-action potentials']]
trues: [['encoding sound', 'Used_for', 'neuronal processing'], ['discrete nerve-action potentials', 'Used_for', 'analog pressure wave']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35396039603960394, 0.3575, 0.3557213930348258]
text: We also found that OA is crucial for auditory processing by onset neurons ( ONs ) in the next neuronal stage , the auditory brainstem .
result: extract = Extract([Triple(Other_scientific_term("OA"), Rel("Used_for"), Task("auditory processing")), Triple(Other_scientific_term("OA"), Rel("Used_for"), Other_scientific_term("onset neurons ( ONs )")), Triple(Other_scientific_term("onset neurons ( ONs )"), Rel("Part_of"), Other_scientific_term("auditory brainstem"))])
pred: [['OA', 'Used_for', 'auditory processing'], ['OA', 'Used_for', 'onset neurons ( ONs )'], ['onset neurons ( ONs )', 'Part_of', 'auditory brainstem']]
trues: [['OA', 'Used_for', 'auditory processing'], ['onset neurons ( ONs )', 'Used_for', 'OA']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.35388409371146734, 0.35785536159601, 0.35585864848109117]
text: Training instances are generated from experience and observation , and a variant of GOLEM is used to learn action models from these instances .
result: extract = Extract([Triple(Other_scientific_term("Training instances"), Rel("Used_for"), Method("action models")), Triple(Method("GOLEM"), Rel("Used_for"), Method("action models"))])
pred: [['Training instances', 'Used_for', 'action models'], ['GOLEM', 'Used_for', 'action models']]
trues: [['GOLEM', 'Used_for', 'action models']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.35424354243542433, 0.3586550435865504, 0.3564356435643564]
text: The formalism 's intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary , we intend it to allow TAGs to be used beyond their role in syntax proper .
result: extract = Extract([Triple(Other_scientific_term("TAGs"), Rel("Used_for"), Other_scientific_term("natural languages")), Triple(Other_scientific_term("TAGs"), Rel("Used_for"), Other_scientific_term("natural language"))])
pred: [['TAGs', 'Used_for', 'natural languages'], ['TAGs', 'Used_for', 'natural language']]
trues: [['logical form language', 'Used_for', 'semantics'], ['TAGs', 'Used_for', 'syntax proper']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35337423312883437, 0.3577639751552795, 0.35555555555555557]
text: OA improved phase locking in the auditory nerve ( AN ) and raised ASR accuracy for features derived from AN fibers ( ANFs ) .
result: extract = Extract([Triple(Metric("OA"), Rel("Used_for"), Other_scientific_term("phase locking")), Triple(Metric("OA"), Rel("Used_for"), Other_scientific_term("auditory nerve ( AN )")), Triple(Metric("OA"), Rel("Used_for"), Other_scientific_term("ANFs")), Triple(Other_scientific_term("phase locking"), Rel("Feature_of"), Other_scientific_term("auditory nerve ( AN )")), Triple(Other_scientific_term("phase locking"), Rel("Feature_of"), Other_scientific_term("ANFs")), Triple(Other_scientific_term("ANFs"), Rel("Used_for"), Task("ASR"))])
pred: [['OA', 'Used_for', 'phase locking'], ['OA', 'Used_for', 'auditory nerve ( AN )'], ['OA', 'Used_for', 'ANFs'], ['phase locking', 'Feature_of', 'auditory nerve ( AN )'], ['phase locking', 'Feature_of', 'ANFs'], ['ANFs', 'Used_for', 'ASR']]
trues: [['OA', 'Used_for', 'phase locking in the auditory nerve ( AN )'], ['OA', 'Used_for', 'features'], ['ASR accuracy', 'Evaluate_for', 'features'], ['AN fibers ( ANFs )', 'Used_for', 'features']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3507917174177832, 0.35599505562422745, 0.35337423312883437]
text: This paper presents a critical discussion of the various approaches that have been used in the evaluation of Natural Language systems .
result: extract = Extract([Triple(Generic("approaches"), Rel("Used_for"), Task("evaluation of Natural Language systems"))])
pred: [['approaches', 'Used_for', 'evaluation of Natural Language systems']]
trues: [['approaches', 'Used_for', 'evaluation of Natural Language systems']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3515815085158151, 0.3567901234567901, 0.3541666666666667]
text: We use novel priors , generate sparse and dense detection maps and our results show high detection rate with rejection to pathological motion and occlusion .
result: extract = Extract([Triple(Other_scientific_term("priors"), Rel("Used_for"), Generic("results")), Triple(Other_scientific_term("priors"), Rel("Conjunction"), Method("sparse and dense detection maps")), Triple(Method("sparse and dense detection maps"), Rel("Used_for"), Generic("results"))])
pred: [['priors', 'Used_for', 'results'], ['priors', 'Conjunction', 'sparse and dense detection maps'], ['sparse and dense detection maps', 'Used_for', 'results']]
trues: [['priors', 'Used_for', 'sparse and dense detection maps'], ['pathological motion', 'Conjunction', 'occlusion']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3503030303030303, 0.35591133004926107, 0.35308491142333537]
text: The task of machine translation ( MT ) evaluation is closely related to the task of sentence-level semantic equivalence classification .
result: extract = Extract([Triple(Task("machine translation ( MT ) evaluation"), Rel("Conjunction"), Task("sentence-level semantic equivalence classification"))])
pred: [['machine translation ( MT ) evaluation', 'Conjunction', 'sentence-level semantic equivalence classification']]
trues: [['machine translation ( MT ) evaluation', 'Conjunction', 'sentence-level semantic equivalence classification']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35108958837772397, 0.35670356703567035, 0.35387431360585725]
text: When the scene exhibits a persistent dynamic behavior in time , such an assumption is violated and detection performance deteriorates .
result: extract = Extract([Triple(Other_scientific_term("persistent dynamic behavior"), Rel("Feature_of"), Other_scientific_term("scene"))])
pred: [['persistent dynamic behavior', 'Feature_of', 'scene']]
trues: [['persistent dynamic behavior', 'Feature_of', 'scene']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3518742442563482, 0.3574938574938575, 0.35466179159049355]
text: It has also been studied in the framework of Japanese information extraction ( [ 3 ] ) in recent years .
result: extract = Extract([Triple(Generic("It"), Rel("Used_for"), Task("Japanese information extraction")), Triple(Task("Japanese information extraction"), Rel("Hyponym_of"), Task("information extraction")), Triple(Task("Japanese information extraction"), Rel("Used_for"), Generic("It"))])
pred: [['It', 'Used_for', 'Japanese information extraction'], ['Japanese information extraction', 'Hyponym_of', 'information extraction'], ['Japanese information extraction', 'Used_for', 'It']]
trues: [['Japanese information extraction', 'Used_for', 'It']]
metrics_sample: [0.3333333333333333, 1.0, 0.5]
metrics_current: [0.35180722891566263, 0.35828220858895704, 0.3550151975683891]
text: Among other experiments , we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the KITTI2012 dataset by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data .
result: extract = Extract([Triple(Generic("approach"), Rel("Used_for"), Method("confidence measures")), Triple(Material("KITTI2012 dataset"), Rel("Evaluate_for"), Method("confidence measures")), Triple(Material("laser ground truth data"), Rel("Used_for"), Method("confidence measures")), Triple(Material("laser ground truth data"), Rel("Conjunction"), Material("automatically generated training data")), Triple(Material("automatically generated training data"), Rel("Used_for"), Method("confidence measures"))])
pred: [['approach', 'Used_for', 'confidence measures'], ['KITTI2012 dataset', 'Evaluate_for', 'confidence measures'], ['laser ground truth data', 'Used_for', 'confidence measures'], ['laser ground truth data', 'Conjunction', 'automatically generated training data'], ['automatically generated training data', 'Used_for', 'confidence measures']]
trues: [['approach', 'Used_for', 'learned confidence measures'], ['KITTI2012 dataset', 'Evaluate_for', 'learned confidence measures'], ['automatically generated training data', 'Used_for', 'them'], ['laser ground truth data', 'Compare', 'automatically generated training data']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.34970059880239523, 0.35653235653235654, 0.3530834340991536]
text: A language learning experiment showed that assessors can differentiate native from non-native language essays in less than 100 words .
result: extract = Extract([Triple(Material("native from non-native language essays"), Rel("Used_for"), Task("language learning experiment"))])
pred: [['native from non-native language essays', 'Used_for', 'language learning experiment']]
trues: [['language learning', 'Evaluate_for', 'assessors']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3492822966507177, 0.35609756097560974, 0.35265700483091783]
text: We have built and will demonstrate an application of this approach called LCS-Marine .
result: extract = Extract([Triple(Generic("approach"), Rel("Used_for"), Generic("application"))])
pred: [['approach', 'Used_for', 'application']]
trues: [['approach', 'Used_for', 'LCS-Marine']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.34886499402628435, 0.3556638246041413, 0.3522316043425814]
text: Topic signatures can be useful in a number of Natural Language Processing ( NLP ) applications , such as Word Sense Disambiguation ( WSD ) and Text Summarisation .
result: extract = Extract([Triple(Other_scientific_term("Topic signatures"), Rel("Used_for"), Task("Natural Language Processing ( NLP ) applications")), Triple(Task("Word Sense Disambiguation ( WSD )"), Rel("Hyponym_of"), Task("Natural Language Processing ( NLP ) applications")), Triple(Task("Word Sense Disambiguation ( WSD )"), Rel("Conjunction"), Task("Text Summarisation")), Triple(Task("Text Summarisation"), Rel("Hyponym_of"), Task("Natural Language Processing ( NLP ) applications"))])
pred: [['Topic signatures', 'Used_for', 'Natural Language Processing ( NLP ) applications'], ['Word Sense Disambiguation ( WSD )', 'Hyponym_of', 'Natural Language Processing ( NLP ) applications'], ['Word Sense Disambiguation ( WSD )', 'Conjunction', 'Text Summarisation'], ['Text Summarisation', 'Hyponym_of', 'Natural Language Processing ( NLP ) applications']]
trues: [['Topic signatures', 'Used_for', 'Natural Language Processing ( NLP ) applications'], ['Topic signatures', 'Used_for', 'Word Sense Disambiguation ( WSD )'], ['Topic signatures', 'Used_for', 'Text Summarisation'], ['Word Sense Disambiguation ( WSD )', 'Hyponym_of', 'Natural Language Processing ( NLP ) applications'], ['Word Sense Disambiguation ( WSD )', 'Conjunction', 'Text Summarisation'], ['Text Summarisation', 'Hyponym_of', 'Natural Language Processing ( NLP ) applications']]
metrics_sample: [1.0, 0.6666666666666666, 0.8]
metrics_current: [0.351961950059453, 0.35792019347037485, 0.35491606714628293]
text: Here we develop an approach for 1 distance that begins with an explicit and exactly distance-preserving embedding of the points into 2 2 .
result: extract = Extract([Triple(Method("explicit and exactly distance-preserving embedding"), Rel("Used_for"), Generic("approach")), Triple(Other_scientific_term("points"), Rel("Feature_of"), Method("explicit and exactly distance-preserving embedding")), Triple(Other_scientific_term("points"), Rel("Used_for"), Generic("approach")), Triple(Other_scientific_term("2"), Rel("Feature_of"), Other_scientific_term("points"))])
pred: [['explicit and exactly distance-preserving embedding', 'Used_for', 'approach'], ['points', 'Feature_of', 'explicit and exactly distance-preserving embedding'], ['points', 'Used_for', 'approach'], ['2', 'Feature_of', 'points']]
trues: [['approach', 'Used_for', '1 distance']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3502958579881657, 0.357487922705314, 0.35385534967124926]
text: This paper deals with the problem of generating the fundamental frequency ( F0 ) contour of speech from a text input for text-to-speech synthesis .
result: extract = Extract([Triple(Material("text input"), Rel("Used_for"), Other_scientific_term("fundamental frequency ( F0 ) contour of speech")), Triple(Material("text input"), Rel("Used_for"), Task("text-to-speech synthesis"))])
pred: [['text input', 'Used_for', 'fundamental frequency ( F0 ) contour of speech'], ['text input', 'Used_for', 'text-to-speech synthesis']]
trues: [['fundamental frequency ( F0 ) contour of speech', 'Used_for', 'text-to-speech synthesis'], ['text input', 'Used_for', 'fundamental frequency ( F0 ) contour of speech']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.35064935064935066, 0.3578313253012048, 0.3542039355992844]
text: This paper describes the impact of using different-quality references on evaluation .
result: extract = Extract([Triple(Other_scientific_term("different-quality references"), Rel("Used_for"), Task("evaluation"))])
pred: [['different-quality references', 'Used_for', 'evaluation']]
trues: [['different-quality references', 'Used_for', 'evaluation']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35141509433962265, 0.358604091456077, 0.35497319833234064]
text: Using this approach , we extract parallel data from large Chinese , Arabic , and English non-parallel newspaper corpora .
result: extract = Extract([Triple(Generic("approach"), Rel("Used_for"), Material("parallel data")), Triple(Material("Chinese , Arabic , and English non-parallel newspaper corpora"), Rel("Used_for"), Generic("approach"))])
pred: [['approach', 'Used_for', 'parallel data'], ['Chinese , Arabic , and English non-parallel newspaper corpora', 'Used_for', 'approach']]
trues: [['approach', 'Used_for', 'parallel data'], ['parallel data', 'Part_of', 'Chinese , Arabic , and English non-parallel newspaper corpora']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3517647058823529, 0.3589435774309724, 0.35531788472964937]
text: In this paper we propose a logical formalism , which , among other things , is suitable for representing determiners without forcing a particular interpretation when their meaning is still not clear .
result: extract = Extract([Triple(Method("logical formalism"), Rel("Used_for"), Other_scientific_term("determiners"))])
pred: [['logical formalism', 'Used_for', 'determiners']]
trues: [['logical formalism', 'Used_for', 'determiners']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3525264394829612, 0.3597122302158273, 0.35608308605341243]
text: We show that there is a closed-form ( analytical ) solution for one part of the Kullback-Leibler distance , viz the cross-entropy .
result: extract = Extract([Triple(Other_scientific_term("cross-entropy"), Rel("Hyponym_of"), Other_scientific_term("Kullback-Leibler distance"))])
pred: [['cross-entropy', 'Hyponym_of', 'Kullback-Leibler distance']]
trues: [['closed-form ( analytical ) solution', 'Used_for', 'Kullback-Leibler distance'], ['closed-form ( analytical ) solution', 'Used_for', 'cross-entropy'], ['cross-entropy', 'Part_of', 'Kullback-Leibler distance']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.352112676056338, 0.35842293906810035, 0.35523978685612784]
text: Finally , we show how the estimation of the tensors from point correspondences is achieved through factorization , and discuss the estimation from line correspondences .
result: extract = Extract([Triple(Method("factorization"), Rel("Used_for"), Task("estimation from line correspondences"))])
pred: [['factorization', 'Used_for', 'estimation from line correspondences']]
trues: [['point correspondences', 'Used_for', 'estimation of the tensors'], ['factorization', 'Used_for', 'tensors'], ['line correspondences', 'Used_for', 'estimation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3516998827667057, 0.35714285714285715, 0.3544004725339633]
text: Our goal is to learn a Mahalanobis distance by minimizing a loss defined on the weighted sum of the precision at different ranks .
result: extract = Extract([Triple(Metric("weighted sum of the precision at different ranks"), Rel("Used_for"), Method("Mahalanobis distance"))])
pred: [['weighted sum of the precision at different ranks', 'Used_for', 'Mahalanobis distance']]
trues: [['loss', 'Used_for', 'Mahalanobis distance'], ['weighted sum', 'Feature_of', 'precision']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.351288056206089, 0.35629453681710216, 0.3537735849056604]
text: This paper presents necessary and sufficient conditions for the use of demonstrative expressions in English and discusses implications for current discourse processing algorithms .
result: extract = Extract([Triple(Other_scientific_term("demonstrative expressions"), Rel("Used_for"), Other_scientific_term("English")), Triple(Method("discourse processing algorithms"), Rel("Used_for"), Other_scientific_term("demonstrative expressions"))])
pred: [['demonstrative expressions', 'Used_for', 'English'], ['discourse processing algorithms', 'Used_for', 'demonstrative expressions']]
trues: [['demonstrative expressions', 'Feature_of', 'English'], ['implications', 'Used_for', 'discourse processing algorithms']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35046728971962615, 0.35545023696682465, 0.3529411764705882]
text: Experimental results on our dataset and the public G3D dataset both demonstrate very promising performance of our scheme .
result: extract = Extract([Triple(Generic("dataset"), Rel("Evaluate_for"), Generic("scheme")), Triple(Generic("dataset"), Rel("Conjunction"), Material("G3D dataset")), Triple(Material("G3D dataset"), Rel("Evaluate_for"), Generic("scheme"))])
pred: [['dataset', 'Evaluate_for', 'scheme'], ['dataset', 'Conjunction', 'G3D dataset'], ['G3D dataset', 'Evaluate_for', 'scheme']]
trues: [['dataset', 'Conjunction', 'G3D dataset']]
metrics_sample: [0.3333333333333333, 1.0, 0.5]
metrics_current: [0.35040745052386496, 0.3562130177514793, 0.3532863849765258]
text: Inherent ambiguities in the computation of features are addressed by using a data-dependent bandwidth for density estimation using kernels .
result: extract = Extract([Triple(Other_scientific_term("data-dependent bandwidth"), Rel("Used_for"), Method("density estimation")), Triple(Other_scientific_term("data-dependent bandwidth"), Rel("Used_for"), Other_scientific_term("features")), Triple(Method("kernels"), Rel("Used_for"), Method("density estimation"))])
pred: [['data-dependent bandwidth', 'Used_for', 'density estimation'], ['data-dependent bandwidth', 'Used_for', 'features'], ['kernels', 'Used_for', 'density estimation']]
trues: [['ambiguities', 'Feature_of', 'computation of features'], ['data-dependent bandwidth', 'Used_for', 'ambiguities'], ['data-dependent bandwidth', 'Used_for', 'density estimation'], ['kernels', 'Used_for', 'density estimation']]
metrics_sample: [0.6666666666666666, 0.5, 0.5714285714285715]
metrics_current: [0.351508120649652, 0.3568904593639576, 0.35417884278199885]
text: This allows us to use our method to act not only as a faster procedure for cascade evaluation , but also as a tool to perform efficient branch-and-bound object detection with nonlinear quality functions , in particular kernel-ized support vector machines .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Task("cascade evaluation")), Triple(Generic("method"), Rel("Used_for"), Task("branch-and-bound object detection")), Triple(Other_scientific_term("nonlinear quality functions"), Rel("Feature_of"), Task("branch-and-bound object detection")), Triple(Method("kernel-ized support vector machines"), Rel("Hyponym_of"), Other_scientific_term("nonlinear quality functions"))])
pred: [['method', 'Used_for', 'cascade evaluation'], ['method', 'Used_for', 'branch-and-bound object detection'], ['nonlinear quality functions', 'Feature_of', 'branch-and-bound object detection'], ['kernel-ized support vector machines', 'Hyponym_of', 'nonlinear quality functions']]
trues: [['method', 'Used_for', 'cascade evaluation'], ['method', 'Used_for', 'branch-and-bound object detection'], ['nonlinear quality functions', 'Used_for', 'branch-and-bound object detection'], ['kernel-ized support vector machines', 'Hyponym_of', 'nonlinear quality functions']]
metrics_sample: [0.75, 0.75, 0.75]
metrics_current: [0.3533487297921478, 0.35873388042203985, 0.35602094240837695]
text: Given a new image , detection and classification are achieved by determining the position and viewpoint of the model that maximize recognition scores of the candidate objects .
result: extract = Extract([Triple(Other_scientific_term("position and viewpoint"), Rel("Used_for"), Task("detection and classification")), Triple(Other_scientific_term("position and viewpoint"), Rel("Used_for"), Other_scientific_term("model")), Triple(Other_scientific_term("recognition scores"), Rel("Used_for"), Other_scientific_term("candidate objects"))])
pred: [['position and viewpoint', 'Used_for', 'detection and classification'], ['position and viewpoint', 'Used_for', 'model'], ['recognition scores', 'Used_for', 'candidate objects']]
trues: [['image', 'Used_for', 'detection'], ['image', 'Used_for', 'classification'], ['detection', 'Conjunction', 'classification'], ['position', 'Used_for', 'detection'], ['position', 'Used_for', 'classification'], ['position', 'Conjunction', 'viewpoint'], ['viewpoint', 'Used_for', 'detection'], ['viewpoint', 'Used_for', 'classification']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3521288837744534, 0.3554006968641115, 0.35375722543352606]
text: Translations are produced by means of a beam-search decoder .
result: extract = Extract([Triple(Method("beam-search decoder"), Rel("Used_for"), Other_scientific_term("Translations"))])
pred: [['beam-search decoder', 'Used_for', 'Translations']]
trues: [['beam-search decoder', 'Used_for', 'Translations']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3528735632183908, 0.3561484918793503, 0.35450346420323325]
text: A demonstration ( in UNIX ) for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis ( ICALL ) , including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples .
result: extract = Extract([Triple(Method("Applied Natural Language Processing"), Rel("Used_for"), Task("intelligent computer-assisted morphological analysis ( ICALL )")), Triple(Task("intelligent computer-assisted morphological analysis ( ICALL )"), Rel("Used_for"), Other_scientific_term("disambiguated morphological analysis")), Triple(Task("intelligent computer-assisted morphological analysis ( ICALL )"), Rel("Used_for"), Other_scientific_term("lemmatized indexing")), Triple(Other_scientific_term("disambiguated morphological analysis"), Rel("Conjunction"), Other_scientific_term("lemmatized indexing")), Triple(Material("aligned bilingual corpus of word examples"), Rel("Used_for"), Other_scientific_term("disambiguated morphological analysis")), Triple(Material("aligned bilingual corpus of word examples"), Rel("Used_for"), Other_scientific_term("lemmatized indexing"))])
pred: [['Applied Natural Language Processing', 'Used_for', 'intelligent computer-assisted morphological analysis ( ICALL )'], ['intelligent computer-assisted morphological analysis ( ICALL )', 'Used_for', 'disambiguated morphological analysis'], ['intelligent computer-assisted morphological analysis ( ICALL )', 'Used_for', 'lemmatized indexing'], ['disambiguated morphological analysis', 'Conjunction', 'lemmatized indexing'], ['aligned bilingual corpus of word examples', 'Used_for', 'disambiguated morphological analysis'], ['aligned bilingual corpus of word examples', 'Used_for', 'lemmatized indexing']]
trues: [['components', 'Used_for', 'intelligent computer-assisted morphological analysis ( ICALL )'], ['disambiguated morphological analysis', 'Hyponym_of', 'components'], ['disambiguated morphological analysis', 'Conjunction', 'lemmatized indexing'], ['disambiguated morphological analysis', 'Used_for', 'aligned bilingual corpus'], ['lemmatized indexing', 'Hyponym_of', 'components'], ['lemmatized indexing', 'Used_for', 'aligned bilingual corpus']]
metrics_sample: [0.16666666666666666, 0.16666666666666666, 0.16666666666666666]
metrics_current: [0.3515981735159817, 0.3548387096774194, 0.3532110091743119]
text: This paper presents a machine learning approach to bare slice disambiguation in dialogue .
result: extract = Extract([Triple(Method("machine learning approach"), Rel("Used_for"), Task("bare slice disambiguation")), Triple(Task("bare slice disambiguation"), Rel("Used_for"), Task("dialogue"))])
pred: [['machine learning approach', 'Used_for', 'bare slice disambiguation'], ['bare slice disambiguation', 'Used_for', 'dialogue']]
trues: [['machine learning approach', 'Used_for', 'bare slice disambiguation'], ['dialogue', 'Used_for', 'bare slice disambiguation']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.35193621867881547, 0.35517241379310344, 0.35354691075514877]
text: The purpose of this research is to test the efficacy of applying automated evaluation techniques , originally devised for the evaluation of human language learners , to the output of machine translation ( MT ) systems .
result: extract = Extract([Triple(Method("automated evaluation techniques"), Rel("Used_for"), Method("machine translation ( MT ) systems"))])
pred: [['automated evaluation techniques', 'Used_for', 'machine translation ( MT ) systems']]
trues: [['automated evaluation techniques', 'Used_for', 'evaluation of human language learners']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3515358361774744, 0.3547646383467279, 0.35314285714285715]
text: We introduce a method to accelerate the evaluation of object detection cascades with the help of a divide-and-conquer procedure in the space of candidate regions .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Task("evaluation of object detection cascades")), Triple(Other_scientific_term("divide-and-conquer procedure"), Rel("Used_for"), Generic("method")), Triple(Other_scientific_term("divide-and-conquer procedure"), Rel("Used_for"), Other_scientific_term("candidate regions"))])
pred: [['method', 'Used_for', 'evaluation of object detection cascades'], ['divide-and-conquer procedure', 'Used_for', 'method'], ['divide-and-conquer procedure', 'Used_for', 'candidate regions']]
trues: [['method', 'Used_for', 'evaluation of object detection cascades'], ['divide-and-conquer procedure', 'Used_for', 'method'], ['space of candidate regions', 'Feature_of', 'divide-and-conquer procedure']]
metrics_sample: [0.6666666666666666, 0.6666666666666666, 0.6666666666666666]
metrics_current: [0.3526077097505669, 0.35583524027459956, 0.35421412300683375]
text: The model is evaluated on English and Czech newspaper texts , and is then validated on French broadcast news transcriptions .
result: extract = Extract([Triple(Material("English and Czech newspaper texts"), Rel("Evaluate_for"), Generic("model")), Triple(Material("French broadcast news transcriptions"), Rel("Evaluate_for"), Generic("model"))])
pred: [['English and Czech newspaper texts', 'Evaluate_for', 'model'], ['French broadcast news transcriptions', 'Evaluate_for', 'model']]
trues: [['English and Czech newspaper texts', 'Evaluate_for', 'model'], ['French broadcast news transcriptions', 'Evaluate_for', 'model']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35407239819004527, 0.3573059360730594, 0.3556818181818182]
text: This paper highlights a particular class of miscommunication -- reference problems -- by describing a case study and techniques for avoiding failures of reference .
result: extract = Extract([Triple(Generic("class"), Rel("Hyponym_of"), Other_scientific_term("miscommunication")), Triple(Generic("class"), Rel("Conjunction"), Task("reference problems")), Triple(Task("reference problems"), Rel("Hyponym_of"), Other_scientific_term("miscommunication"))])
pred: [['class', 'Hyponym_of', 'miscommunication'], ['class', 'Conjunction', 'reference problems'], ['reference problems', 'Hyponym_of', 'miscommunication']]
trues: [['reference problems', 'Hyponym_of', 'miscommunication'], ['techniques', 'Used_for', 'failures of reference']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.35400225479143177, 0.357630979498861, 0.35580736543909347]
text: In this paper , we introduce an annotation scheme for scientific articles which can be used to build such a resource in a consistent way .
result: extract = Extract([Triple(Method("annotation scheme"), Rel("Used_for"), Material("scientific articles")), Triple(Method("annotation scheme"), Rel("Used_for"), Generic("resource")), Triple(Generic("resource"), Rel("Used_for"), Generic("consistent way"))])
pred: [['annotation scheme', 'Used_for', 'scientific articles'], ['annotation scheme', 'Used_for', 'resource'], ['resource', 'Used_for', 'consistent way']]
trues: [['annotation scheme', 'Used_for', 'scientific articles'], ['annotation scheme', 'Used_for', 'resource']]
metrics_sample: [0.6666666666666666, 1.0, 0.8]
metrics_current: [0.3550561797752809, 0.35909090909090907, 0.35706214689265536]
text: The integrated learning system has been experimentally validated in simulated construction and ooce domains .
result: extract = Extract([Triple(Method("integrated learning system"), Rel("Used_for"), Material("construction and ooce domains"))])
pred: [['integrated learning system', 'Used_for', 'construction and ooce domains']]
trues: [['simulated construction', 'Evaluate_for', 'integrated learning system'], ['simulated construction', 'Conjunction', 'ooce domains'], ['ooce domains', 'Evaluate_for', 'integrated learning system']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3546576879910213, 0.3578708946772367, 0.35625704622322435]
text: The experimental tests are carried out on the Verbmobil task ( German-English , 8000-word vocabulary ) , which is a limited-domain spoken-language task .
result: extract = Extract([Triple(Material("Verbmobil task"), Rel("Hyponym_of"), Task("limited-domain spoken-language task")), Triple(Material("German-English"), Rel("Feature_of"), Material("Verbmobil task")), Triple(Material("German-English"), Rel("Conjunction"), Material("8000-word vocabulary")), Triple(Material("8000-word vocabulary"), Rel("Feature_of"), Material("Verbmobil task"))])
pred: [['Verbmobil task', 'Hyponym_of', 'limited-domain spoken-language task'], ['German-English', 'Feature_of', 'Verbmobil task'], ['German-English', 'Conjunction', '8000-word vocabulary'], ['8000-word vocabulary', 'Feature_of', 'Verbmobil task']]
trues: [['Verbmobil task', 'Hyponym_of', 'limited-domain spoken-language task']]
metrics_sample: [0.25, 1.0, 0.4]
metrics_current: [0.3541899441340782, 0.3585972850678733, 0.3563799887577291]
text: A new algorithm is proposed for novel view generation in one-to-one teleconferencing applications .
result: extract = Extract([Triple(Generic("algorithm"), Rel("Used_for"), Task("one-to-one teleconferencing applications"))])
pred: [['algorithm', 'Used_for', 'one-to-one teleconferencing applications']]
trues: [['algorithm', 'Used_for', 'novel view generation'], ['novel view generation', 'Used_for', 'one-to-one teleconferencing applications']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35379464285714285, 0.3577878103837472, 0.3557800224466891]
text: We evaluate across two corpora ( conversational telephone speech and broadcast news speech ) on both human transcriptions and speech recognition output .
result: extract = Extract([Triple(Material("conversational telephone speech"), Rel("Evaluate_for"), Generic("corpora")), Triple(Material("conversational telephone speech"), Rel("Conjunction"), Material("broadcast news speech")), Triple(Material("broadcast news speech"), Rel("Evaluate_for"), Generic("corpora")), Triple(Other_scientific_term("human transcriptions"), Rel("Evaluate_for"), Generic("corpora")), Triple(Other_scientific_term("human transcriptions"), Rel("Conjunction"), Other_scientific_term("speech recognition output")), Triple(Other_scientific_term("speech recognition output"), Rel("Evaluate_for"), Generic("corpora"))])
pred: [['conversational telephone speech', 'Evaluate_for', 'corpora'], ['conversational telephone speech', 'Conjunction', 'broadcast news speech'], ['broadcast news speech', 'Evaluate_for', 'corpora'], ['human transcriptions', 'Evaluate_for', 'corpora'], ['human transcriptions', 'Conjunction', 'speech recognition output'], ['speech recognition output', 'Evaluate_for', 'corpora']]
trues: [['corpora', 'Evaluate_for', 'human transcriptions'], ['corpora', 'Evaluate_for', 'speech recognition output'], ['conversational telephone speech', 'Hyponym_of', 'corpora'], ['conversational telephone speech', 'Conjunction', 'broadcast news speech'], ['broadcast news speech', 'Hyponym_of', 'corpora'], ['human transcriptions', 'Conjunction', 'speech recognition output']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.35365853658536583, 0.3576233183856502, 0.3556298773690078]
text: Although the study of clustering is centered around an intuitively compelling goal , it has been very difficult to develop a unified framework for reasoning about it at a technical level , and profoundly diverse approaches to clustering abound in the research community .
result: extract = Extract([Triple(Task("clustering"), Rel("Used_for"), Generic("it"))])
pred: [['clustering', 'Used_for', 'it']]
trues: [['unified framework', 'Used_for', 'reasoning']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3532668881506091, 0.3572228443449048, 0.3552338530066815]
text: The fact that Turkish is an agglutinating free word order language presents a challenge for language theories .
result: extract = Extract([Triple(Other_scientific_term("agglutinating free word order language"), Rel("Feature_of"), Material("Turkish"))])
pred: [['agglutinating free word order language', 'Feature_of', 'Turkish']]
trues: [['Turkish', 'Hyponym_of', 'agglutinating free word order language']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3528761061946903, 0.3568232662192394, 0.3548387096774194]
text: Sentence boundary detection in speech is important for enriching speech recognition output , making it easier for humans to read and downstream modules to process .
result: extract = Extract([Triple(Task("Sentence boundary detection"), Rel("Used_for"), Task("speech recognition")), Triple(Task("Sentence boundary detection"), Rel("Used_for"), Task("speech recognition")), Triple(Task("speech recognition"), Rel("Used_for"), Task("downstream modules"))])
pred: [['Sentence boundary detection', 'Used_for', 'speech recognition'], ['Sentence boundary detection', 'Used_for', 'speech recognition'], ['speech recognition', 'Used_for', 'downstream modules']]
trues: [['Sentence boundary detection', 'Used_for', 'speech recognition output'], ['speech', 'Used_for', 'Sentence boundary detection']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35209713024282563, 0.3560267857142857, 0.3540510543840178]
text: A purely functional implementation of LR-parsers is given , together with a simple correctness proof .
result: extract = Extract([Triple(Method("functional implementation"), Rel("Used_for"), Method("LR-parsers"))])
pred: [['functional implementation', 'Used_for', 'LR-parsers']]
trues: [['correctness proof', 'Conjunction', 'LR-parsers']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35170893054024255, 0.3556298773690078, 0.35365853658536583]
text: Some of the extracts were expert human translations , others were machine translation outputs .
result: extract = Extract([Triple(Material("expert human translations"), Rel("Hyponym_of"), Other_scientific_term("extracts")), Triple(Material("expert human translations"), Rel("Conjunction"), Material("machine translation outputs")), Triple(Material("machine translation outputs"), Rel("Hyponym_of"), Other_scientific_term("extracts"))])
pred: [['expert human translations', 'Hyponym_of', 'extracts'], ['expert human translations', 'Conjunction', 'machine translation outputs'], ['machine translation outputs', 'Hyponym_of', 'extracts']]
trues: [['machine translation outputs', 'Conjunction', 'expert human translations']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3505494505494505, 0.35523385300668153, 0.35287610619469023]
text: Here , we leverage a logistic stick-breaking representation and recent innovations in Pólya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods , enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead .
result: extract = Extract([Triple(Other_scientific_term("logistic stick-breaking representation"), Rel("Used_for"), Other_scientific_term("multinomial distribution")), Triple(Other_scientific_term("logistic stick-breaking representation"), Rel("Conjunction"), Method("Pólya-gamma augmentation")), Triple(Method("Pólya-gamma augmentation"), Rel("Used_for"), Other_scientific_term("multinomial distribution")), Triple(Other_scientific_term("multinomial distribution"), Rel("Used_for"), Other_scientific_term("latent variables")), Triple(Other_scientific_term("jointly Gaussian likelihoods"), Rel("Used_for"), Other_scientific_term("multinomial distribution")), Triple(Other_scientific_term("jointly Gaussian likelihoods"), Rel("Feature_of"), Other_scientific_term("latent variables")), Triple(Method("Bayesian inference techniques"), Rel("Used_for"), Method("Gaussian models")), Triple(Method("Gaussian models"), Rel("Compare"), Generic("overhead"))])
pred: [['logistic stick-breaking representation', 'Used_for', 'multinomial distribution'], ['logistic stick-breaking representation', 'Conjunction', 'Pólya-gamma augmentation'], ['Pólya-gamma augmentation', 'Used_for', 'multinomial distribution'], ['multinomial distribution', 'Used_for', 'latent variables'], ['jointly Gaussian likelihoods', 'Used_for', 'multinomial distribution'], ['jointly Gaussian likelihoods', 'Feature_of', 'latent variables'], ['Bayesian inference techniques', 'Used_for', 'Gaussian models'], ['Gaussian models', 'Compare', 'overhead']]
trues: [['logistic stick-breaking representation', 'Used_for', 'multinomial distribution'], ['Pólya-gamma augmentation', 'Used_for', 'multinomial distribution'], ['latent variables', 'Part_of', 'multinomial distribution'], ['jointly Gaussian likelihoods', 'Feature_of', 'latent variables'], ['Bayesian inference techniques', 'Used_for', 'Gaussian models'], ['minimal overhead', 'Feature_of', 'Gaussian models']]
metrics_sample: [0.5, 0.6666666666666666, 0.5714285714285715]
metrics_current: [0.35185185185185186, 0.3573008849557522, 0.3545554335894621]
text: In addition , we conduct synthetic experiments as well as a real application on noisy image classification to show that our method also improves the performance in practice by exploiting side information .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Task("noisy image classification")), Triple(Other_scientific_term("side information"), Rel("Used_for"), Generic("method"))])
pred: [['method', 'Used_for', 'noisy image classification'], ['side information', 'Used_for', 'method']]
trues: [['noisy image classification', 'Evaluate_for', 'method'], ['side information', 'Used_for', 'method']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3521739130434783, 0.3576158940397351, 0.3548740416210296]
text: The strong description and modeling properties of differential features make them useful tools that can be efficiently used as constraints for 3 -- D reconstruction .
result: extract = Extract([Triple(Other_scientific_term("differential features"), Rel("Used_for"), Other_scientific_term("constraints")), Triple(Other_scientific_term("constraints"), Rel("Used_for"), Task("3 -- D reconstruction"))])
pred: [['differential features', 'Used_for', 'constraints'], ['constraints', 'Used_for', '3 -- D reconstruction']]
trues: [['them', 'Used_for', '3 -- D reconstruction']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.351409978308026, 0.3572216097023153, 0.3542919628212138]
text: Such a triangulation provides a piecewise-linear interpolant useful for light field super-resolution .
result: extract = Extract([Triple(Other_scientific_term("piecewise-linear interpolant"), Rel("Used_for"), Task("light field super-resolution"))])
pred: [['piecewise-linear interpolant', 'Used_for', 'light field super-resolution']]
trues: [['triangulation', 'Used_for', 'piecewise-linear interpolant'], ['piecewise-linear interpolant', 'Used_for', 'light field super-resolution']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.352112676056338, 0.35753575357535755, 0.3548034934497816]
text: Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods .
result: extract = Extract([Triple(Material("ACE corpora"), Rel("Evaluate_for"), Method("clustering methods")), Triple(Method("spectral clustering based approach"), Rel("Compare"), Method("clustering methods"))])
pred: [['ACE corpora', 'Evaluate_for', 'clustering methods'], ['spectral clustering based approach', 'Compare', 'clustering methods']]
trues: [['ACE corpora', 'Evaluate_for', 'spectral clustering based approach'], ['ACE corpora', 'Evaluate_for', 'clustering methods'], ['spectral clustering based approach', 'Compare', 'clustering methods']]
metrics_sample: [1.0, 0.6666666666666666, 0.8]
metrics_current: [0.3535135135135135, 0.35855263157894735, 0.3560152422427872]
text: This paper outlines Plume as it currently exists and describes our detailed design for extending Plume to handle passives , relative clauses , and interrogatives in a general manner .
result: extract = Extract([Triple(Generic("it"), Rel("Used_for"), Method("Plume")), Triple(Task("passives"), Rel("Conjunction"), Task("relative clauses")), Triple(Task("relative clauses"), Rel("Conjunction"), Task("interrogatives"))])
pred: [['it', 'Used_for', 'Plume'], ['passives', 'Conjunction', 'relative clauses'], ['relative clauses', 'Conjunction', 'interrogatives']]
trues: [['Plume', 'Used_for', 'passives'], ['Plume', 'Used_for', 'relative clauses'], ['Plume', 'Used_for', 'interrogatives'], ['passives', 'Conjunction', 'relative clauses'], ['relative clauses', 'Conjunction', 'interrogatives']]
metrics_sample: [0.6666666666666666, 0.4, 0.5]
metrics_current: [0.3545258620689655, 0.35877862595419846, 0.35663956639566397]
text: We show that the trainable sentence planner performs better than the rule-based systems and the baselines , and as well as the hand-crafted system .
result: extract = Extract([Triple(Method("trainable sentence planner"), Rel("Compare"), Method("rule-based systems")), Triple(Method("trainable sentence planner"), Rel("Compare"), Generic("baselines")), Triple(Method("rule-based systems"), Rel("Compare"), Method("baselines")), Triple(Method("rule-based systems"), Rel("Conjunction"), Method("hand-crafted system")), Triple(Generic("baselines"), Rel("Conjunction"), Method("hand-crafted system"))])
pred: [['trainable sentence planner', 'Compare', 'rule-based systems'], ['trainable sentence planner', 'Compare', 'baselines'], ['rule-based systems', 'Compare', 'baselines'], ['rule-based systems', 'Conjunction', 'hand-crafted system'], ['baselines', 'Conjunction', 'hand-crafted system']]
trues: [['trainable sentence planner', 'Compare', 'rule-based systems'], ['trainable sentence planner', 'Compare', 'baselines'], ['trainable sentence planner', 'Compare', 'hand-crafted system'], ['rule-based systems', 'Conjunction', 'baselines'], ['baselines', 'Conjunction', 'hand-crafted system']]
metrics_sample: [0.6, 0.6, 0.6]
metrics_current: [0.3558413719185423, 0.3600867678958785, 0.35795148247978437]
text: Human action recognition from well-segmented 3D skeleton data has been intensively studied and attracting an increasing attention .
result: extract = Extract([Triple(Material("well-segmented 3D skeleton data"), Rel("Used_for"), Task("Human action recognition"))])
pred: [['well-segmented 3D skeleton data', 'Used_for', 'Human action recognition']]
trues: [['well-segmented 3D skeleton data', 'Used_for', 'Human action recognition']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35653104925053536, 0.3607800650054171, 0.35864297253634897]
text: We argue that it is necessary to draw a line between generalizable semantic principles and domain-specific semantic information .
result: extract = Extract([Triple(Other_scientific_term("generalizable semantic principles"), Rel("Compare"), Other_scientific_term("domain-specific semantic information"))])
pred: [['generalizable semantic principles', 'Compare', 'domain-specific semantic information']]
trues: [['generalizable semantic principles', 'Compare', 'domain-specific semantic information']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3572192513368984, 0.36147186147186144, 0.3593329747175901]
text: We propose to incorporate a priori geometric constraints in a 3 -- D stereo reconstruction scheme to cope with the many cases where image information alone is not sufficient to accurately recover 3 -- D shape .
result: extract = Extract([Triple(Other_scientific_term("a priori geometric constraints"), Rel("Part_of"), Method("3 -- D stereo reconstruction scheme"))])
pred: [['a priori geometric constraints', 'Part_of', '3 -- D stereo reconstruction scheme']]
trues: [['priori geometric constraints', 'Part_of', '3 -- D stereo reconstruction scheme'], ['image information', 'Used_for', '3 -- D shape']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35683760683760685, 0.36069114470842334, 0.35875402792696026]
text: Listen-Communicate-Show ( LCS ) is a new paradigm for human interaction with data sources .
result: extract = Extract([Triple(Generic("data sources"), Rel("Used_for"), Task("human interaction"))])
pred: [['data sources', 'Used_for', 'human interaction']]
trues: [['Listen-Communicate-Show ( LCS )', 'Used_for', 'human interaction with data sources']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35645677694770544, 0.36030204962243795, 0.3583690987124463]
text: These mattes are implied by a set of hypothesized object boundary fragments in the scene .
result: extract = Extract([Triple(Other_scientific_term("object boundary fragments"), Rel("Part_of"), Other_scientific_term("scene"))])
pred: [['object boundary fragments', 'Part_of', 'scene']]
trues: [['hypothesized object boundary fragments', 'Used_for', 'mattes']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35607675906183367, 0.3599137931034483, 0.35798499464094313]
text: At the core of the externally digital architecture is a high-density , low-power analog array performing binary-binary partial matrix-vector multiplication .
result: extract = Extract([Triple(Other_scientific_term("high-density , low-power analog array"), Rel("Part_of"), Method("externally digital architecture")), Triple(Method("binary-binary partial matrix-vector multiplication"), Rel("Used_for"), Other_scientific_term("high-density , low-power analog array"))])
pred: [['high-density , low-power analog array', 'Part_of', 'externally digital architecture'], ['binary-binary partial matrix-vector multiplication', 'Used_for', 'high-density , low-power analog array']]
trues: [['high-density , low-power analog array', 'Part_of', 'externally digital architecture'], ['binary-binary partial matrix-vector multiplication', 'Used_for', 'high-density , low-power analog array']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3574468085106383, 0.36129032258064514, 0.3593582887700535]
text: In this paper , we evaluate the use of a conditional random field ( CRF ) for this task and relate results with this model to our prior work .
result: extract = Extract([Triple(Method("conditional random field ( CRF )"), Rel("Used_for"), Generic("task")), Triple(Method("conditional random field ( CRF )"), Rel("Used_for"), Generic("model")), Triple(Generic("model"), Rel("Compare"), Generic("prior work"))])
pred: [['conditional random field ( CRF )', 'Used_for', 'task'], ['conditional random field ( CRF )', 'Used_for', 'model'], ['model', 'Compare', 'prior work']]
trues: [['conditional random field ( CRF )', 'Used_for', 'task']]
metrics_sample: [0.3333333333333333, 1.0, 0.5]
metrics_current: [0.35737009544008486, 0.3619763694951665, 0.35965848452508004]
text: By employing a joint classification and regression optimization objective , this network is capable of automatically localizing the start and end points of actions more accurately .
result: extract = Extract([Triple(Method("classification and regression optimization objective"), Rel("Used_for"), Generic("network")), Triple(Other_scientific_term("start and end points of actions"), Rel("Feature_of"), Generic("network"))])
pred: [['classification and regression optimization objective', 'Used_for', 'network'], ['start and end points of actions', 'Feature_of', 'network']]
trues: [['joint classification and regression optimization objective', 'Used_for', 'network']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3566137566137566, 0.361587982832618, 0.35908364411294613]
text: In particular , we make extensive use of Pustejovsky 's principles concerning the predicative information associated with nominals .
result: extract = Extract([Triple(Other_scientific_term("Pustejovsky 's principles"), Rel("Used_for"), Other_scientific_term("predicative information")), Triple(Other_scientific_term("predicative information"), Rel("Feature_of"), Other_scientific_term("nominals"))])
pred: [["Pustejovsky 's principles", 'Used_for', 'predicative information'], ['predicative information', 'Feature_of', 'nominals']]
trues: [['nominals', 'Feature_of', 'predicative information']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3558606124604013, 0.3612004287245445, 0.35851063829787233]
text: This paper gives an overall account of a prototype natural language question answering system , called Chat-80 .
result: extract = Extract([Triple(Method("natural language question answering system"), Rel("Hyponym_of"), Method("prototype natural language question answering system")), Triple(Method("Chat-80"), Rel("Hyponym_of"), Method("natural language question answering system"))])
pred: [['natural language question answering system', 'Hyponym_of', 'prototype natural language question answering system'], ['Chat-80', 'Hyponym_of', 'natural language question answering system']]
trues: [['Chat-80', 'Hyponym_of', 'natural language question answering system']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3561643835616438, 0.3618843683083512, 0.3590015932023367]
text: With relevant approach , we identify important contents by PageRank algorithm on the event map constructed from documents .
result: extract = Extract([Triple(Method("PageRank algorithm"), Rel("Used_for"), Other_scientific_term("important contents")), Triple(Other_scientific_term("event map"), Rel("Used_for"), Method("PageRank algorithm")), Triple(Material("documents"), Rel("Used_for"), Other_scientific_term("event map"))])
pred: [['PageRank algorithm', 'Used_for', 'important contents'], ['event map', 'Used_for', 'PageRank algorithm'], ['documents', 'Used_for', 'event map']]
trues: [['PageRank algorithm', 'Used_for', 'relevant approach'], ['event map', 'Used_for', 'PageRank algorithm'], ['documents', 'Used_for', 'event map']]
metrics_sample: [0.6666666666666666, 0.6666666666666666, 0.6666666666666666]
metrics_current: [0.35714285714285715, 0.3628601921024546, 0.35997882477501325]
text: Structural or numerical constraints can then be added locally to the reconstruction process through a constrained optimization scheme .
result: extract = Extract([Triple(Other_scientific_term("Structural or numerical constraints"), Rel("Part_of"), Method("reconstruction process")), Triple(Method("constrained optimization scheme"), Rel("Used_for"), Method("reconstruction process"))])
pred: [['Structural or numerical constraints', 'Part_of', 'reconstruction process'], ['constrained optimization scheme', 'Used_for', 'reconstruction process']]
trues: [['Structural or numerical constraints', 'Used_for', 'reconstruction process'], ['constrained optimization scheme', 'Used_for', 'Structural or numerical constraints']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35639412997903563, 0.362087326943557, 0.3592181722134179]
text: GLOSSER is designed to support reading and learning to read in a foreign language .
result: extract = Extract([Triple(Generic("GLOSSER"), Rel("Used_for"), Task("reading and learning to read")), Triple(Other_scientific_term("foreign language"), Rel("Feature_of"), Task("reading and learning to read"))])
pred: [['GLOSSER', 'Used_for', 'reading and learning to read'], ['foreign language', 'Feature_of', 'reading and learning to read']]
trues: [['GLOSSER', 'Used_for', 'reading and learning']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35564853556485354, 0.3617021276595745, 0.35864978902953587]
text: Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing ( [ 1 ] [ 2 ] ) .
result: extract = Extract([Triple(Task("Recognition of proper nouns"), Rel("Used_for"), Task("morphological analysis")), Triple(Task("morphological analysis"), Rel("Used_for"), Material("Japanese text"))])
pred: [['Recognition of proper nouns', 'Used_for', 'morphological analysis'], ['morphological analysis', 'Used_for', 'Japanese text']]
trues: [['Recognition of proper nouns', 'Part_of', 'morphological analysis'], ['proper nouns', 'Part_of', 'Japanese text'], ['morphological analysis', 'Used_for', 'Japanese text processing']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35490605427974947, 0.3605514316012725, 0.3577064702788006]
text: Although hidden Markov models ( HMMs ) provide a suitable generative model for field structured text , general unsupervised HMM learning fails to learn useful structure in either of our domains .
result: extract = Extract([Triple(Method("hidden Markov models ( HMMs )"), Rel("Used_for"), Method("generative model")), Triple(Method("unsupervised HMM learning"), Rel("Used_for"), Method("generative model"))])
pred: [['hidden Markov models ( HMMs )', 'Used_for', 'generative model'], ['unsupervised HMM learning', 'Used_for', 'generative model']]
trues: [['hidden Markov models ( HMMs )', 'Used_for', 'generative model'], ['generative model', 'Used_for', 'field structured text']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.35520833333333335, 0.36084656084656086, 0.35800524934383204]
text: Joint matrix triangularization is often used for estimating the joint eigenstructure of a set M of matrices , with applications in signal processing and machine learning .
result: extract = Extract([Triple(Method("Joint matrix triangularization"), Rel("Used_for"), Other_scientific_term("joint eigenstructure")), Triple(Other_scientific_term("joint eigenstructure"), Rel("Used_for"), Material("M of matrices")), Triple(Material("M of matrices"), Rel("Used_for"), Task("signal processing")), Triple(Material("M of matrices"), Rel("Used_for"), Task("machine learning")), Triple(Task("signal processing"), Rel("Conjunction"), Task("machine learning"))])
pred: [['Joint matrix triangularization', 'Used_for', 'joint eigenstructure'], ['joint eigenstructure', 'Used_for', 'M of matrices'], ['M of matrices', 'Used_for', 'signal processing'], ['M of matrices', 'Used_for', 'machine learning'], ['signal processing', 'Conjunction', 'machine learning']]
trues: [['Joint matrix triangularization', 'Used_for', 'joint eigenstructure'], ['joint eigenstructure', 'Used_for', 'signal processing'], ['joint eigenstructure', 'Used_for', 'machine learning'], ['signal processing', 'Conjunction', 'machine learning']]
metrics_sample: [0.4, 0.5, 0.4444444444444445]
metrics_current: [0.35544041450777203, 0.36143308746048475, 0.35841170323928945]
text: The robust principal component analysis ( robust PCA ) problem has been considered in many machine learning applications , where the goal is to decompose the data matrix to a low rank part plus a sparse residual .
result: extract = Extract([Triple(Task("robust principal component analysis ( robust PCA ) problem"), Rel("Used_for"), Task("machine learning applications")), Triple(Other_scientific_term("low rank part"), Rel("Conjunction"), Other_scientific_term("sparse residual"))])
pred: [['robust principal component analysis ( robust PCA ) problem', 'Used_for', 'machine learning applications'], ['low rank part', 'Conjunction', 'sparse residual']]
trues: [['robust principal component analysis ( robust PCA ) problem', 'Used_for', 'machine learning applications'], ['low rank part', 'Part_of', 'data matrix'], ['low rank part', 'Conjunction', 'sparse residual'], ['sparse residual', 'Part_of', 'data matrix']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.35677352637021714, 0.3620146904512067, 0.35937499999999994]
text: We describe how to estimate these indicators and how to integrate them into a retrieval approach based on language models .
result: extract = Extract([Triple(Generic("indicators"), Rel("Part_of"), Task("retrieval approach")), Triple(Method("language models"), Rel("Used_for"), Task("retrieval approach"))])
pred: [['indicators', 'Part_of', 'retrieval approach'], ['language models', 'Used_for', 'retrieval approach']]
trues: [['them', 'Part_of', 'retrieval approach'], ['language models', 'Used_for', 'them']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3560371517027864, 0.3612565445026178, 0.3586278586278587]
text: In this paper , we propose a new method for the modeling and subtraction of such scenes .
result: extract = Extract([Triple(Generic("method"), Rel("Used_for"), Task("modeling and subtraction of such scenes"))])
pred: [['method', 'Used_for', 'modeling and subtraction of such scenes']]
trues: [['method', 'Used_for', 'modeling and subtraction of such scenes']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35670103092783506, 0.3619246861924686, 0.3592938733125649]
text: A central goal of this research is to merge approaches from pivot MT , interactive MT , and multilingual text authoring .
result: extract = Extract([Triple(Method("pivot MT"), Rel("Conjunction"), Method("interactive MT")), Triple(Method("interactive MT"), Rel("Conjunction"), Method("multilingual text authoring"))])
pred: [['pivot MT', 'Conjunction', 'interactive MT'], ['interactive MT', 'Conjunction', 'multilingual text authoring']]
trues: [['pivot MT', 'Conjunction', 'interactive MT'], ['interactive MT', 'Conjunction', 'multilingual text authoring']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35802469135802467, 0.36325678496868474, 0.36062176165803106]
text: We also address a more general problem of matrix rank degeneration & non-isolated minima in the low-rank matrix optimization by using new type of regularizer which approximately enforces the or-thonormality of the learned matrix very efficiently .
result: extract = Extract([Triple(Other_scientific_term("matrix rank degeneration"), Rel("Conjunction"), Other_scientific_term("non-isolated minima")), Triple(Other_scientific_term("non-isolated minima"), Rel("Part_of"), Task("low-rank matrix optimization")), Triple(Method("regularizer"), Rel("Used_for"), Other_scientific_term("or-thonormality of the learned matrix"))])
pred: [['matrix rank degeneration', 'Conjunction', 'non-isolated minima'], ['non-isolated minima', 'Part_of', 'low-rank matrix optimization'], ['regularizer', 'Used_for', 'or-thonormality of the learned matrix']]
trues: [['matrix rank degeneration', 'Conjunction', 'non-isolated minima'], ['matrix rank degeneration', 'Feature_of', 'low-rank matrix optimization'], ['non-isolated minima', 'Feature_of', 'low-rank matrix optimization'], ['regularizer', 'Used_for', 'low-rank matrix optimization'], ['regularizer', 'Used_for', 'or-thonormality'], ['or-thonormality', 'Feature_of', 'learned matrix']]
metrics_sample: [0.3333333333333333, 0.16666666666666666, 0.2222222222222222]
metrics_current: [0.35794871794871796, 0.3620331950207469, 0.35997937080969566]
text: Unlike existing interest point detectors , which measure pixel-wise differences in image intensity , our detectors incorporate histogram-based representations , and thus can find image regions that present a distinct distribution in the neighborhood .
result: extract = Extract([Triple(Other_scientific_term("histogram-based representations"), Rel("Used_for"), Method("interest point detectors"))])
pred: [['histogram-based representations', 'Used_for', 'interest point detectors']]
trues: [['pixel-wise differences in image intensity', 'Evaluate_for', 'interest point detectors'], ['histogram-based representations', 'Part_of', 'detectors']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35758196721311475, 0.3612836438923395, 0.3594232749742533]
text: However , when the object becomes partially or totally occluded , such local tracking is prone to failure , especially when common prediction techniques like the Kalman filter do not provide a good estimate of object parameters in future frames .
result: extract = Extract([Triple(Method("Kalman filter"), Rel("Used_for"), Other_scientific_term("object parameters"))])
pred: [['Kalman filter', 'Used_for', 'object parameters']]
trues: [['Kalman filter', 'Part_of', 'prediction techniques']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3572159672466735, 0.3609100310237849, 0.3590534979423868]
text: We present Minimum Bayes-Risk ( MBR ) decoding for statistical machine translation .
result: extract = Extract([Triple(Method("Minimum Bayes-Risk ( MBR ) decoding"), Rel("Used_for"), Task("statistical machine translation"))])
pred: [['Minimum Bayes-Risk ( MBR ) decoding', 'Used_for', 'statistical machine translation']]
trues: [['Minimum Bayes-Risk ( MBR ) decoding', 'Used_for', 'statistical machine translation']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35787321063394684, 0.3615702479338843, 0.3597122302158274]
text: Based on these results , we present an ECA that uses verbal and nonverbal grounding acts to update dialogue state .
result: extract = Extract([Triple(Method("ECA"), Rel("Used_for"), Other_scientific_term("dialogue state")), Triple(Other_scientific_term("verbal and nonverbal grounding acts"), Rel("Used_for"), Method("ECA"))])
pred: [['ECA', 'Used_for', 'dialogue state'], ['verbal and nonverbal grounding acts', 'Used_for', 'ECA']]
trues: [['verbal and nonverbal grounding acts', 'Used_for', 'ECA'], ['verbal and nonverbal grounding acts', 'Used_for', 'dialogue state']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.35816326530612247, 0.3618556701030928, 0.36]
text: We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars .
result: extract = Extract([Triple(Method("Tree Adjoining Grammars"), Rel("Conjunction"), Method("Head Grammars"))])
pred: [['Tree Adjoining Grammars', 'Conjunction', 'Head Grammars']]
trues: [['Tree Adjoining Grammars', 'Hyponym_of', 'grammatical formalisms'], ['Tree Adjoining Grammars', 'Compare', 'Head Grammars'], ['Head Grammars', 'Hyponym_of', 'grammatical formalisms']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3577981651376147, 0.3607399794450154, 0.3592630501535312]
text: In this paper , we study the problem of online action detection from the streaming skeleton data .
result: extract = Extract([Triple(Material("streaming skeleton data"), Rel("Used_for"), Task("online action detection"))])
pred: [['streaming skeleton data', 'Used_for', 'online action detection']]
trues: [['streaming skeleton data', 'Used_for', 'online action detection']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35845213849287166, 0.3613963039014374, 0.35991820040899797]