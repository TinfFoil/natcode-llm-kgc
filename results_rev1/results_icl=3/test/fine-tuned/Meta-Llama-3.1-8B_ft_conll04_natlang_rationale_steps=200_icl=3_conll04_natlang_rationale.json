[{"Model": "./models/Meta-Llama-3.1-8B_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.49888641425389757, "Recall": 0.5503685503685504, "F1_Score": 0.5233644859813084, "rel_type_metrics": {"Kill": {"precision": 0.8, "recall": 0.851063829787234, "f1": 0.8247422680412372}, "Live_in": {"precision": 0.4489795918367347, "recall": 0.4489795918367347, "f1": 0.4489795918367347}, "Organization_based_in": {"precision": 0.5227272727272727, "recall": 0.4791666666666667, "f1": 0.5}, "Located_in": {"precision": 0.5949367088607594, "recall": 0.5222222222222223, "f1": 0.5562130177514792}, "Work_for": {"precision": 0.3560606060606061, "recall": 0.618421052631579, "f1": 0.4519230769230769}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-26-20-56-23", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/Meta-Llama-3.1-8B_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.5010706638115632, "Recall": 0.5749385749385749, "F1_Score": 0.5354691075514875, "rel_type_metrics": {"Live_in": {"precision": 0.3939393939393939, "recall": 0.3979591836734694, "f1": 0.3959390862944162}, "Work_for": {"precision": 0.3700787401574803, "recall": 0.618421052631579, "f1": 0.46305418719211827}, "Organization_based_in": {"precision": 0.5520833333333334, "recall": 0.5520833333333334, "f1": 0.5520833333333334}, "Located_in": {"precision": 0.574468085106383, "recall": 0.6, "f1": 0.5869565217391305}, "Kill": {"precision": 0.803921568627451, "recall": 0.8723404255319149, "f1": 0.8367346938775511}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-14-27-19", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/Meta-Llama-3.1-8B_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.4967462039045553, "Recall": 0.5626535626535627, "F1_Score": 0.5276497695852534, "rel_type_metrics": {"Work_for": {"precision": 0.3194444444444444, "recall": 0.6052631578947368, "f1": 0.41818181818181815}, "Organization_based_in": {"precision": 0.6707317073170732, "recall": 0.5729166666666666, "f1": 0.6179775280898876}, "Kill": {"precision": 0.8163265306122449, "recall": 0.851063829787234, "f1": 0.8333333333333334}, "Located_in": {"precision": 0.5104166666666666, "recall": 0.5444444444444444, "f1": 0.5268817204301075}, "Live_in": {"precision": 0.43333333333333335, "recall": 0.3979591836734694, "f1": 0.41489361702127664}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-01-08-59", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}]