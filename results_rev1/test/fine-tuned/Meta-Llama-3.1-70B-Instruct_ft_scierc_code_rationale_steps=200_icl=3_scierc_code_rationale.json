[{"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_scierc_code_rationale_steps=200_icl=3", "Precision": 0.10236220472440945, "Recall": 0.10677618069815195, "F1_Score": 0.10452261306532663, "rel_type_metrics": {"Used_for": {"precision": 0.13468013468013468, "recall": 0.150093808630394, "f1": 0.1419698314108252}, "Compare": {"precision": 0.17647058823529413, "recall": 0.15789473684210525, "f1": 0.16666666666666669}, "Feature_of": {"precision": 0.008695652173913044, "recall": 0.01694915254237288, "f1": 0.011494252873563218}, "Evaluate_for": {"precision": 0, "recall": 0.0, "f1": 0}, "Part_of": {"precision": 0.06862745098039216, "recall": 0.2222222222222222, "f1": 0.10486891385767792}, "Conjunction": {"precision": 0, "recall": 0.0, "f1": 0}, "Hyponym_of": {"precision": 0.06976744186046512, "recall": 0.04477611940298507, "f1": 0.05454545454545454}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2025-01-11-13-03-46", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_scierc_code_rationale_steps=200_icl=3", "Precision": 0.10620689655172413, "Recall": 0.15811088295687886, "F1_Score": 0.12706270627062705, "rel_type_metrics": {"Compare": {"precision": 0, "recall": 0.0, "f1": 0}, "Evaluate_for": {"precision": 0, "recall": 0.0, "f1": 0}, "Hyponym_of": {"precision": 0.08064516129032258, "recall": 0.14925373134328357, "f1": 0.10471204188481675}, "Part_of": {"precision": 0.03767123287671233, "recall": 0.1746031746031746, "f1": 0.061971830985915494}, "Conjunction": {"precision": 0.2191780821917808, "recall": 0.13008130081300814, "f1": 0.16326530612244897}, "Feature_of": {"precision": 0.04854368932038835, "recall": 0.0847457627118644, "f1": 0.06172839506172839}, "Used_for": {"precision": 0.1306884480746791, "recall": 0.2101313320825516, "f1": 0.16115107913669066}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2025-01-11-13-43-22", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_scierc_code_rationale_steps=200_icl=3", "Precision": 0.10763636363636364, "Recall": 0.15195071868583163, "F1_Score": 0.12601106853980418, "rel_type_metrics": {"Compare": {"precision": 0.27906976744186046, "recall": 0.3157894736842105, "f1": 0.29629629629629634}, "Part_of": {"precision": 0.03880597014925373, "recall": 0.20634920634920634, "f1": 0.06532663316582914}, "Feature_of": {"precision": 0.05194805194805195, "recall": 0.06779661016949153, "f1": 0.058823529411764705}, "Hyponym_of": {"precision": 0.08, "recall": 0.14925373134328357, "f1": 0.10416666666666666}, "Evaluate_for": {"precision": 0.0641025641025641, "recall": 0.054945054945054944, "f1": 0.05917159763313609}, "Used_for": {"precision": 0.14630467571644043, "recall": 0.18198874296435272, "f1": 0.16220735785953178}, "Conjunction": {"precision": 0.30434782608695654, "recall": 0.056910569105691054, "f1": 0.09589041095890412}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2025-01-11-14-18-32", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}]