[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_scierc_natlang_rationale_steps=200_icl=3", "Precision": 0.36213468869123255, "Recall": 0.2926078028747433, "F1_Score": 0.323679727427598, "rel_type_metrics": {"Used_for": {"precision": 0.34297520661157027, "recall": 0.31144465290806755, "f1": 0.3264503441494592}, "Compare": {"precision": 0.42424242424242425, "recall": 0.3684210526315789, "f1": 0.3943661971830986}, "Evaluate_for": {"precision": 0.27472527472527475, "recall": 0.27472527472527475, "f1": 0.27472527472527475}, "Conjunction": {"precision": 0.4942528735632184, "recall": 0.34959349593495936, "f1": 0.40952380952380957}, "Hyponym_of": {"precision": 0.5178571428571429, "recall": 0.43283582089552236, "f1": 0.47154471544715443}, "Part_of": {"precision": 0.42857142857142855, "recall": 0.09523809523809523, "f1": 0.15584415584415587}, "Feature_of": {"precision": 0.2, "recall": 0.03389830508474576, "f1": 0.05797101449275363}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2024-12-26-20-29-04", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_scierc_natlang_rationale_steps=200_icl=3", "Precision": 0.3714689265536723, "Recall": 0.27002053388090347, "F1_Score": 0.31272294887039237, "rel_type_metrics": {"Feature_of": {"precision": 0.2, "recall": 0.03389830508474576, "f1": 0.05797101449275363}, "Used_for": {"precision": 0.3834080717488789, "recall": 0.32082551594746717, "f1": 0.34933605720122574}, "Hyponym_of": {"precision": 0.4523809523809524, "recall": 0.2835820895522388, "f1": 0.3486238532110092}, "Conjunction": {"precision": 0.43243243243243246, "recall": 0.2601626016260163, "f1": 0.32487309644670054}, "Evaluate_for": {"precision": 0.29333333333333333, "recall": 0.24175824175824176, "f1": 0.26506024096385544}, "Part_of": {"precision": 0.19230769230769232, "recall": 0.07936507936507936, "f1": 0.11235955056179774}, "Compare": {"precision": 0.35294117647058826, "recall": 0.3157894736842105, "f1": 0.33333333333333337}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2024-12-28-13-56-04", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_scierc_natlang_rationale_steps=200_icl=3", "Precision": 0.35071090047393366, "Recall": 0.22792607802874743, "F1_Score": 0.2762912258867455, "rel_type_metrics": {"Used_for": {"precision": 0.35148514851485146, "recall": 0.26641651031894936, "f1": 0.30309498399146206}, "Part_of": {"precision": 0.36363636363636365, "recall": 0.06349206349206349, "f1": 0.1081081081081081}, "Compare": {"precision": 0.38461538461538464, "recall": 0.2631578947368421, "f1": 0.3125}, "Evaluate_for": {"precision": 0.19672131147540983, "recall": 0.13186813186813187, "f1": 0.15789473684210528}, "Conjunction": {"precision": 0.4146341463414634, "recall": 0.2764227642276423, "f1": 0.33170731707317075}, "Feature_of": {"precision": 0.16666666666666666, "recall": 0.03389830508474576, "f1": 0.056338028169014086}, "Hyponym_of": {"precision": 0.4864864864864865, "recall": 0.26865671641791045, "f1": 0.3461538461538462}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2024-12-29-00-40-06", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}]