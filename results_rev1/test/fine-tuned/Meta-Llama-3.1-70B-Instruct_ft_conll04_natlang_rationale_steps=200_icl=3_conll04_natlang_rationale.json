[{"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.13432835820895522, "Recall": 0.11056511056511056, "F1_Score": 0.12129380053908356, "rel_type_metrics": {"Organization_based_in": {"precision": 0, "recall": 0.0, "f1": 0}, "Work_for": {"precision": 0.5, "recall": 0.02631578947368421, "f1": 0.05}, "Live_in": {"precision": 0.17098445595854922, "recall": 0.336734693877551, "f1": 0.2268041237113402}, "Kill": {"precision": 0.4, "recall": 0.0425531914893617, "f1": 0.07692307692307691}, "Located_in": {"precision": 0.2, "recall": 0.08888888888888889, "f1": 0.12307692307692307}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-08-21-10-30", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.22596964586846544, "Recall": 0.32923832923832924, "F1_Score": 0.268, "rel_type_metrics": {"Live_in": {"precision": 0.26126126126126126, "recall": 0.29591836734693877, "f1": 0.27751196172248804}, "Located_in": {"precision": 0.19444444444444445, "recall": 0.6222222222222222, "f1": 0.2962962962962963}, "Work_for": {"precision": 0.5714285714285714, "recall": 0.10526315789473684, "f1": 0.17777777777777778}, "Organization_based_in": {"precision": 0.40217391304347827, "recall": 0.3854166666666667, "f1": 0.39361702127659576}, "Kill": {"precision": 1.0, "recall": 0.0851063829787234, "f1": 0.1568627450980392}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-08-22-57-18", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-70B-Instruct_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.1259259259259259, "Recall": 0.16707616707616707, "F1_Score": 0.14361140443505804, "rel_type_metrics": {"Organization_based_in": {"precision": 0.2926829268292683, "recall": 0.125, "f1": 0.17518248175182483}, "Located_in": {"precision": 0.25510204081632654, "recall": 0.2777777777777778, "f1": 0.2659574468085106}, "Kill": {"precision": 0.8421052631578947, "recall": 0.3404255319148936, "f1": 0.48484848484848486}, "Live_in": {"precision": 0.0, "recall": 0.0, "f1": 0}, "Work_for": {"precision": 0.375, "recall": 0.19736842105263158, "f1": 0.2586206896551724}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-08-23-33-19", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}]