[{"Model": "./models/Meta-Llama-3.1-8B_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.525, "Recall": 0.5675675675675675, "F1_Score": 0.5454545454545455, "rel_type_metrics": {"Organization_based_in": {"precision": 0.7638888888888888, "recall": 0.5729166666666666, "f1": 0.6547619047619048}, "Kill": {"precision": 0.8723404255319149, "recall": 0.8723404255319149, "f1": 0.8723404255319149}, "Located_in": {"precision": 0.5319148936170213, "recall": 0.5555555555555556, "f1": 0.5434782608695652}, "Work_for": {"precision": 0.3125, "recall": 0.5921052631578947, "f1": 0.40909090909090906}, "Live_in": {"precision": 0.4819277108433735, "recall": 0.40816326530612246, "f1": 0.44198895027624313}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-27-02-26-26", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Meta-Llama-3.1-8B_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.4343629343629344, "Recall": 0.5528255528255528, "F1_Score": 0.4864864864864865, "rel_type_metrics": {"Located_in": {"precision": 0.4842105263157895, "recall": 0.5111111111111111, "f1": 0.4972972972972973}, "Kill": {"precision": 0.8163265306122449, "recall": 0.851063829787234, "f1": 0.8333333333333334}, "Organization_based_in": {"precision": 0.28191489361702127, "recall": 0.5520833333333334, "f1": 0.3732394366197183}, "Live_in": {"precision": 0.43478260869565216, "recall": 0.40816326530612246, "f1": 0.4210526315789474}, "Work_for": {"precision": 0.48936170212765956, "recall": 0.6052631578947368, "f1": 0.5411764705882353}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-21-33-07", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Meta-Llama-3.1-8B_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.5119305856832972, "Recall": 0.5798525798525799, "F1_Score": 0.5437788018433181, "rel_type_metrics": {"Live_in": {"precision": 0.44680851063829785, "recall": 0.42857142857142855, "f1": 0.4375}, "Located_in": {"precision": 0.6153846153846154, "recall": 0.6222222222222222, "f1": 0.6187845303867403}, "Work_for": {"precision": 0.6301369863013698, "recall": 0.6052631578947368, "f1": 0.6174496644295302}, "Kill": {"precision": 0.34782608695652173, "recall": 0.851063829787234, "f1": 0.49382716049382713}, "Organization_based_in": {"precision": 0.5909090909090909, "recall": 0.5416666666666666, "f1": 0.5652173913043478}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-11-49-17", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}]