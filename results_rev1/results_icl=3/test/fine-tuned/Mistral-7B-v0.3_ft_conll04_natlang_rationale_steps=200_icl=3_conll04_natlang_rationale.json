[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.6264501160092807, "Recall": 0.6633906633906634, "F1_Score": 0.6443914081145585, "rel_type_metrics": {"Organization_based_in": {"precision": 0.47058823529411764, "recall": 0.5833333333333334, "f1": 0.5209302325581395}, "Live_in": {"precision": 0.6476190476190476, "recall": 0.6938775510204082, "f1": 0.6699507389162561}, "Kill": {"precision": 0.8627450980392157, "recall": 0.9361702127659575, "f1": 0.8979591836734694}, "Work_for": {"precision": 0.6875, "recall": 0.5789473684210527, "f1": 0.6285714285714286}, "Located_in": {"precision": 0.6304347826086957, "recall": 0.6444444444444445, "f1": 0.6373626373626373}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-26-20-36-22", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.5501066098081023, "Recall": 0.6339066339066339, "F1_Score": 0.589041095890411, "rel_type_metrics": {"Kill": {"precision": 0.8936170212765957, "recall": 0.8936170212765957, "f1": 0.8936170212765957}, "Organization_based_in": {"precision": 0.6575342465753424, "recall": 0.5, "f1": 0.5680473372781065}, "Live_in": {"precision": 0.4657534246575342, "recall": 0.6938775510204082, "f1": 0.5573770491803278}, "Located_in": {"precision": 0.40601503759398494, "recall": 0.6, "f1": 0.48430493273542596}, "Work_for": {"precision": 0.6571428571428571, "recall": 0.6052631578947368, "f1": 0.6301369863013698}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-14-03-25", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.5657894736842105, "Recall": 0.6339066339066339, "F1_Score": 0.5979142526071842, "rel_type_metrics": {"Kill": {"precision": 0.8936170212765957, "recall": 0.8936170212765957, "f1": 0.8936170212765957}, "Organization_based_in": {"precision": 0.6533333333333333, "recall": 0.5104166666666666, "f1": 0.5730994152046783}, "Located_in": {"precision": 0.3430232558139535, "recall": 0.6555555555555556, "f1": 0.45038167938931295}, "Work_for": {"precision": 0.6923076923076923, "recall": 0.5921052631578947, "f1": 0.6382978723404255}, "Live_in": {"precision": 0.6494845360824743, "recall": 0.6428571428571429, "f1": 0.6461538461538462}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-00-46-22", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}]