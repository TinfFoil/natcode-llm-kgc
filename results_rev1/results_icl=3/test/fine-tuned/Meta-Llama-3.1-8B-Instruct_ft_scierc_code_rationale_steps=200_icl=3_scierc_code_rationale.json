[{"Model": "./models/Meta-Llama-3.1-8B-Instruct_ft_scierc_code_rationale_steps=200_icl=3", "Precision": 0.30752688172043013, "Recall": 0.14681724845995894, "F1_Score": 0.19874913134120917, "rel_type_metrics": {"Conjunction": {"precision": 0.5625, "recall": 0.07317073170731707, "f1": 0.12949640287769784}, "Feature_of": {"precision": 0.07407407407407407, "recall": 0.03389830508474576, "f1": 0.046511627906976744}, "Used_for": {"precision": 0.3277591973244147, "recall": 0.18386491557223264, "f1": 0.23557692307692307}, "Part_of": {"precision": 0.27586206896551724, "recall": 0.12698412698412698, "f1": 0.17391304347826084}, "Compare": {"precision": 0.4444444444444444, "recall": 0.21052631578947367, "f1": 0.2857142857142857}, "Evaluate_for": {"precision": 0.2545454545454545, "recall": 0.15384615384615385, "f1": 0.1917808219178082}, "Hyponym_of": {"precision": 0.19047619047619047, "recall": 0.05970149253731343, "f1": 0.09090909090909091}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2024-12-26-23-48-29", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-8B-Instruct_ft_scierc_code_rationale_steps=200_icl=3", "Precision": 0.3069852941176471, "Recall": 0.17145790554414783, "F1_Score": 0.22002635046113306, "rel_type_metrics": {"Hyponym_of": {"precision": 0.38461538461538464, "recall": 0.22388059701492538, "f1": 0.2830188679245283}, "Evaluate_for": {"precision": 0.3469387755102041, "recall": 0.18681318681318682, "f1": 0.24285714285714288}, "Feature_of": {"precision": 0.125, "recall": 0.05084745762711865, "f1": 0.07228915662650602}, "Compare": {"precision": 0.22727272727272727, "recall": 0.13157894736842105, "f1": 0.16666666666666666}, "Used_for": {"precision": 0.29737609329446063, "recall": 0.19136960600375236, "f1": 0.23287671232876714}, "Conjunction": {"precision": 0.3958333333333333, "recall": 0.15447154471544716, "f1": 0.22222222222222224}, "Part_of": {"precision": 0.3157894736842105, "recall": 0.09523809523809523, "f1": 0.14634146341463414}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2024-12-28-18-15-36", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-8B-Instruct_ft_scierc_code_rationale_steps=200_icl=3", "Precision": 0.27816091954022987, "Recall": 0.12422997946611909, "F1_Score": 0.1717530163236338, "rel_type_metrics": {"Hyponym_of": {"precision": 0.35714285714285715, "recall": 0.14925373134328357, "f1": 0.21052631578947364}, "Conjunction": {"precision": 0.4375, "recall": 0.11382113821138211, "f1": 0.1806451612903226}, "Evaluate_for": {"precision": 0.21568627450980393, "recall": 0.12087912087912088, "f1": 0.15492957746478872}, "Feature_of": {"precision": 0.043478260869565216, "recall": 0.03389830508474576, "f1": 0.0380952380952381}, "Part_of": {"precision": 0.25, "recall": 0.06349206349206349, "f1": 0.10126582278481013}, "Used_for": {"precision": 0.30416666666666664, "recall": 0.13696060037523453, "f1": 0.1888745148771022}, "Compare": {"precision": 0.3684210526315789, "recall": 0.18421052631578946, "f1": 0.24561403508771928}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2024-12-29-07-00-40", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}]