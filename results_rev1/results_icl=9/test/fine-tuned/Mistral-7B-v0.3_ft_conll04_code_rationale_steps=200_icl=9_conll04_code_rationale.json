[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_rationale_steps=200_icl=9", "Precision": 0.6759493670886076, "Recall": 0.6560196560196561, "F1_Score": 0.6658354114713217, "rel_type_metrics": {"Organization_based_in": {"precision": 0.6666666666666666, "recall": 0.5625, "f1": 0.6101694915254238}, "Work_for": {"precision": 0.5915492957746479, "recall": 0.5526315789473685, "f1": 0.5714285714285715}, "Kill": {"precision": 0.8235294117647058, "recall": 0.8936170212765957, "f1": 0.8571428571428571}, "Live_in": {"precision": 0.7142857142857143, "recall": 0.7142857142857143, "f1": 0.7142857142857143}, "Located_in": {"precision": 0.6344086021505376, "recall": 0.6555555555555556, "f1": 0.644808743169399}}, "n_icl_samples": 9, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-20-49-00", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_rationale_steps=200_icl=9", "Precision": 0.6971279373368147, "Recall": 0.6560196560196561, "F1_Score": 0.6759493670886076, "rel_type_metrics": {"Work_for": {"precision": 0.7241379310344828, "recall": 0.5526315789473685, "f1": 0.6268656716417911}, "Organization_based_in": {"precision": 0.6790123456790124, "recall": 0.5729166666666666, "f1": 0.6214689265536723}, "Kill": {"precision": 0.84, "recall": 0.8936170212765957, "f1": 0.8659793814432989}, "Located_in": {"precision": 0.6451612903225806, "recall": 0.6666666666666666, "f1": 0.6557377049180327}, "Live_in": {"precision": 0.68, "recall": 0.6938775510204082, "f1": 0.686868686868687}}, "n_icl_samples": 9, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-20-55-52", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_rationale_steps=200_icl=9", "Precision": 0.6871794871794872, "Recall": 0.6584766584766585, "F1_Score": 0.6725219573400251, "rel_type_metrics": {"Kill": {"precision": 0.84, "recall": 0.8936170212765957, "f1": 0.8659793814432989}, "Located_in": {"precision": 0.6813186813186813, "recall": 0.6888888888888889, "f1": 0.6850828729281768}, "Organization_based_in": {"precision": 0.6582278481012658, "recall": 0.5416666666666666, "f1": 0.5942857142857143}, "Work_for": {"precision": 0.5657894736842105, "recall": 0.5657894736842105, "f1": 0.5657894736842105}, "Live_in": {"precision": 0.7419354838709677, "recall": 0.7040816326530612, "f1": 0.7225130890052356}}, "n_icl_samples": 9, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-21-02-28", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}]