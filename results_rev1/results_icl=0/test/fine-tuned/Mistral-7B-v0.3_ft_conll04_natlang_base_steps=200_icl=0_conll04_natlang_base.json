[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=0", "Precision": 0.7135135135135136, "Recall": 0.6486486486486487, "F1_Score": 0.6795366795366796, "rel_type_metrics": {"Located_in": {"precision": 0.6883116883116883, "recall": 0.5888888888888889, "f1": 0.6347305389221557}, "Work_for": {"precision": 0.7142857142857143, "recall": 0.6578947368421053, "f1": 0.684931506849315}, "Organization_based_in": {"precision": 0.6794871794871795, "recall": 0.5520833333333334, "f1": 0.6091954022988507}, "Live_in": {"precision": 0.6979166666666666, "recall": 0.6836734693877551, "f1": 0.6907216494845362}, "Kill": {"precision": 0.8367346938775511, "recall": 0.8723404255319149, "f1": 0.8541666666666667}}, "n_icl_samples": 0, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-01-56-35", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=0", "Precision": 0.7119565217391305, "Recall": 0.6437346437346437, "F1_Score": 0.6761290322580645, "rel_type_metrics": {"Organization_based_in": {"precision": 0.6794871794871795, "recall": 0.5520833333333334, "f1": 0.6091954022988507}, "Kill": {"precision": 0.84, "recall": 0.8936170212765957, "f1": 0.8659793814432989}, "Work_for": {"precision": 0.7164179104477612, "recall": 0.631578947368421, "f1": 0.6713286713286712}, "Live_in": {"precision": 0.6979166666666666, "recall": 0.6836734693877551, "f1": 0.6907216494845362}, "Located_in": {"precision": 0.6753246753246753, "recall": 0.5777777777777777, "f1": 0.622754491017964}}, "n_icl_samples": 0, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-01-58-33", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=0", "Precision": 0.7138964577656676, "Recall": 0.6437346437346437, "F1_Score": 0.6770025839793281, "rel_type_metrics": {"Kill": {"precision": 0.8367346938775511, "recall": 0.8723404255319149, "f1": 0.8541666666666667}, "Organization_based_in": {"precision": 0.6923076923076923, "recall": 0.5625, "f1": 0.6206896551724138}, "Work_for": {"precision": 0.7058823529411765, "recall": 0.631578947368421, "f1": 0.6666666666666667}, "Live_in": {"precision": 0.6979166666666666, "recall": 0.6836734693877551, "f1": 0.6907216494845362}, "Located_in": {"precision": 0.6842105263157895, "recall": 0.5777777777777777, "f1": 0.6265060240963854}}, "n_icl_samples": 0, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-02-00-27", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}]