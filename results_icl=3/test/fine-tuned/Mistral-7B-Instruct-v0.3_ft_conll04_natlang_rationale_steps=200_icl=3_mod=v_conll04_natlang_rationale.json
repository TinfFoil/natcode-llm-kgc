[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3_mod=v", "Precision": 0.6451612903225806, "Recall": 0.4914004914004914, "F1_Score": 0.5578800557880056, "rel_type_metrics": {"Work_for": {"precision": 0.6615384615384615, "recall": 0.5657894736842105, "f1": 0.6099290780141844}, "Live_in": {"precision": 0.6590909090909091, "recall": 0.5918367346938775, "f1": 0.6236559139784946}, "Organization_based_in": {"precision": 0.6078431372549019, "recall": 0.3229166666666667, "f1": 0.4217687074829932}, "Kill": {"precision": 0.8292682926829268, "recall": 0.723404255319149, "f1": 0.7727272727272727}, "Located_in": {"precision": 0.576271186440678, "recall": 0.37777777777777777, "f1": 0.4563758389261745}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-15-52", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3_mod=v", "Precision": 0.6, "Recall": 0.47174447174447176, "F1_Score": 0.5281980742778543, "rel_type_metrics": {"Live_in": {"precision": 0.6708860759493671, "recall": 0.5408163265306123, "f1": 0.5988700564971752}, "Located_in": {"precision": 0.5396825396825397, "recall": 0.37777777777777777, "f1": 0.4444444444444445}, "Work_for": {"precision": 0.5757575757575758, "recall": 0.5, "f1": 0.5352112676056339}, "Organization_based_in": {"precision": 0.5428571428571428, "recall": 0.3958333333333333, "f1": 0.4578313253012048}, "Kill": {"precision": 0.8285714285714286, "recall": 0.6170212765957447, "f1": 0.7073170731707318}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-20-43", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3_mod=v", "Precision": 0.5958083832335329, "Recall": 0.48894348894348894, "F1_Score": 0.5371120107962212, "rel_type_metrics": {"Organization_based_in": {"precision": 0.4772727272727273, "recall": 0.4375, "f1": 0.45652173913043476}, "Located_in": {"precision": 0.5789473684210527, "recall": 0.36666666666666664, "f1": 0.44897959183673464}, "Live_in": {"precision": 0.6547619047619048, "recall": 0.5612244897959183, "f1": 0.6043956043956044}, "Kill": {"precision": 0.7222222222222222, "recall": 0.5531914893617021, "f1": 0.6265060240963856}, "Work_for": {"precision": 0.6515151515151515, "recall": 0.5657894736842105, "f1": 0.6056338028169014}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-25-24", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}]