[{"Model": "./models/CodeQwen1.5-7B_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.5144628099173554, "Recall": 0.6117936117936118, "F1_Score": 0.5589225589225589, "rel_type_metrics": {"Work_for": {"precision": 0.6, "recall": 0.6710526315789473, "f1": 0.6335403726708074}, "Kill": {"precision": 0.8163265306122449, "recall": 0.851063829787234, "f1": 0.8333333333333334}, "Located_in": {"precision": 0.3082191780821918, "recall": 0.5, "f1": 0.3813559322033898}, "Live_in": {"precision": 0.6593406593406593, "recall": 0.6122448979591837, "f1": 0.6349206349206349}, "Organization_based_in": {"precision": 0.4690265486725664, "recall": 0.5520833333333334, "f1": 0.507177033492823}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-21-55-35", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/CodeQwen1.5-7B_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.508130081300813, "Recall": 0.6142506142506142, "F1_Score": 0.5561735261401558, "rel_type_metrics": {"Live_in": {"precision": 0.6344086021505376, "recall": 0.6020408163265306, "f1": 0.6178010471204188}, "Organization_based_in": {"precision": 0.31137724550898205, "recall": 0.5416666666666666, "f1": 0.39543726235741444}, "Work_for": {"precision": 0.5882352941176471, "recall": 0.6578947368421053, "f1": 0.6211180124223602}, "Kill": {"precision": 0.82, "recall": 0.8723404255319149, "f1": 0.8453608247422681}, "Located_in": {"precision": 0.4948453608247423, "recall": 0.5333333333333333, "f1": 0.5133689839572193}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-12-17-52", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/CodeQwen1.5-7B_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.5650118203309693, "Recall": 0.5872235872235873, "F1_Score": 0.5759036144578314, "rel_type_metrics": {"Work_for": {"precision": 0.4017857142857143, "recall": 0.5921052631578947, "f1": 0.4787234042553191}, "Organization_based_in": {"precision": 0.5543478260869565, "recall": 0.53125, "f1": 0.5425531914893618}, "Kill": {"precision": 0.8163265306122449, "recall": 0.851063829787234, "f1": 0.8333333333333334}, "Live_in": {"precision": 0.6705882352941176, "recall": 0.5816326530612245, "f1": 0.6229508196721311}, "Located_in": {"precision": 0.5411764705882353, "recall": 0.5111111111111111, "f1": 0.5257142857142858}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-12-21-36", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}]