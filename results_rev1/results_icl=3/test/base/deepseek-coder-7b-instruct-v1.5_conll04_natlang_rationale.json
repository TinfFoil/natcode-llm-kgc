[{"Model": "deepseek-ai/deepseek-coder-7b-instruct-v1.5", "Precision": 0.060790273556231005, "Recall": 0.09828009828009827, "F1_Score": 0.07511737089201878, "rel_type_metrics": {"Live_in": {"precision": 0, "recall": 0.0, "f1": 0}, "Kill": {"precision": 0, "recall": 0.0, "f1": 0}, "Work_for": {"precision": 0.2777777777777778, "recall": 0.13157894736842105, "f1": 0.17857142857142858}, "Organization_based_in": {"precision": 0, "recall": 0.0, "f1": 0}, "Located_in": {"precision": 0.11363636363636363, "recall": 0.3333333333333333, "f1": 0.16949152542372878}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-26-21-18-24", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "deepseek-ai/deepseek-coder-7b-instruct-v1.5", "Precision": 0.059964726631393295, "Recall": 0.08353808353808354, "F1_Score": 0.06981519507186858, "rel_type_metrics": {"Organization_based_in": {"precision": 0.7368421052631579, "recall": 0.14583333333333334, "f1": 0.24347826086956523}, "Kill": {"precision": 0, "recall": 0.0, "f1": 0}, "Live_in": {"precision": 0.12345679012345678, "recall": 0.10204081632653061, "f1": 0.111731843575419}, "Work_for": {"precision": 0, "recall": 0.0, "f1": 0}, "Located_in": {"precision": 0.17857142857142858, "recall": 0.1111111111111111, "f1": 0.13698630136986303}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-15-00-29", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "deepseek-ai/deepseek-coder-7b-instruct-v1.5", "Precision": 0.02830188679245283, "Recall": 0.051597051597051594, "F1_Score": 0.03655352480417754, "rel_type_metrics": {"Kill": {"precision": 1.0, "recall": 0.0425531914893617, "f1": 0.08163265306122448}, "Located_in": {"precision": 0.13636363636363635, "recall": 0.06666666666666667, "f1": 0.08955223880597016}, "Organization_based_in": {"precision": 0.9, "recall": 0.09375, "f1": 0.169811320754717}, "Live_in": {"precision": 0.08333333333333333, "recall": 0.01020408163265306, "f1": 0.01818181818181818}, "Work_for": {"precision": 0.04411764705882353, "recall": 0.039473684210526314, "f1": 0.04166666666666667}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-01-57-46", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": true, "chat_model": true}]