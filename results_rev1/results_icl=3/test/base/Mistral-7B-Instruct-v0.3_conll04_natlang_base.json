[{"Model": "mistralai/Mistral-7B-Instruct-v0.3", "Precision": 0.032225579053373615, "Recall": 0.07862407862407862, "F1_Score": 0.045714285714285714, "rel_type_metrics": {"Work_for": {"precision": 0, "recall": 0.0, "f1": 0}, "Located_in": {"precision": 0.21333333333333335, "recall": 0.17777777777777778, "f1": 0.19393939393939397}, "Organization_based_in": {"precision": 0.2727272727272727, "recall": 0.125, "f1": 0.17142857142857143}, "Kill": {"precision": 0, "recall": 0.0, "f1": 0}, "Live_in": {"precision": 0.36363636363636365, "recall": 0.04081632653061224, "f1": 0.07339449541284403}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-10-54-27", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "mistralai/Mistral-7B-Instruct-v0.3", "Precision": 0.053987730061349694, "Recall": 0.10810810810810811, "F1_Score": 0.07201309328968904, "rel_type_metrics": {"Located_in": {"precision": 0.2348993288590604, "recall": 0.3888888888888889, "f1": 0.2928870292887029}, "Live_in": {"precision": 0.10204081632653061, "recall": 0.05102040816326531, "f1": 0.06802721088435375}, "Work_for": {"precision": 0.0, "recall": 0.0, "f1": 0}, "Kill": {"precision": 0, "recall": 0.0, "f1": 0}, "Organization_based_in": {"precision": 1.0, "recall": 0.041666666666666664, "f1": 0.07999999999999999}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-10-55-42", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "mistralai/Mistral-7B-Instruct-v0.3", "Precision": 0.04669260700389105, "Recall": 0.08845208845208845, "F1_Score": 0.06112054329371815, "rel_type_metrics": {"Organization_based_in": {"precision": 0, "recall": 0.0, "f1": 0}, "Located_in": {"precision": 0.2619047619047619, "recall": 0.12222222222222222, "f1": 0.16666666666666666}, "Live_in": {"precision": 0.3888888888888889, "recall": 0.07142857142857142, "f1": 0.12068965517241378}, "Kill": {"precision": 0.14285714285714285, "recall": 0.14893617021276595, "f1": 0.14583333333333334}, "Work_for": {"precision": 0.5238095238095238, "recall": 0.14473684210526316, "f1": 0.22680412371134023}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-10-57-08", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": true, "chat_model": true}]