text: We propose a corpus-based method ( Biber ,1993 ; Nagao ,1993 ; Smadja ,1993 ) which generates Noun Classifier Associations ( NCA ) to overcome the problems in classifier assignment and semantic construction of noun phrase .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
corpus-based method
Noun Classifier Associations ( NCA )
classifier assignment
corpus-based method
classifier assignment
semantic construction of noun phrase
semantic construction of noun phrase
corpus-based method
Noun Classifier Associations ( NCA )
semantic construction of noun phrase

triple_list: [["corpus-based method", "Used_for", "Noun Classifier Associations ( NCA )"], ["classifier assignment", "Used_for", "corpus-based method"], ["classifier assignment", "Conjunction", "semantic construction of noun phrase"], ["semantic construction of noun phrase", "Used_for", "corpus-based method"], ["Noun Classifier Associations ( NCA )", "Used_for", "semantic construction of noun phrase"]]
pred: [('corpus-based method', 'Used_for', 'Noun Classifier Associations ( NCA )'), ('classifier assignment', 'Used_for', 'corpus-based method'), ('classifier assignment', 'Conjunction', 'semantic construction of noun phrase'), ('semantic construction of noun phrase', 'Used_for', 'corpus-based method'), ('Noun Classifier Associations ( NCA )', 'Used_for', 'semantic construction of noun phrase')]
trues: [['corpus-based method', 'Used_for', 'Noun Classifier Associations ( NCA )'], ['corpus-based method', 'Used_for', 'classifier assignment'], ['corpus-based method', 'Used_for', 'semantic construction of noun phrase'], ['Noun Classifier Associations ( NCA )', 'Used_for', 'classifier assignment'], ['Noun Classifier Associations ( NCA )', 'Used_for', 'semantic construction of noun phrase'], ['classifier assignment', 'Conjunction', 'semantic construction of noun phrase']]
metrics_sample: [0.6, 0.5, 0.5454545454545454]
metrics_current: [0.6, 0.5, 0.5454545454545454]
text: Experiments on synthetic and real data show that both our triangulation and LAGC algorithms outperform state-of-the-art solutions in accuracy and visual quality .
result: The candidate relations for this text are:
Compare
Evaluate_for
Evaluate_for
Evaluate_for
The candidate entities for this text are:
triangulation
LAGC algorithms
accuracy
triangulation
accuracy
LAGC algorithms
visual quality
triangulation
visual quality
LAGC algorithms

triple_list: [["triangulation", "Compare", "LAGC algorithms"], ["accuracy", "Evaluate_for", "triangulation"], ["accuracy", "Evaluate_for", "LAGC algorithms"], ["visual quality", "Evaluate_for", "triangulation"], ["visual quality", "Evaluate_for", "LAGC algorithms"]]
pred: [('triangulation', 'Compare', 'LAGC algorithms'), ('accuracy', 'Evaluate_for', 'triangulation'), ('accuracy', 'Evaluate_for', 'LAGC algorithms'), ('visual quality', 'Evaluate_for', 'triangulation'), ('visual quality', 'Evaluate_for', 'LAGC algorithms')]
trues: [['synthetic and real data', 'Evaluate_for', 'triangulation and LAGC algorithms'], ['synthetic and real data', 'Evaluate_for', 'state-of-the-art solutions'], ['triangulation and LAGC algorithms', 'Compare', 'state-of-the-art solutions'], ['accuracy', 'Evaluate_for', 'triangulation and LAGC algorithms'], ['accuracy', 'Evaluate_for', 'state-of-the-art solutions'], ['visual quality', 'Evaluate_for', 'triangulation and LAGC algorithms'], ['visual quality', 'Evaluate_for', 'state-of-the-art solutions']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3, 0.23076923076923078, 0.2608695652173913]
text: Thus , in this paper , we study the problem of robust PCA with side information , where both prior structure and features of entities are exploited for recovery .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
prior structure
robust PCA with side information
features of entities
robust PCA with side information
features of entities
prior structure

triple_list: [["prior structure", "Used_for", "robust PCA with side information"], ["features of entities", "Used_for", "robust PCA with side information"], ["features of entities", "Used_for", "prior structure"]]
pred: [('prior structure', 'Used_for', 'robust PCA with side information'), ('features of entities', 'Used_for', 'robust PCA with side information'), ('features of entities', 'Used_for', 'prior structure')]
trues: [['side information', 'Used_for', 'robust PCA'], ['prior structure', 'Conjunction', 'features of entities'], ['prior structure', 'Used_for', 'recovery'], ['features of entities', 'Used_for', 'recovery']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.23076923076923078, 0.17647058823529413, 0.20000000000000004]
text: The proposed approach does not require a separate segmentation stage and is applicable to cluttered scenes .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
approach
cluttered scenes

triple_list: [["approach", "Used_for", "cluttered scenes"]]
pred: [('approach', 'Used_for', 'cluttered scenes')]
trues: [['approach', 'Used_for', 'cluttered scenes']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.2857142857142857, 0.2222222222222222, 0.25]
text: In the Object Recognition task , there exists a di-chotomy between the categorization of objects and estimating object pose , where the former necessitates a view-invariant representation , while the latter requires a representation capable of capturing pose information over different categories of objects .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
categorization of objects
Object Recognition task
estimating object pose
Object Recognition task
categorization of objects
estimating object pose
view-invariant representation
categorization of objects
representation
estimating object pose

triple_list: [["categorization of objects", "Used_for", "Object Recognition task"], ["estimating object pose", "Used_for", "Object Recognition task"], ["categorization of objects", "Conjunction", "estimating object pose"], ["view-invariant representation", "Used_for", "categorization of objects"], ["representation", "Used_for", "estimating object pose"]]
pred: [('categorization of objects', 'Used_for', 'Object Recognition task'), ('estimating object pose', 'Used_for', 'Object Recognition task'), ('categorization of objects', 'Conjunction', 'estimating object pose'), ('view-invariant representation', 'Used_for', 'categorization of objects'), ('representation', 'Used_for', 'estimating object pose')]
trues: [['categorization of objects', 'Part_of', 'Object Recognition task'], ['categorization of objects', 'Conjunction', 'estimating object pose'], ['estimating object pose', 'Part_of', 'Object Recognition task'], ['view-invariant representation', 'Used_for', 'former'], ['representation', 'Used_for', 'latter'], ['representation', 'Used_for', 'pose information']]
metrics_sample: [0.2, 0.16666666666666666, 0.1818181818181818]
metrics_current: [0.2631578947368421, 0.20833333333333334, 0.23255813953488372]
text: This paper presents the results of automatically inducing a Combinatory Categorial Grammar ( CCG ) lexicon from a Turkish dependency treebank .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
Turkish dependency treebank
Combinatory Categorial Grammar ( CCG ) lexicon
Turkish dependency treebank
Combinatory Categorial Grammar ( CCG ) lexicon

triple_list: [["Turkish dependency treebank", "Used_for", "Combinatory Categorial Grammar ( CCG ) lexicon"], ["Turkish dependency treebank", "Used_for", "Combinatory Categorial Grammar ( CCG ) lexicon"]]
pred: [('Turkish dependency treebank', 'Used_for', 'Combinatory Categorial Grammar ( CCG ) lexicon'), ('Turkish dependency treebank', 'Used_for', 'Combinatory Categorial Grammar ( CCG ) lexicon')]
trues: [['Combinatory Categorial Grammar ( CCG ) lexicon', 'Part_of', 'Turkish dependency treebank']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.25, 0.2, 0.22222222222222224]
text: To improve topical blog post retrieval we incorporate textual credibility indicators in the retrieval process .
result: The candidate relations for this text are:
Used_for
Part_of
The candidate entities for this text are:
textual credibility indicators
topical blog post retrieval
textual credibility indicators
retrieval process

triple_list: [["textual credibility indicators", "Used_for", "topical blog post retrieval"], ["textual credibility indicators", "Part_of", "retrieval process"]]
pred: [('textual credibility indicators', 'Used_for', 'topical blog post retrieval'), ('textual credibility indicators', 'Part_of', 'retrieval process')]
trues: [['textual credibility indicators', 'Used_for', 'topical blog post retrieval'], ['textual credibility indicators', 'Part_of', 'retrieval process']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3181818181818182, 0.25925925925925924, 0.2857142857142857]
text: In this paper we propose a logical formalism , which , among other things , is suitable for representing determiners without forcing a particular interpretation when their meaning is still not clear .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
logical formalism
determiners

triple_list: [["logical formalism", "Used_for", "determiners"]]
pred: [('logical formalism', 'Used_for', 'determiners')]
trues: [['logical formalism', 'Used_for', 'determiners']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.34782608695652173, 0.2857142857142857, 0.3137254901960784]
text: Based on this , MINPRAN uses random sampling to search for the t and the number of inliers to the t that are least likely to have occurred randomly .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
random sampling
MINPRAN
random sampling
number of inliers

triple_list: [["random sampling", "Used_for", "MINPRAN"], ["random sampling", "Used_for", "number of inliers"]]
pred: [('random sampling', 'Used_for', 'MINPRAN'), ('random sampling', 'Used_for', 'number of inliers')]
trues: [['random sampling', 'Used_for', 'MINPRAN']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.36, 0.3103448275862069, 0.3333333333333333]
text: We have built and will demonstrate an application of this approach called LCS-Marine .
result: The candidate relations for this text are:
Hyponym_of
The candidate entities for this text are:
LCS-Marine
approach

triple_list: [["LCS-Marine", "Hyponym_of", "approach"]]
pred: [('LCS-Marine', 'Hyponym_of', 'approach')]
trues: [['approach', 'Used_for', 'LCS-Marine']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.34615384615384615, 0.3, 0.32142857142857145]
text: GLOSSER is designed to support reading and learning to read in a foreign language .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
GLOSSER
reading and learning to read in a foreign language

triple_list: [["GLOSSER", "Used_for", "reading and learning to read in a foreign language"]]
pred: [('GLOSSER', 'Used_for', 'reading and learning to read in a foreign language')]
trues: [['GLOSSER', 'Used_for', 'reading and learning']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3333333333333333, 0.2903225806451613, 0.3103448275862069]
text: A mixed-signal paradigm is presented for high-resolution parallel inner-product computation in very high dimensions , suitable for efficient implementation of kernels in image processing .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
mixed-signal paradigm
high-resolution parallel inner-product computation
high-resolution parallel inner-product computation
image processing

triple_list: [["mixed-signal paradigm", "Used_for", "high-resolution parallel inner-product computation"], ["high-resolution parallel inner-product computation", "Used_for", "image processing"]]
pred: [('mixed-signal paradigm', 'Used_for', 'high-resolution parallel inner-product computation'), ('high-resolution parallel inner-product computation', 'Used_for', 'image processing')]
trues: [['mixed-signal paradigm', 'Used_for', 'high-resolution parallel inner-product computation'], ['mixed-signal paradigm', 'Used_for', 'kernels'], ['kernels', 'Used_for', 'image processing']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.3448275862068966, 0.29411764705882354, 0.31746031746031744]
text: Furthermore , we show how the recently developed efficient subwindow search ( ESS ) procedure [ 11 ] can be integrated into the last stage of our method .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
efficient subwindow search ( ESS ) procedure
method
last stage
method

triple_list: [["efficient subwindow search ( ESS ) procedure", "Used_for", "method"], ["last stage", "Used_for", "method"]]
pred: [('efficient subwindow search ( ESS ) procedure', 'Used_for', 'method'), ('last stage', 'Used_for', 'method')]
trues: [['subwindow search ( ESS ) procedure', 'Part_of', 'method']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3225806451612903, 0.2857142857142857, 0.303030303030303]
text: A general procedure for the estimation and quantization of these cycloidal motion parameters for arbitrary handwriting is presented .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
procedure
estimation and quantization of these cycloidal motion parameters

triple_list: [["procedure", "Used_for", "estimation and quantization of these cycloidal motion parameters"]]
pred: [('procedure', 'Used_for', 'estimation and quantization of these cycloidal motion parameters')]
trues: [['cycloidal motion parameters', 'Used_for', 'arbitrary handwriting']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3125, 0.2777777777777778, 0.29411764705882354]
text: When the scene exhibits a persistent dynamic behavior in time , such an assumption is violated and detection performance deteriorates .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
detection
persistent dynamic behavior in time

triple_list: [["detection", "Used_for", "persistent dynamic behavior in time"]]
pred: [('detection', 'Used_for', 'persistent dynamic behavior in time')]
trues: [['persistent dynamic behavior', 'Feature_of', 'scene']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30303030303030304, 0.2702702702702703, 0.2857142857142857]
text: For non-LR grammars the time-complexity of our parser is cubic if the functions that constitute the parser are implemented as memo-functions , i.e. functions that memorize the results of previous invocations .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
memo-functions
functions

triple_list: [["memo-functions", "Used_for", "functions"]]
pred: [('memo-functions', 'Used_for', 'functions')]
trues: [['time-complexity', 'Evaluate_for', 'parser'], ['parser', 'Used_for', 'non-LR grammars'], ['memo-functions', 'Used_for', 'parser']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.29411764705882354, 0.25, 0.27027027027027023]
text: It has also been studied in the framework of Japanese information extraction ( [ 3 ] ) in recent years .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Japanese information extraction
It

triple_list: [["Japanese information extraction", "Used_for", "It"]]
pred: [('Japanese information extraction', 'Used_for', 'It')]
trues: [['Japanese information extraction', 'Used_for', 'It']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3142857142857143, 0.2682926829268293, 0.2894736842105263]
text: The results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis for a useful disambiguation tool .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
cooccurrence statistics
disambiguation tool

triple_list: [["cooccurrence statistics", "Used_for", "disambiguation tool"]]
pred: [('cooccurrence statistics', 'Used_for', 'disambiguation tool')]
trues: [['cooccurrence statistics', 'Used_for', 'disambiguation tool']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3333333333333333, 0.2857142857142857, 0.30769230769230765]
text: This probably occurs because each model has different strengths and weaknesses for modeling the knowledge sources .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
model
knowledge sources

triple_list: [["model", "Used_for", "knowledge sources"]]
pred: [('model', 'Used_for', 'knowledge sources')]
trues: [['model', 'Used_for', 'knowledge sources']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35135135135135137, 0.3023255813953488, 0.32499999999999996]
text: The subjects were given three minutes per extract to determine whether they believed the sample output to be an expert human translation or a machine translation .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
three minutes
extract

triple_list: [["three minutes", "Used_for", "extract"]]
pred: [('three minutes', 'Used_for', 'extract')]
trues: [['expert human translation', 'Compare', 'machine translation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.34210526315789475, 0.29545454545454547, 0.3170731707317074]
text: Many of the resources used are derived from data created by human beings out of an NLP context , especially regarding MT and reference translations .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
resources
NLP context
human beings
resources
MT
reference translations
MT
reference translations

triple_list: [["resources", "Used_for", "NLP context"], ["human beings", "Used_for", "resources"], ["MT", "Conjunction", "reference translations"], ["MT", "Used_for", "reference translations"]]
pred: [('resources', 'Used_for', 'NLP context'), ('human beings', 'Used_for', 'resources'), ('MT', 'Conjunction', 'reference translations'), ('MT', 'Used_for', 'reference translations')]
trues: [['MT', 'Hyponym_of', 'NLP'], ['MT', 'Conjunction', 'reference translations'], ['reference translations', 'Hyponym_of', 'NLP']]
metrics_sample: [0.25, 0.3333333333333333, 0.28571428571428575]
metrics_current: [0.3333333333333333, 0.2978723404255319, 0.3146067415730337]
text: The applicability of many current information extraction techniques is severely limited by the need for supervised training data .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
supervised training data
information extraction techniques

triple_list: [["supervised training data", "Used_for", "information extraction techniques"]]
pred: [('supervised training data', 'Used_for', 'information extraction techniques')]
trues: [['supervised training data', 'Used_for', 'information extraction techniques']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3488372093023256, 0.3125, 0.32967032967032966]
text: We show that there is a closed-form ( analytical ) solution for one part of the Kullback-Leibler distance , viz the cross-entropy .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
analytical solution
Kullback-Leibler distance

triple_list: [["analytical solution", "Used_for", "Kullback-Leibler distance"]]
pred: [('analytical solution', 'Used_for', 'Kullback-Leibler distance')]
trues: [['closed-form ( analytical ) solution', 'Used_for', 'Kullback-Leibler distance'], ['closed-form ( analytical ) solution', 'Used_for', 'cross-entropy'], ['cross-entropy', 'Part_of', 'Kullback-Leibler distance']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3409090909090909, 0.29411764705882354, 0.3157894736842105]
text: b ) The UV procedure is based on three different confidence tests , two based on acoustic measures and one founded on linguistic information , applied in a hierarchical structure .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
confidence tests
UV procedure
acoustic measures
confidence tests
acoustic measures
linguistic information
linguistic information
confidence tests
hierarchical structure
confidence tests

triple_list: [["confidence tests", "Used_for", "UV procedure"], ["acoustic measures", "Used_for", "confidence tests"], ["acoustic measures", "Used_for", "linguistic information"], ["linguistic information", "Conjunction", "confidence tests"], ["hierarchical structure", "Used_for", "confidence tests"]]
pred: [('confidence tests', 'Used_for', 'UV procedure'), ('acoustic measures', 'Used_for', 'confidence tests'), ('acoustic measures', 'Used_for', 'linguistic information'), ('linguistic information', 'Conjunction', 'confidence tests'), ('hierarchical structure', 'Used_for', 'confidence tests')]
trues: [['confidence tests', 'Used_for', 'UV procedure'], ['confidence tests', 'Used_for', 'hierarchical structure'], ['two', 'Hyponym_of', 'confidence tests'], ['acoustic measures', 'Used_for', 'two'], ['one', 'Hyponym_of', 'confidence tests'], ['linguistic information', 'Used_for', 'one']]
metrics_sample: [0.2, 0.16666666666666666, 0.1818181818181818]
metrics_current: [0.32653061224489793, 0.2807017543859649, 0.30188679245283023]
text: This allows us to use our method to act not only as a faster procedure for cascade evaluation , but also as a tool to perform efficient branch-and-bound object detection with nonlinear quality functions , in particular kernel-ized support vector machines .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Used_for
The candidate entities for this text are:
method
cascade evaluation
method
branch-and-bound object detection
nonlinear quality functions
branch-and-bound object detection
kernel-ized support vector machines
nonlinear quality functions

triple_list: [["method", "Used_for", "cascade evaluation"], ["method", "Used_for", "branch-and-bound object detection"], ["nonlinear quality functions", "Used_for", "branch-and-bound object detection"], ["kernel-ized support vector machines", "Used_for", "nonlinear quality functions"]]
pred: [('method', 'Used_for', 'cascade evaluation'), ('method', 'Used_for', 'branch-and-bound object detection'), ('nonlinear quality functions', 'Used_for', 'branch-and-bound object detection'), ('kernel-ized support vector machines', 'Used_for', 'nonlinear quality functions')]
trues: [['method', 'Used_for', 'cascade evaluation'], ['method', 'Used_for', 'branch-and-bound object detection'], ['nonlinear quality functions', 'Used_for', 'branch-and-bound object detection'], ['kernel-ized support vector machines', 'Hyponym_of', 'nonlinear quality functions']]
metrics_sample: [0.75, 0.75, 0.75]
metrics_current: [0.3584905660377358, 0.3114754098360656, 0.3333333333333333]
text: Bayesian inference then samples the rules , disambiguating and combining them to create complex tree structures that maximize a discriminative model 's posterior on a target unlabeled corpus .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
Bayesian inference
rules
rules
tree structures
tree structures
discriminative model 's posterior

triple_list: [["Bayesian inference", "Used_for", "rules"], ["rules", "Used_for", "tree structures"], ["tree structures", "Used_for", "discriminative model 's posterior"]]
pred: [('Bayesian inference', 'Used_for', 'rules'), ('rules', 'Used_for', 'tree structures'), ('tree structures', 'Used_for', "discriminative model 's posterior")]
trues: [['Bayesian inference', 'Used_for', 'rules'], ['them', 'Used_for', 'complex tree structures'], ['complex tree structures', 'Used_for', "discriminative model 's posterior"], ['unlabeled corpus', 'Used_for', "discriminative model 's posterior"]]
metrics_sample: [0.3333333333333333, 0.25, 0.28571428571428575]
metrics_current: [0.35714285714285715, 0.3076923076923077, 0.3305785123966942]
text: We show how features that are imaged through a transparent object behave differently from those that are rigidly attached to the scene .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
features
those

triple_list: [["features", "Compare", "those"]]
pred: [('features', 'Compare', 'those')]
trues: [['those', 'Compare', 'features']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3508771929824561, 0.30303030303030304, 0.3252032520325204]
text: Learned confidence measures gain increasing importance for outlier removal and quality improvement in stereo vision .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
Learned confidence measures
outlier removal
Learned confidence measures
quality improvement

triple_list: [["Learned confidence measures", "Used_for", "outlier removal"], ["Learned confidence measures", "Used_for", "quality improvement"]]
pred: [('Learned confidence measures', 'Used_for', 'outlier removal'), ('Learned confidence measures', 'Used_for', 'quality improvement')]
trues: [['Learned confidence measures', 'Used_for', 'outlier removal'], ['Learned confidence measures', 'Used_for', 'quality improvement'], ['outlier removal', 'Conjunction', 'quality improvement'], ['outlier removal', 'Part_of', 'stereo vision'], ['quality improvement', 'Part_of', 'stereo vision']]
metrics_sample: [1.0, 0.4, 0.5714285714285715]
metrics_current: [0.3728813559322034, 0.30985915492957744, 0.3384615384615385]
text: Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Used_for
The candidate entities for this text are:
syntactic structure features
relation extraction
parse tree
syntactic structure features
convolution tree kernel
syntactic structure features
convolution tree kernel
relation extraction

triple_list: [["syntactic structure features", "Used_for", "relation extraction"], ["parse tree", "Used_for", "syntactic structure features"], ["convolution tree kernel", "Used_for", "syntactic structure features"], ["convolution tree kernel", "Used_for", "relation extraction"]]
pred: [('syntactic structure features', 'Used_for', 'relation extraction'), ('parse tree', 'Used_for', 'syntactic structure features'), ('convolution tree kernel', 'Used_for', 'syntactic structure features'), ('convolution tree kernel', 'Used_for', 'relation extraction')]
trues: [['syntactic structure features', 'Feature_of', 'parse tree'], ['syntactic structure features', 'Used_for', 'relation extraction'], ['convolution tree kernel', 'Used_for', 'features']]
metrics_sample: [0.25, 0.3333333333333333, 0.28571428571428575]
metrics_current: [0.36507936507936506, 0.3108108108108108, 0.3357664233576642]
text: Owing to these variations , the pedestrian data is distributed as highly-curved manifolds in the feature space , despite the current convolutional neural networks ( CNN ) 's capability of feature extraction .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
feature space
pedestrian data
convolutional neural networks ( CNN )
feature extraction

triple_list: [["feature space", "Used_for", "pedestrian data"], ["convolutional neural networks ( CNN )", "Used_for", "feature extraction"]]
pred: [('feature space', 'Used_for', 'pedestrian data'), ('convolutional neural networks ( CNN )', 'Used_for', 'feature extraction')]
trues: [['highly-curved manifolds', 'Used_for', 'pedestrian data'], ['feature space', 'Feature_of', 'highly-curved manifolds'], ['convolutional neural networks ( CNN )', 'Used_for', 'feature extraction']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.36923076923076925, 0.3116883116883117, 0.3380281690140845]
text: It is presented as a generalization of the recursive descent parser .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
It
recursive descent parser

triple_list: [["It", "Used_for", "recursive descent parser"]]
pred: [('It', 'Used_for', 'recursive descent parser')]
trues: [['recursive descent parser', 'Used_for', 'It']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.36363636363636365, 0.3076923076923077, 0.33333333333333337]
text: Techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches .
result: The candidate relations for this text are:
Used_for
Compare
The candidate entities for this text are:
natural language generator
automatically training modules
hand-crafted template-based or rule-based approaches
natural language generator

triple_list: [["natural language generator", "Used_for", "automatically training modules"], ["hand-crafted template-based or rule-based approaches", "Compare", "natural language generator"]]
pred: [('natural language generator', 'Used_for', 'automatically training modules'), ('hand-crafted template-based or rule-based approaches', 'Compare', 'natural language generator')]
trues: [['Techniques', 'Used_for', 'automatically training modules'], ['automatically training modules', 'Part_of', 'natural language generator'], ['utterances', 'Evaluate_for', 'trainable components'], ['utterances', 'Evaluate_for', 'hand-crafted template-based or rule-based approaches'], ['trainable components', 'Compare', 'hand-crafted template-based or rule-based approaches']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35294117647058826, 0.2891566265060241, 0.31788079470198677]
text: Finally , we show how the estimation of the tensors from point correspondences is achieved through factorization , and discuss the estimation from line correspondences .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
factorization
estimation of the tensors
line correspondences
estimation

triple_list: [["factorization", "Used_for", "estimation of the tensors"], ["line correspondences", "Used_for", "estimation"]]
pred: [('factorization', 'Used_for', 'estimation of the tensors'), ('line correspondences', 'Used_for', 'estimation')]
trues: [['point correspondences', 'Used_for', 'estimation of the tensors'], ['factorization', 'Used_for', 'tensors'], ['line correspondences', 'Used_for', 'estimation']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.35714285714285715, 0.29069767441860467, 0.32051282051282054]
text: In particular there are three areas of novelty : ( i ) we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; ( ii ) we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and ( iii ) we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Conjunction
Used_for
Used_for
Used_for
The candidate entities for this text are:
photometric model of image formation
generalized face recognition
statistical model of generic face appearance variation
generalized face recognition
statistical model of generic face appearance variation
photometric model of image formation
generalized face recognition
illumination changes
robustness
generalized face recognition
robust same-identity likelihood
generalized face recognition
robustness
face motion patterns

triple_list: [["photometric model of image formation", "Used_for", "generalized face recognition"], ["statistical model of generic face appearance variation", "Used_for", "generalized face recognition"], ["statistical model of generic face appearance variation", "Used_for", "photometric model of image formation"], ["illumination changes", "Conjunction", "generalized face recognition"], ["robustness", "Used_for", "generalized face recognition"], ["robust same-identity likelihood", "Used_for", "generalized face recognition"], ["robustness", "Used_for", "face motion patterns"]]
pred: [('photometric model of image formation', 'Used_for', 'generalized face recognition'), ('statistical model of generic face appearance variation', 'Used_for', 'generalized face recognition'), ('statistical model of generic face appearance variation', 'Used_for', 'photometric model of image formation'), ('illumination changes', 'Conjunction', 'generalized face recognition'), ('robustness', 'Used_for', 'generalized face recognition'), ('robust same-identity likelihood', 'Used_for', 'generalized face recognition'), ('robustness', 'Used_for', 'face motion patterns')]
trues: [['photometric model', 'Used_for', 'image formation'], ['photometric model', 'Conjunction', 'statistical model'], ['statistical model', 'Used_for', 'generic face appearance variation'], ['statistical model', 'Used_for', 'extreme illumination changes'], ['smoothness', 'Feature_of', 'geodesically local appearance manifold structure'], ['geodesically local appearance manifold structure', 'Conjunction', 'robust same-identity likelihood'], ['robustness', 'Evaluate_for', "video sequence '' reillumination '' algorithm"], ['face motion patterns', 'Feature_of', 'robustness'], ['face motion patterns', 'Part_of', 'video']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3246753246753247, 0.2631578947368421, 0.29069767441860467]
text: Our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments .
result: The candidate relations for this text are:
Compare
Evaluate_for
Evaluate_for
The candidate entities for this text are:
technique
models
paraphrase classification accuracy
technique
paraphrase classification accuracy
models

triple_list: [["technique", "Compare", "models"], ["paraphrase classification accuracy", "Evaluate_for", "technique"], ["paraphrase classification accuracy", "Evaluate_for", "models"]]
pred: [('technique', 'Compare', 'models'), ('paraphrase classification accuracy', 'Evaluate_for', 'technique'), ('paraphrase classification accuracy', 'Evaluate_for', 'models')]
trues: [['technique', 'Compare', 'models'], ['paraphrase classification accuracy', 'Evaluate_for', 'technique'], ['paraphrase classification accuracy', 'Evaluate_for', 'models']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35, 0.2857142857142857, 0.3146067415730337]
text: Although hidden Markov models ( HMMs ) provide a suitable generative model for field structured text , general unsupervised HMM learning fails to learn useful structure in either of our domains .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
hidden Markov models ( HMMs )
generative model
generative model
field structured text

triple_list: [["hidden Markov models ( HMMs )", "Used_for", "generative model"], ["generative model", "Used_for", "field structured text"]]
pred: [('hidden Markov models ( HMMs )', 'Used_for', 'generative model'), ('generative model', 'Used_for', 'field structured text')]
trues: [['hidden Markov models ( HMMs )', 'Used_for', 'generative model'], ['generative model', 'Used_for', 'field structured text']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.36585365853658536, 0.3, 0.3296703296703297]
text: Moreover , a cost zero solution always exists and can only be excluded by hard budget constraints .
result: The candidate relations for this text are:
Feature_of
The candidate entities for this text are:
cost zero solution
hard budget constraints

triple_list: [["cost zero solution", "Feature_of", "hard budget constraints"]]
pred: [('cost zero solution', 'Feature_of', 'hard budget constraints')]
trues: [['hard budget constraints', 'Used_for', 'cost zero solution']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3614457831325301, 0.297029702970297, 0.32608695652173914]
text: Our proposed approach builds on recent advances in spectral clustering , image matting , and boundary detection .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Conjunction
The candidate entities for this text are:
spectral clustering
approach
spectral clustering
image matting
image matting
approach
image matting
boundary detection
boundary detection
approach

triple_list: [["spectral clustering", "Used_for", "approach"], ["spectral clustering", "Used_for", "image matting"], ["image matting", "Used_for", "approach"], ["image matting", "Conjunction", "boundary detection"], ["boundary detection", "Used_for", "approach"]]
pred: [('spectral clustering', 'Used_for', 'approach'), ('spectral clustering', 'Used_for', 'image matting'), ('image matting', 'Used_for', 'approach'), ('image matting', 'Conjunction', 'boundary detection'), ('boundary detection', 'Used_for', 'approach')]
trues: [['spectral clustering', 'Used_for', 'approach'], ['spectral clustering', 'Conjunction', 'image matting'], ['image matting', 'Used_for', 'approach'], ['image matting', 'Conjunction', 'boundary detection'], ['boundary detection', 'Used_for', 'approach']]
metrics_sample: [0.8, 0.8, 0.8000000000000002]
metrics_current: [0.38636363636363635, 0.32075471698113206, 0.35051546391752575]
text: We apply cluster analysis on the sampled parameter space to redetect the object and renew the local tracker .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
cluster analysis
object
sampled parameter space
cluster analysis

triple_list: [["cluster analysis", "Used_for", "object"], ["sampled parameter space", "Used_for", "cluster analysis"]]
pred: [('cluster analysis', 'Used_for', 'object'), ('sampled parameter space', 'Used_for', 'cluster analysis')]
trues: [['cluster analysis', 'Used_for', 'sampled parameter space'], ['cluster analysis', 'Used_for', 'local tracker']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.37777777777777777, 0.3148148148148148, 0.34343434343434337]
text: In this situation , the general method is to segment the raw corpus automatically using a word list , correct the output sentences by hand , and build a model from the segmented corpus .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
word list
segment raw corpus
segmented corpus
model
segmented corpus
model

triple_list: [["word list", "Used_for", "segment raw corpus"], ["segmented corpus", "Used_for", "model"], ["segmented corpus", "Used_for", "model"]]
pred: [('word list', 'Used_for', 'segment raw corpus'), ('segmented corpus', 'Used_for', 'model'), ('segmented corpus', 'Used_for', 'model')]
trues: [['method', 'Used_for', 'raw corpus'], ['word list', 'Used_for', 'method'], ['segmented corpus', 'Used_for', 'model']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.3804347826086957, 0.3153153153153153, 0.3448275862068966]
text: We conclude that previous approaches have neglected to evaluate systems in the context of their use , e.g. solving a task requiring data retrieval .
result: The candidate relations for this text are:
Evaluate_for
The candidate entities for this text are:
systems
task requiring data retrieval

triple_list: [["systems", "Evaluate_for", "task requiring data retrieval"]]
pred: [('systems', 'Evaluate_for', 'task requiring data retrieval')]
trues: [['approaches', 'Evaluate_for', 'systems'], ['systems', 'Used_for', 'task'], ['data retrieval', 'Part_of', 'task']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3763440860215054, 0.30701754385964913, 0.3381642512077295]
text: Our morphological analyzer has done all the necessary work for the recognition and classification of proper names , numerical and temporal expressions , i.e. Named Entity ( NE ) items in the Japanese text .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
morphological analyzer
recognition and classification of proper names
morphological analyzer
numerical and temporal expressions

triple_list: [["morphological analyzer", "Used_for", "recognition and classification of proper names"], ["morphological analyzer", "Used_for", "numerical and temporal expressions"]]
pred: [('morphological analyzer', 'Used_for', 'recognition and classification of proper names'), ('morphological analyzer', 'Used_for', 'numerical and temporal expressions')]
trues: [['morphological analyzer', 'Used_for', 'recognition and classification of proper names , numerical and temporal expressions'], ['Named Entity ( NE ) items', 'Hyponym_of', 'proper names , numerical and temporal expressions'], ['Named Entity ( NE ) items', 'Part_of', 'Japanese text']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3684210526315789, 0.29914529914529914, 0.330188679245283]
text: We then derive a scalable stochastic gradient descent algorithm for the resulting learning problem .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
stochastic gradient descent algorithm
learning problem

triple_list: [["stochastic gradient descent algorithm", "Used_for", "learning problem"]]
pred: [('stochastic gradient descent algorithm', 'Used_for', 'learning problem')]
trues: [['stochastic gradient descent algorithm', 'Used_for', 'learning problem']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.375, 0.3050847457627119, 0.3364485981308411]
text: We investigate the verbal and nonverbal means for grounding , and propose a design for embodied conversational agents that relies on both kinds of signals to establish common ground in human-computer interaction .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
verbal and nonverbal means
grounding
verbal and nonverbal means
design
verbal and nonverbal means
embodied conversational agents
design
grounding
human-computer interaction
design

triple_list: [["verbal and nonverbal means", "Used_for", "grounding"], ["verbal and nonverbal means", "Used_for", "design"], ["verbal and nonverbal means", "Used_for", "embodied conversational agents"], ["design", "Conjunction", "grounding"], ["human-computer interaction", "Used_for", "design"]]
pred: [('verbal and nonverbal means', 'Used_for', 'grounding'), ('verbal and nonverbal means', 'Used_for', 'design'), ('verbal and nonverbal means', 'Used_for', 'embodied conversational agents'), ('design', 'Conjunction', 'grounding'), ('human-computer interaction', 'Used_for', 'design')]
trues: [['verbal and nonverbal means', 'Used_for', 'grounding'], ['design', 'Used_for', 'embodied conversational agents'], ['common ground', 'Used_for', 'human-computer interaction']]
metrics_sample: [0.2, 0.3333333333333333, 0.25]
metrics_current: [0.36633663366336633, 0.30578512396694213, 0.3333333333333333]
text: We have implemented a restricted domain parser called Plume .
result: The candidate relations for this text are:
Hyponym_of
The candidate entities for this text are:
Plume
restricted domain parser

triple_list: [["Plume", "Hyponym_of", "restricted domain parser"]]
pred: [('Plume', 'Hyponym_of', 'restricted domain parser')]
trues: [['Plume', 'Hyponym_of', 'restricted domain parser']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.37254901960784315, 0.3114754098360656, 0.3392857142857143]
text: While previous approaches relied on geometric , appearance , or correlation-based information for establishing correspondences between static cameras , they each have well-known limitations and are not extendable to wide-area settings with PTZ cameras .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
geometric , appearance , or correlation-based information
approaches
geometric , appearance , or correlation-based information
correspondences
static cameras
PTZ cameras
PTZ cameras
approaches

triple_list: [["geometric , appearance , or correlation-based information", "Used_for", "approaches"], ["geometric , appearance , or correlation-based information", "Used_for", "correspondences"], ["static cameras", "Conjunction", "PTZ cameras"], ["PTZ cameras", "Used_for", "approaches"]]
pred: [('geometric , appearance , or correlation-based information', 'Used_for', 'approaches'), ('geometric , appearance , or correlation-based information', 'Used_for', 'correspondences'), ('static cameras', 'Conjunction', 'PTZ cameras'), ('PTZ cameras', 'Used_for', 'approaches')]
trues: [['geometric , appearance , or correlation-based information', 'Used_for', 'approaches']]
metrics_sample: [0.25, 1.0, 0.4]
metrics_current: [0.36792452830188677, 0.3170731707317073, 0.34061135371179035]
text: Recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus , so we used visual inspection and automatic speech recognition ( ASR ) to investigate an offset adaptation ( OA ) model proposed by Zhang et al. [ 1 ] .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
visual inspection
pool models of the inner hair cell synapse
automatic speech recognition ( ASR )
pool models of the inner hair cell synapse
offset adaptation ( OA ) model
pool models of the inner hair cell synapse

triple_list: [["visual inspection", "Used_for", "pool models of the inner hair cell synapse"], ["automatic speech recognition ( ASR )", "Used_for", "pool models of the inner hair cell synapse"], ["offset adaptation ( OA ) model", "Used_for", "pool models of the inner hair cell synapse"]]
pred: [('visual inspection', 'Used_for', 'pool models of the inner hair cell synapse'), ('automatic speech recognition ( ASR )', 'Used_for', 'pool models of the inner hair cell synapse'), ('offset adaptation ( OA ) model', 'Used_for', 'pool models of the inner hair cell synapse')]
trues: [['pool models', 'Used_for', 'inner hair cell synapse'], ['visual inspection', 'Conjunction', 'automatic speech recognition ( ASR )'], ['visual inspection', 'Used_for', 'offset adaptation ( OA ) model'], ['automatic speech recognition ( ASR )', 'Used_for', 'offset adaptation ( OA ) model']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3577981651376147, 0.30708661417322836, 0.3305084745762712]
text: We have applied it to real scenes that include transparent objects and recovered the shapes of the objects with high accuracy .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
it
real scenes

triple_list: [["it", "Used_for", "real scenes"]]
pred: [('it', 'Used_for', 'real scenes')]
trues: [['it', 'Used_for', 'real scenes'], ['it', 'Used_for', 'shapes of the objects'], ['transparent objects', 'Part_of', 'real scenes'], ['accuracy', 'Evaluate_for', 'shapes of the objects']]
metrics_sample: [1.0, 0.25, 0.4]
metrics_current: [0.36363636363636365, 0.3053435114503817, 0.3319502074688797]
text: In this paper , we propose a novel moderate positive sample mining method to train robust CNN for person re-identification , dealing with the problem of large variation .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
moderative positive sample mining method
CNN
CNN
person re-identification

triple_list: [["moderative positive sample mining method", "Used_for", "CNN"], ["CNN", "Used_for", "person re-identification"]]
pred: [('moderative positive sample mining method', 'Used_for', 'CNN'), ('CNN', 'Used_for', 'person re-identification')]
trues: [['moderate positive sample mining method', 'Used_for', 'robust CNN'], ['robust CNN', 'Used_for', 'person re-identification']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35714285714285715, 0.3007518796992481, 0.32653061224489793]
text: With the rise of deep archi-tectures , the prime focus has been on object category recognition .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
deep archi-tectures
object category recognition

triple_list: [["deep archi-tectures", "Used_for", "object category recognition"]]
pred: [('deep archi-tectures', 'Used_for', 'object category recognition')]
trues: [['deep archi-tectures', 'Used_for', 'object category recognition']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.36283185840707965, 0.30597014925373134, 0.33198380566801616]
text: We report the performance of the MBR decoders on a Chinese-to-English translation task .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Chinese-to-English translation task
MBR decoders

triple_list: [["Chinese-to-English translation task", "Used_for", "MBR decoders"]]
pred: [('Chinese-to-English translation task', 'Used_for', 'MBR decoders')]
trues: [['MBR decoders', 'Used_for', 'Chinese-to-English translation task']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35964912280701755, 0.3037037037037037, 0.3293172690763052]
text: Towards deep analysis of compositional classes of paraphrases , we have examined a class-oriented framework for collecting paraphrase examples , in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
class-oriented framework
paraphrase examples
automatic candidate generation
paraphrase examples
manual judgement
paraphrase examples

triple_list: [["class-oriented framework", "Used_for", "paraphrase examples"], ["automatic candidate generation", "Used_for", "paraphrase examples"], ["manual judgement", "Used_for", "paraphrase examples"]]
pred: [('class-oriented framework', 'Used_for', 'paraphrase examples'), ('automatic candidate generation', 'Used_for', 'paraphrase examples'), ('manual judgement', 'Used_for', 'paraphrase examples')]
trues: [['class-oriented framework', 'Used_for', 'compositional classes of paraphrases'], ['class-oriented framework', 'Used_for', 'paraphrase examples'], ['automatic candidate generation', 'Used_for', 'sentential paraphrases'], ['automatic candidate generation', 'Conjunction', 'manual judgement'], ['manual judgement', 'Used_for', 'sentential paraphrases']]
metrics_sample: [0.3333333333333333, 0.2, 0.25]
metrics_current: [0.358974358974359, 0.3, 0.32684824902723736]
text: With relevant approach , we identify important contents by PageRank algorithm on the event map constructed from documents .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
PageRank algorithm
contents
documents
PageRank algorithm
documents
event map

triple_list: [["PageRank algorithm", "Used_for", "contents"], ["documents", "Used_for", "PageRank algorithm"], ["documents", "Used_for", "event map"]]
pred: [('PageRank algorithm', 'Used_for', 'contents'), ('documents', 'Used_for', 'PageRank algorithm'), ('documents', 'Used_for', 'event map')]
trues: [['PageRank algorithm', 'Used_for', 'relevant approach'], ['event map', 'Used_for', 'PageRank algorithm'], ['documents', 'Used_for', 'event map']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.35833333333333334, 0.3006993006993007, 0.3269961977186312]
text: They improve the reconstruction results and enforce their consistency with a priori knowledge about object shape .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
They
reconstruction results

triple_list: [["They", "Used_for", "reconstruction results"]]
pred: [('They', 'Used_for', 'reconstruction results')]
trues: [['They', 'Used_for', 'reconstruction'], ['object shape', 'Feature_of', 'priori knowledge']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35537190082644626, 0.296551724137931, 0.3233082706766917]
text: Furthermore , this paper presents a novel algorithm for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts ( flicker ) ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Used_for
The candidate entities for this text are:
algorithm
temporal maintenance of a background model
temporal maintenance of a background model
rendering of occlusions
temporal maintenance of a background model
reducing temporal artefacts ( flicker )
cost aggregation algorithm
three-dimensional matching cost space

triple_list: [["algorithm", "Used_for", "temporal maintenance of a background model"], ["temporal maintenance of a background model", "Used_for", "rendering of occlusions"], ["temporal maintenance of a background model", "Used_for", "reducing temporal artefacts ( flicker )"], ["cost aggregation algorithm", "Used_for", "three-dimensional matching cost space"]]
pred: [('algorithm', 'Used_for', 'temporal maintenance of a background model'), ('temporal maintenance of a background model', 'Used_for', 'rendering of occlusions'), ('temporal maintenance of a background model', 'Used_for', 'reducing temporal artefacts ( flicker )'), ('cost aggregation algorithm', 'Used_for', 'three-dimensional matching cost space')]
trues: [['algorithm', 'Used_for', 'temporal maintenance of a background model'], ['algorithm', 'Used_for', 'rendering of occlusions'], ['algorithm', 'Used_for', 'temporal artefacts ( flicker )'], ['cost aggregation algorithm', 'Conjunction', 'algorithm'], ['cost aggregation algorithm', 'Used_for', 'three-dimensional matching cost space']]
metrics_sample: [0.5, 0.4, 0.4444444444444445]
metrics_current: [0.36, 0.3, 0.3272727272727273]
text: We evaluate the proposed methods through several transliteration/backtransliteration experiments for English/Chinese and English/Japanese language pairs .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
methods
transliteration/backtransliteration experiments
English/Chinese and English/Japanese language pairs
transliteration/backtransliteration experiments
English/Chinese and English/Japanese language pairs
transliteration/backtransliteration experiments
English
English/Chinese and English/Japanese language pairs
transliteration/backtransliteration experiments
Japanese

triple_list: [["methods", "Used_for", "transliteration/backtransliteration experiments"], ["English/Chinese and English/Japanese language pairs", "Used_for", "transliteration/backtransliteration experiments"], ["English/Chinese and English/Japanese language pairs", "Conjunction", "transliteration/backtransliteration experiments"], ["English", "Used_for", "English/Chinese and English/Japanese language pairs"], ["transliteration/backtransliteration experiments", "Used_for", "Japanese"]]
pred: [('methods', 'Used_for', 'transliteration/backtransliteration experiments'), ('English/Chinese and English/Japanese language pairs', 'Used_for', 'transliteration/backtransliteration experiments'), ('English/Chinese and English/Japanese language pairs', 'Conjunction', 'transliteration/backtransliteration experiments'), ('English', 'Used_for', 'English/Chinese and English/Japanese language pairs'), ('transliteration/backtransliteration experiments', 'Used_for', 'Japanese')]
trues: [['transliteration/backtransliteration', 'Evaluate_for', 'methods'], ['transliteration/backtransliteration', 'Used_for', 'English/Chinese and English/Japanese language pairs']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.34615384615384615, 0.29605263157894735, 0.3191489361702127]
text: We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus ( 100,000 words ) and exploiting a large non-parallel corpus .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
parallel corpus
MT system
non-parallel corpus
MT system

triple_list: [["parallel corpus", "Used_for", "MT system"], ["non-parallel corpus", "Used_for", "MT system"]]
pred: [('parallel corpus', 'Used_for', 'MT system'), ('non-parallel corpus', 'Used_for', 'MT system')]
trues: [['parallel corpus', 'Used_for', 'MT system'], ['parallel corpus', 'Conjunction', 'non-parallel corpus'], ['non-parallel corpus', 'Used_for', 'MT system']]
metrics_sample: [1.0, 0.6666666666666666, 0.8]
metrics_current: [0.3560606060606061, 0.3032258064516129, 0.3275261324041812]
text: Starting from a DP-based solution to the traveling salesman problem , we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an efficient search algorithm .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
DP-based solution
traveling salesman problem
technique
search algorithm

triple_list: [["DP-based solution", "Used_for", "traveling salesman problem"], ["technique", "Used_for", "search algorithm"]]
pred: [('DP-based solution', 'Used_for', 'traveling salesman problem'), ('technique', 'Used_for', 'search algorithm')]
trues: [['technique', 'Used_for', 'search algorithm']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3582089552238806, 0.3076923076923077, 0.3310344827586207]
text: We introduce a novel method of shape constrained image segmentation which is based on mixtures of feature distributions for color and texture as well as probabilistic shape knowledge .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
method
shape constrained image segmentation
feature distributions
method
feature distributions
texture
texture
method
shape knowledge
method

triple_list: [["method", "Used_for", "shape constrained image segmentation"], ["feature distributions", "Used_for", "method"], ["feature distributions", "Conjunction", "texture"], ["texture", "Used_for", "method"], ["shape knowledge", "Used_for", "method"]]
pred: [('method', 'Used_for', 'shape constrained image segmentation'), ('feature distributions', 'Used_for', 'method'), ('feature distributions', 'Conjunction', 'texture'), ('texture', 'Used_for', 'method'), ('shape knowledge', 'Used_for', 'method')]
trues: [['method', 'Used_for', 'shape constrained image segmentation'], ['mixtures of feature distributions', 'Used_for', 'method'], ['mixtures of feature distributions', 'Used_for', 'color'], ['mixtures of feature distributions', 'Used_for', 'texture'], ['mixtures of feature distributions', 'Used_for', 'probabilistic shape knowledge'], ['color', 'Conjunction', 'texture'], ['texture', 'Conjunction', 'probabilistic shape knowledge']]
metrics_sample: [0.2, 0.14285714285714285, 0.16666666666666666]
metrics_current: [0.35251798561151076, 0.3006134969325153, 0.32450331125827814]
text: The system incorporates a decision-tree classifier for 30 scf types which tests for the presence of grammatical relations ( grs ) in the output of a robust statistical parser .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Conjunction
The candidate entities for this text are:
decision-tree classifier
scf types
decision-tree classifier
grammatical relations ( grs )
decision-tree classifier
robust statistical parser
grammatical relations ( grs )
robust statistical parser

triple_list: [["decision-tree classifier", "Used_for", "scf types"], ["decision-tree classifier", "Used_for", "grammatical relations ( grs )"], ["decision-tree classifier", "Used_for", "robust statistical parser"], ["grammatical relations ( grs )", "Conjunction", "robust statistical parser"]]
pred: [('decision-tree classifier', 'Used_for', 'scf types'), ('decision-tree classifier', 'Used_for', 'grammatical relations ( grs )'), ('decision-tree classifier', 'Used_for', 'robust statistical parser'), ('grammatical relations ( grs )', 'Conjunction', 'robust statistical parser')]
trues: [['decision-tree classifier', 'Part_of', 'system'], ['decision-tree classifier', 'Used_for', 'grammatical relations ( grs )']]
metrics_sample: [0.25, 0.5, 0.3333333333333333]
metrics_current: [0.34965034965034963, 0.30303030303030304, 0.3246753246753247]
text: Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of Chinese text available in corpora and on the Web .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
method
word senses
method
Chinese
English
Chinese
English
word senses
Chinese text
method

triple_list: [["method", "Used_for", "word senses"], ["method", "Used_for", "Chinese"], ["English", "Conjunction", "Chinese"], ["English", "Used_for", "word senses"], ["Chinese text", "Used_for", "method"]]
pred: [('method', 'Used_for', 'word senses'), ('method', 'Used_for', 'Chinese'), ('English', 'Conjunction', 'Chinese'), ('English', 'Used_for', 'word senses'), ('Chinese text', 'Used_for', 'method')]
trues: [['Chinese text', 'Part_of', 'corpora'], ['Chinese text', 'Part_of', 'Web'], ['corpora', 'Conjunction', 'Web']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.33783783783783783, 0.2976190476190476, 0.31645569620253167]
text: We present Minimum Bayes-Risk ( MBR ) decoding for statistical machine translation .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Minimum Bayes-Risk ( MBR ) decoding
statistical machine translation

triple_list: [["Minimum Bayes-Risk ( MBR ) decoding", "Used_for", "statistical machine translation"]]
pred: [('Minimum Bayes-Risk ( MBR ) decoding', 'Used_for', 'statistical machine translation')]
trues: [['Minimum Bayes-Risk ( MBR ) decoding', 'Used_for', 'statistical machine translation']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3422818791946309, 0.30177514792899407, 0.320754716981132]
text: A bio-inspired model for an analog programmable array processor ( APAP ) , based on studies on the vertebrate retina , has permitted the realization of complex programmable spatio-temporal dynamics in VLSI .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
bio-inspired model
analog programmable array processor ( APAP )
vertebrate retina
bio-inspired model

triple_list: [["bio-inspired model", "Used_for", "analog programmable array processor ( APAP )"], ["vertebrate retina", "Used_for", "bio-inspired model"]]
pred: [('bio-inspired model', 'Used_for', 'analog programmable array processor ( APAP )'), ('vertebrate retina', 'Used_for', 'bio-inspired model')]
trues: [['bio-inspired model', 'Used_for', 'analog programmable array processor ( APAP )'], ['bio-inspired model', 'Used_for', 'complex programmable spatio-temporal dynamics'], ['vertebrate retina', 'Used_for', 'bio-inspired model'], ['complex programmable spatio-temporal dynamics', 'Feature_of', 'VLSI']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3509933774834437, 0.3063583815028902, 0.3271604938271605]
text: Overall summarization quality of the proposed system is state-of-the-art , with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator .
result: The candidate relations for this text are:
Evaluate_for
The candidate entities for this text are:
grammaticality
system output

triple_list: [["grammaticality", "Evaluate_for", "system output"]]
pred: [('grammaticality', 'Evaluate_for', 'system output')]
trues: [['summarization quality', 'Evaluate_for', 'system'], ['grammaticality', 'Evaluate_for', 'system'], ['constraint-based parser/generator', 'Used_for', 'system']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.34868421052631576, 0.30113636363636365, 0.3231707317073171]
text: We propose a novel metric learning formulation called Weighted Approximate Rank Component Analysis ( WARCA ) .
result: The candidate relations for this text are:
Hyponym_of
The candidate entities for this text are:
Weighted Approximate Rank Component Analysis ( WARCA )
metric learning formulation

triple_list: [["Weighted Approximate Rank Component Analysis ( WARCA )", "Hyponym_of", "metric learning formulation"]]
pred: [('Weighted Approximate Rank Component Analysis ( WARCA )', 'Hyponym_of', 'metric learning formulation')]
trues: [['Weighted Approximate Rank Component Analysis ( WARCA )', 'Hyponym_of', 'metric learning formulation']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35294117647058826, 0.3050847457627119, 0.3272727272727273]
text: We show that various features based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for question-answer pairing .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
features
question-answer pairing
structure of email-threads
features

triple_list: [["features", "Used_for", "question-answer pairing"], ["structure of email-threads", "Used_for", "features"]]
pred: [('features', 'Used_for', 'question-answer pairing'), ('structure of email-threads', 'Used_for', 'features')]
trues: [['features', 'Used_for', 'lexical similarity'], ['features', 'Used_for', 'question-answer pairing'], ['structure of email-threads', 'Used_for', 'features'], ['lexical similarity', 'Feature_of', 'discourse segments']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.36129032258064514, 0.30939226519337015, 0.33333333333333337]
text: Multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide matching and reconstruction , allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Used_for
Conjunction
The candidate entities for this text are:
Multi-view constraints
matching and reconstruction
normalized representation of their appearance
matching and reconstruction
matching and reconstruction
three-dimensional affine and Euclidean models
images
three-dimensional affine and Euclidean models
recognition
three-dimensional affine and Euclidean models

triple_list: [["Multi-view constraints", "Used_for", "matching and reconstruction"], ["normalized representation of their appearance", "Used_for", "matching and reconstruction"], ["matching and reconstruction", "Used_for", "three-dimensional affine and Euclidean models"], ["images", "Used_for", "three-dimensional affine and Euclidean models"], ["recognition", "Conjunction", "three-dimensional affine and Euclidean models"]]
pred: [('Multi-view constraints', 'Used_for', 'matching and reconstruction'), ('normalized representation of their appearance', 'Used_for', 'matching and reconstruction'), ('matching and reconstruction', 'Used_for', 'three-dimensional affine and Euclidean models'), ('images', 'Used_for', 'three-dimensional affine and Euclidean models'), ('recognition', 'Conjunction', 'three-dimensional affine and Euclidean models')]
trues: [['Multi-view constraints', 'Conjunction', 'normalized representation'], ['Multi-view constraints', 'Used_for', 'matching'], ['Multi-view constraints', 'Used_for', 'reconstruction'], ['normalized representation', 'Used_for', 'matching'], ['normalized representation', 'Used_for', 'reconstruction'], ['matching', 'Conjunction', 'reconstruction'], ['images', 'Used_for', 'acquisition of true three-dimensional affine and Euclidean models']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35, 0.2978723404255319, 0.3218390804597701]
text: We consider the problem of computing the Kullback-Leibler distance , also called the relative entropy , between a probabilistic context-free grammar and a probabilistic finite automaton .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
probabilistic context-free grammar
probabilistic finite automaton

triple_list: [["probabilistic context-free grammar", "Compare", "probabilistic finite automaton"]]
pred: [('probabilistic context-free grammar', 'Compare', 'probabilistic finite automaton')]
trues: [['probabilistic context-free grammar', 'Compare', 'probabilistic finite automaton']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35403726708074534, 0.30158730158730157, 0.32571428571428573]
text: This work presents an automated technique for detecting reflections in image sequences by analyzing motion trajectories of feature points .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
automated technique
reflections
motion trajectories of feature points
reflections

triple_list: [["automated technique", "Used_for", "reflections"], ["motion trajectories of feature points", "Used_for", "reflections"]]
pred: [('automated technique', 'Used_for', 'reflections'), ('motion trajectories of feature points', 'Used_for', 'reflections')]
trues: [['technique', 'Used_for', 'detecting reflections in image sequences'], ['motion trajectories', 'Used_for', 'technique'], ['feature points', 'Feature_of', 'motion trajectories']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3496932515337423, 0.296875, 0.32112676056338024]
text: We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word-to-word alignments from an MT system , and syntactic structure from parse-trees of source and target language sentences .
result: The candidate relations for this text are:
Used_for
Conjunction
Used_for
Conjunction
Used_for
The candidate entities for this text are:
word strings
loss functions
word strings
word-to-word alignments
word-to-word alignments
loss functions
word-to-word alignments
syntactic structure
syntactic structure
loss functions

triple_list: [["word strings", "Used_for", "loss functions"], ["word strings", "Conjunction", "word-to-word alignments"], ["word-to-word alignments", "Used_for", "loss functions"], ["word-to-word alignments", "Conjunction", "syntactic structure"], ["syntactic structure", "Used_for", "loss functions"]]
pred: [('word strings', 'Used_for', 'loss functions'), ('word strings', 'Conjunction', 'word-to-word alignments'), ('word-to-word alignments', 'Used_for', 'loss functions'), ('word-to-word alignments', 'Conjunction', 'syntactic structure'), ('syntactic structure', 'Used_for', 'loss functions')]
trues: [['linguistic information', 'Used_for', 'loss functions'], ['word-to-word alignments', 'Used_for', 'loss functions'], ['word-to-word alignments', 'Part_of', 'MT system'], ['syntactic structure', 'Used_for', 'loss functions'], ['parse-trees', 'Part_of', 'syntactic structure']]
metrics_sample: [0.4, 0.4, 0.4000000000000001]
metrics_current: [0.35119047619047616, 0.29949238578680204, 0.3232876712328767]
text: Memo-functions also facilitate a simple way to construct a very compact representation of the parse forest .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Memo-functions
parse forest

triple_list: [["Memo-functions", "Used_for", "parse forest"]]
pred: [('Memo-functions', 'Used_for', 'parse forest')]
trues: [['Memo-functions', 'Used_for', 'parse forest']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.35502958579881655, 0.30303030303030304, 0.32697547683923706]
text: Towards the modeling of the dynamic characteristics , optical flow is computed and utilized as a feature in a higher dimensional space .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
optical flow
dynamic characteristics
optical flow
feature

triple_list: [["optical flow", "Used_for", "dynamic characteristics"], ["optical flow", "Used_for", "feature"]]
pred: [('optical flow', 'Used_for', 'dynamic characteristics'), ('optical flow', 'Used_for', 'feature')]
trues: [['optical flow', 'Used_for', 'modeling of the dynamic characteristics'], ['optical flow', 'Used_for', 'feature'], ['feature', 'Used_for', 'modeling of the dynamic characteristics'], ['higher dimensional space', 'Feature_of', 'feature']]
metrics_sample: [0.5, 0.25, 0.3333333333333333]
metrics_current: [0.3567251461988304, 0.30198019801980197, 0.32707774798927614]
text: The system is trained on 181,000 expert games and shows excellent prediction performance as indicated by its ability to perfectly predict the moves made by professional Go players in 34 % of test positions .
result: The candidate relations for this text are:
Used_for
Evaluate_for
The candidate entities for this text are:
system
prediction performance
prediction performance
moves

triple_list: [["system", "Used_for", "prediction performance"], ["prediction performance", "Evaluate_for", "moves"]]
pred: [('system', 'Used_for', 'prediction performance'), ('prediction performance', 'Evaluate_for', 'moves')]
trues: [['expert games', 'Used_for', 'system']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.35260115606936415, 0.30049261083743845, 0.324468085106383]
text: Unlike existing interest point detectors , which measure pixel-wise differences in image intensity , our detectors incorporate histogram-based representations , and thus can find image regions that present a distinct distribution in the neighborhood .
result: The candidate relations for this text are:
Compare
Used_for
Used_for
The candidate entities for this text are:
interest point detectors
detectors
pixel-wise differences in image intensity
interest point detectors
histogram-based representations
detectors

triple_list: [["interest point detectors", "Compare", "detectors"], ["pixel-wise differences in image intensity", "Used_for", "interest point detectors"], ["histogram-based representations", "Used_for", "detectors"]]
pred: [('interest point detectors', 'Compare', 'detectors'), ('pixel-wise differences in image intensity', 'Used_for', 'interest point detectors'), ('histogram-based representations', 'Used_for', 'detectors')]
trues: [['pixel-wise differences in image intensity', 'Evaluate_for', 'interest point detectors'], ['histogram-based representations', 'Part_of', 'detectors']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3465909090909091, 0.2975609756097561, 0.32020997375328086]
text: We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representations .
result: The candidate relations for this text are:
Used_for
Compare
The candidate entities for this text are:
layers
distributed representations
layers
object pose information

triple_list: [["layers", "Used_for", "distributed representations"], ["layers", "Compare", "object pose information"]]
pred: [('layers', 'Used_for', 'distributed representations'), ('layers', 'Compare', 'object pose information')]
trues: [['layers', 'Part_of', 'CNN models'], ['layers of distributed representations', 'Part_of', 'CNNs'], ['layers of distributed representations', 'Used_for', 'object pose information'], ['this', 'Compare', 'object category representations']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.34269662921348315, 0.291866028708134, 0.31524547803617575]
text: We validate this new method on nine standard person re-identification datasets including two large scale Market-1501 and CUHK03 datasets and show that we improve upon the current state-of-the-art methods on all of them .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
Evaluate_for
Compare
The candidate entities for this text are:
nine standard person re-identification datasets
method
Market-1501 and CUHK03 datasets
nine standard person re-identification datasets
Market-1501 and CUHK03 datasets
state-of-the-art methods
Market-1501 and CUHK03 datasets
state-of-the-art methods

triple_list: [["nine standard person re-identification datasets", "Evaluate_for", "method"], ["Market-1501 and CUHK03 datasets", "Evaluate_for", "nine standard person re-identification datasets"], ["Market-1501 and CUHK03 datasets", "Evaluate_for", "state-of-the-art methods"], ["Market-1501 and CUHK03 datasets", "Compare", "state-of-the-art methods"]]
pred: [('nine standard person re-identification datasets', 'Evaluate_for', 'method'), ('Market-1501 and CUHK03 datasets', 'Evaluate_for', 'nine standard person re-identification datasets'), ('Market-1501 and CUHK03 datasets', 'Evaluate_for', 'state-of-the-art methods'), ('Market-1501 and CUHK03 datasets', 'Compare', 'state-of-the-art methods')]
trues: [['person re-identification datasets', 'Evaluate_for', 'method'], ['scale Market-1501', 'Hyponym_of', 'person re-identification datasets'], ['CUHK03 datasets', 'Hyponym_of', 'person re-identification datasets'], ['CUHK03 datasets', 'Conjunction', 'scale Market-1501']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.33516483516483514, 0.2863849765258216, 0.30886075949367087]
text: Given a new image , detection and classification are achieved by determining the position and viewpoint of the model that maximize recognition scores of the candidate objects .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
image
detection and classification
recognition scores
candidate objects

triple_list: [["image", "Used_for", "detection and classification"], ["recognition scores", "Used_for", "candidate objects"]]
pred: [('image', 'Used_for', 'detection and classification'), ('recognition scores', 'Used_for', 'candidate objects')]
trues: [['image', 'Used_for', 'detection'], ['image', 'Used_for', 'classification'], ['detection', 'Conjunction', 'classification'], ['position', 'Used_for', 'detection'], ['position', 'Used_for', 'classification'], ['position', 'Conjunction', 'viewpoint'], ['viewpoint', 'Used_for', 'detection'], ['viewpoint', 'Used_for', 'classification']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.33152173913043476, 0.27601809954751133, 0.3012345679012346]
text: We provide a principled and provable solution for this problem ; it is parameterless and requires no data insights .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
data insights
it

triple_list: [["data insights", "Used_for", "it"]]
pred: [('data insights', 'Used_for', 'it')]
trues: [['principled and provable solution', 'Used_for', 'problem']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32972972972972975, 0.2747747747747748, 0.2997542997542998]
text: Relaxations of these properties expose some of the interesting ( and unavoidable ) trade-offs at work in well-studied clustering techniques such as single-linkage , sum-of-pairs , k-means , and k-median .
result: The candidate relations for this text are:
Hyponym_of
Conjunction
Hyponym_of
Conjunction
Hyponym_of
Conjunction
Hyponym_of
The candidate entities for this text are:
single-linkage
clustering techniques
single-linkage
sum-of-pairs
sum-of-pairs
clustering techniques
sum-of-pairs
k-means
k-means
clustering techniques
k-means
k-median
k-median
clustering techniques

triple_list: [["single-linkage", "Hyponym_of", "clustering techniques"], ["single-linkage", "Conjunction", "sum-of-pairs"], ["sum-of-pairs", "Hyponym_of", "clustering techniques"], ["sum-of-pairs", "Conjunction", "k-means"], ["k-means", "Hyponym_of", "clustering techniques"], ["k-means", "Conjunction", "k-median"], ["k-median", "Hyponym_of", "clustering techniques"]]
pred: [('single-linkage', 'Hyponym_of', 'clustering techniques'), ('single-linkage', 'Conjunction', 'sum-of-pairs'), ('sum-of-pairs', 'Hyponym_of', 'clustering techniques'), ('sum-of-pairs', 'Conjunction', 'k-means'), ('k-means', 'Hyponym_of', 'clustering techniques'), ('k-means', 'Conjunction', 'k-median'), ('k-median', 'Hyponym_of', 'clustering techniques')]
trues: [['single-linkage', 'Hyponym_of', 'well-studied clustering techniques'], ['single-linkage', 'Conjunction', 'sum-of-pairs'], ['sum-of-pairs', 'Hyponym_of', 'well-studied clustering techniques'], ['sum-of-pairs', 'Conjunction', 'k-means'], ['k-means', 'Hyponym_of', 'well-studied clustering techniques'], ['k-means', 'Conjunction', 'k-median'], ['k-median', 'Hyponym_of', 'well-studied clustering techniques']]
metrics_sample: [0.42857142857142855, 0.42857142857142855, 0.42857142857142855]
metrics_current: [0.3333333333333333, 0.2794759825327511, 0.30403800475059384]
text: Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield significantly better results when compared with state-of-the-art .
result: The candidate relations for this text are:
Evaluate_for
Compare
Evaluate_for
The candidate entities for this text are:
SemCor and Senseval-3 data sets
ensembles
ensembles
state-of-the-art
SemCor and Senseval-3 data sets
ensembles

triple_list: [["SemCor and Senseval-3 data sets", "Evaluate_for", "ensembles"], ["ensembles", "Compare", "state-of-the-art"], ["SemCor and Senseval-3 data sets", "Evaluate_for", "ensembles"]]
pred: [('SemCor and Senseval-3 data sets', 'Evaluate_for', 'ensembles'), ('ensembles', 'Compare', 'state-of-the-art'), ('SemCor and Senseval-3 data sets', 'Evaluate_for', 'ensembles')]
trues: [['SemCor and Senseval-3 data sets', 'Evaluate_for', 'ensembles'], ['SemCor and Senseval-3 data sets', 'Evaluate_for', 'state-of-the-art']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.33505154639175255, 0.2813852813852814, 0.30588235294117644]
text: It works by calculating eigenvectors of an adjacency graph 's Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
eigenvectors
submanifold of data
eigenvectors
cluster number estimation
Laplacian
eigenvectors

triple_list: [["eigenvectors", "Used_for", "submanifold of data"], ["eigenvectors", "Used_for", "cluster number estimation"], ["Laplacian", "Used_for", "eigenvectors"]]
pred: [('eigenvectors', 'Used_for', 'submanifold of data'), ('eigenvectors', 'Used_for', 'cluster number estimation'), ('Laplacian', 'Used_for', 'eigenvectors')]
trues: [['It', 'Used_for', 'submanifold'], ['eigenvectors', 'Used_for', 'It'], ["adjacency graph 's Laplacian", 'Feature_of', 'eigenvectors'], ['high dimensionality space', 'Used_for', 'submanifold'], ['cluster number estimation', 'Used_for', 'It'], ['cluster number estimation', 'Used_for', 'eigenvectors']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3299492385786802, 0.2742616033755274, 0.2995391705069124]
text: We evaluated the topic signatures on a WSD task , where we trained a second-order vector cooccurrence algorithm on standard WSD datasets , with promising results .
result: The candidate relations for this text are:
Used_for
Evaluate_for
Used_for
The candidate entities for this text are:
WSD task
topic signatures
WSD datasets
second-order vector cooccurrence algorithm
second-order vector cooccurrence algorithm
WSD task

triple_list: [["WSD task", "Used_for", "topic signatures"], ["WSD datasets", "Evaluate_for", "second-order vector cooccurrence algorithm"], ["second-order vector cooccurrence algorithm", "Used_for", "WSD task"]]
pred: [('WSD task', 'Used_for', 'topic signatures'), ('WSD datasets', 'Evaluate_for', 'second-order vector cooccurrence algorithm'), ('second-order vector cooccurrence algorithm', 'Used_for', 'WSD task')]
trues: [['WSD task', 'Evaluate_for', 'topic signatures'], ['WSD datasets', 'Used_for', 'second-order vector cooccurrence algorithm']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.325, 0.2719665271966527, 0.29612756264236906]
text: Thus the ASR system is designed to deal with what we refer to as Word Spotting and Noise Spotting capabilities .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
ASR system
Word Spotting and Noise Spotting capabilities

triple_list: [["ASR system", "Used_for", "Word Spotting and Noise Spotting capabilities"]]
pred: [('ASR system', 'Used_for', 'Word Spotting and Noise Spotting capabilities')]
trues: [['ASR system', 'Used_for', 'Word Spotting'], ['ASR system', 'Used_for', 'Noise Spotting capabilities']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32338308457711445, 0.2697095435684647, 0.29411764705882354]
text: The problem of blind separation of underdetermined instantaneous mixtures of independent signals is addressed through a method relying on nonstationarity of the original signals .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
nonstationarity of the original signals
blind separation of underdetermined instantaneous mixtures of independent signals

triple_list: [["nonstationarity of the original signals", "Used_for", "blind separation of underdetermined instantaneous mixtures of independent signals"]]
pred: [('nonstationarity of the original signals', 'Used_for', 'blind separation of underdetermined instantaneous mixtures of independent signals')]
trues: [['method', 'Used_for', 'blind separation of underdetermined instantaneous mixtures of independent signals'], ['nonstationarity', 'Used_for', 'method']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3217821782178218, 0.2674897119341564, 0.29213483146067415]
text: Towards this goal , we also propose a novel Multiple Instance Learning ( MIL ) formulation for the problem based on the logistic softmax function of covariance-based region features within a MAP estimation framework .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
logistic softmax function
Multiple Instance Learning ( MIL ) formulation
covariance-based region features
logistic softmax function
MAP estimation framework
Multiple Instance Learning ( MIL ) formulation

triple_list: [["logistic softmax function", "Used_for", "Multiple Instance Learning ( MIL ) formulation"], ["covariance-based region features", "Used_for", "logistic softmax function"], ["MAP estimation framework", "Used_for", "Multiple Instance Learning ( MIL ) formulation"]]
pred: [('logistic softmax function', 'Used_for', 'Multiple Instance Learning ( MIL ) formulation'), ('covariance-based region features', 'Used_for', 'logistic softmax function'), ('MAP estimation framework', 'Used_for', 'Multiple Instance Learning ( MIL ) formulation')]
trues: [['logistic softmax function of covariance-based region features', 'Used_for', 'Multiple Instance Learning ( MIL ) formulation'], ['MAP estimation framework', 'Used_for', 'Multiple Instance Learning ( MIL ) formulation']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.32195121951219513, 0.2693877551020408, 0.29333333333333333]
text: The goal is to sample a small set of records over which evaluating aggregate queries can be done both efficiently and accurately .
result: The candidate relations for this text are:
Evaluate_for
The candidate entities for this text are:
records
aggregate queries

triple_list: [["records", "Evaluate_for", "aggregate queries"]]
pred: [('records', 'Evaluate_for', 'aggregate queries')]
trues: [['aggregate queries', 'Evaluate_for', 'records']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32038834951456313, 0.2682926829268293, 0.2920353982300885]
text: Our combination methods rely on predominant senses which are derived automatically from raw text .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
combination methods
predominant senses
predominant senses
raw text

triple_list: [["combination methods", "Used_for", "predominant senses"], ["predominant senses", "Used_for", "raw text"]]
pred: [('combination methods', 'Used_for', 'predominant senses'), ('predominant senses', 'Used_for', 'raw text')]
trues: [['predominant senses', 'Used_for', 'combination methods'], ['raw text', 'Used_for', 'predominant senses']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3173076923076923, 0.2661290322580645, 0.2894736842105263]
text: This paper solves a specialized regression problem to obtain sampling probabilities for records in databases .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
sampling probabilities
regression problem

triple_list: [["sampling probabilities", "Used_for", "regression problem"]]
pred: [('sampling probabilities', 'Used_for', 'regression problem')]
trues: [['specialized regression problem', 'Used_for', 'sampling probabilities'], ['sampling probabilities', 'Used_for', 'records'], ['records', 'Part_of', 'databases']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3157894736842105, 0.26294820717131473, 0.28695652173913044]
text: We show that combining anisotropic meshing with a non-quadratic approach to regularization enables us to obtain satisfactory reconstruction results using triangulations with few vertices .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
anisotropic meshing
reconstruction
non-quadratic approach to regularization
reconstruction

triple_list: [["anisotropic meshing", "Used_for", "reconstruction"], ["non-quadratic approach to regularization", "Used_for", "reconstruction"]]
pred: [('anisotropic meshing', 'Used_for', 'reconstruction'), ('non-quadratic approach to regularization', 'Used_for', 'reconstruction')]
trues: [['anisotropic meshing', 'Conjunction', 'non-quadratic approach'], ['anisotropic meshing', 'Used_for', 'reconstruction'], ['non-quadratic approach', 'Used_for', 'regularization'], ['non-quadratic approach', 'Used_for', 'reconstruction'], ['triangulations', 'Used_for', 'reconstruction']]
metrics_sample: [0.5, 0.2, 0.28571428571428575]
metrics_current: [0.3175355450236967, 0.26171875, 0.2869379014989293]
text: In addition , we conduct synthetic experiments as well as a real application on noisy image classification to show that our method also improves the performance in practice by exploiting side information .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
synthetic experiments
method
real application
method
noisy image classification
method

triple_list: [["synthetic experiments", "Used_for", "method"], ["real application", "Used_for", "method"], ["noisy image classification", "Used_for", "method"]]
pred: [('synthetic experiments', 'Used_for', 'method'), ('real application', 'Used_for', 'method'), ('noisy image classification', 'Used_for', 'method')]
trues: [['noisy image classification', 'Evaluate_for', 'method'], ['side information', 'Used_for', 'method']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3130841121495327, 0.2596899224806202, 0.28389830508474573]
text: Topical blog post retrieval is the task of ranking blog posts with respect to their relevance for a given topic .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
blog posts
Topical blog post retrieval

triple_list: [["blog posts", "Used_for", "Topical blog post retrieval"]]
pred: [('blog posts', 'Used_for', 'Topical blog post retrieval')]
trues: [['Topical blog post retrieval', 'Hyponym_of', 'ranking blog posts'], ['relevance', 'Feature_of', 'blog posts']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3116279069767442, 0.25769230769230766, 0.2821052631578947]
text: The seven categories of the scheme are based on rhetorical moves of argumentation .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
rhetorical moves of argumentation
scheme

triple_list: [["rhetorical moves of argumentation", "Used_for", "scheme"]]
pred: [('rhetorical moves of argumentation', 'Used_for', 'scheme')]
trues: [['rhetorical moves of argumentation', 'Used_for', 'scheme']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3148148148148148, 0.26053639846743293, 0.2851153039832285]
text: Our approach outperforms state-of-the-art trackers on the VIVID benchmark datasets .
result: The candidate relations for this text are:
Evaluate_for
Compare
Evaluate_for
The candidate entities for this text are:
VIVID benchmark datasets
approach
VIVID benchmark datasets
trackers
trackers
state-of-the-art trackers

triple_list: [["VIVID benchmark datasets", "Evaluate_for", "approach"], ["VIVID benchmark datasets", "Compare", "trackers"], ["trackers", "Evaluate_for", "state-of-the-art trackers"]]
pred: [('VIVID benchmark datasets', 'Evaluate_for', 'approach'), ('VIVID benchmark datasets', 'Compare', 'trackers'), ('trackers', 'Evaluate_for', 'state-of-the-art trackers')]
trues: [['state-of-the-art trackers', 'Compare', 'approach'], ['VIVID benchmark datasets', 'Evaluate_for', 'approach'], ['VIVID benchmark datasets', 'Evaluate_for', 'state-of-the-art trackers']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.3150684931506849, 0.26136363636363635, 0.2857142857142857]
text: This paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , dags , graphs , and products of them .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
mathematical formalism
combination of various structures

triple_list: [["mathematical formalism", "Used_for", "combination of various structures"]]
pred: [('mathematical formalism', 'Used_for', 'combination of various structures')]
trues: [['strings', 'Hyponym_of', 'structures'], ['strings', 'Conjunction', 'trees'], ['trees', 'Hyponym_of', 'structures'], ['trees', 'Conjunction', 'dags'], ['dags', 'Hyponym_of', 'structures'], ['dags', 'Conjunction', 'graphs'], ['graphs', 'Hyponym_of', 'structures']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31363636363636366, 0.25461254612546125, 0.28105906313645623]
text: We have previously introduced a statistical model describing the generating process of speech F0 contours , based on the discrete-time version of the Fujisaki model .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
statistical model
speech F0 contours
discrete-time version of the Fujisaki model
statistical model

triple_list: [["statistical model", "Used_for", "speech F0 contours"], ["discrete-time version of the Fujisaki model", "Used_for", "statistical model"]]
pred: [('statistical model', 'Used_for', 'speech F0 contours'), ('discrete-time version of the Fujisaki model', 'Used_for', 'statistical model')]
trues: [['statistical model', 'Used_for', 'speech F0 contours'], ['Fujisaki model', 'Used_for', 'statistical model']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3153153153153153, 0.2564102564102564, 0.2828282828282828]
text: Subpixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
unstructured patterns
zero-crossings

triple_list: [["unstructured patterns", "Used_for", "zero-crossings"]]
pred: [('unstructured patterns', 'Used_for', 'zero-crossings')]
trues: [['zero-crossings', 'Used_for', 'Subpixel accuracy']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31390134529147984, 0.25547445255474455, 0.28169014084507044]
text: Related work applies MINPRAN to complex range and intensity data 23 ] .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
MINPRAN
complex range and intensity data

triple_list: [["MINPRAN", "Used_for", "complex range and intensity data"]]
pred: [('MINPRAN', 'Used_for', 'complex range and intensity data')]
trues: [['MINPRAN', 'Used_for', 'complex range'], ['MINPRAN', 'Used_for', 'intensity data']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3125, 0.2536231884057971, 0.28]
text: However , when the object becomes partially or totally occluded , such local tracking is prone to failure , especially when common prediction techniques like the Kalman filter do not provide a good estimate of object parameters in future frames .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
Kalman filter
object parameters
Kalman filter
prediction techniques

triple_list: [["Kalman filter", "Used_for", "object parameters"], ["Kalman filter", "Used_for", "prediction techniques"]]
pred: [('Kalman filter', 'Used_for', 'object parameters'), ('Kalman filter', 'Used_for', 'prediction techniques')]
trues: [['Kalman filter', 'Part_of', 'prediction techniques']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30973451327433627, 0.2527075812274368, 0.2783300198807157]
text: We further propose a parameter training algorithm for the present model based on a decision tree-based context clustering .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
decision tree-based context clustering
parameter training algorithm
decision tree-based context clustering
model

triple_list: [["decision tree-based context clustering", "Used_for", "parameter training algorithm"], ["decision tree-based context clustering", "Used_for", "model"]]
pred: [('decision tree-based context clustering', 'Used_for', 'parameter training algorithm'), ('decision tree-based context clustering', 'Used_for', 'model')]
trues: [['parameter training algorithm', 'Used_for', 'model'], ['decision tree-based context clustering', 'Used_for', 'parameter training algorithm']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.31140350877192985, 0.25448028673835127, 0.28007889546351084]
text: Experimental results from a real telephone application on a natural number recognition task show an 50 % reduction in recognition errors with a moderate 12 % rejection rate of correct utterances and a low 1.5 % rate of false acceptance .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
Evaluate_for
The candidate entities for this text are:
recognition errors
natural number recognition task
recognition errors
telephone application
rejection rate
natural number recognition task

triple_list: [["recognition errors", "Evaluate_for", "natural number recognition task"], ["recognition errors", "Evaluate_for", "telephone application"], ["rejection rate", "Evaluate_for", "natural number recognition task"]]
pred: [('recognition errors', 'Evaluate_for', 'natural number recognition task'), ('recognition errors', 'Evaluate_for', 'telephone application'), ('rejection rate', 'Evaluate_for', 'natural number recognition task')]
trues: [['natural number recognition task', 'Feature_of', 'telephone application'], ['recognition errors', 'Evaluate_for', 'natural number recognition task']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.3116883116883117, 0.25622775800711745, 0.28125]
text: We propose a novel step toward the unsupervised seg-mentation of whole objects by combining '' hints '' of partial scene segmentation offered by multiple soft , binary mattes .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
unsupervised seg-mentation
whole objects
'' hints ''
unsupervised seg-mentation
'' hints ''
partial scene segmentation
soft , binary mattes
'' hints ''

triple_list: [["unsupervised seg-mentation", "Used_for", "whole objects"], ["'' hints ''", "Used_for", "unsupervised seg-mentation"], ["'' hints ''", "Conjunction", "partial scene segmentation"], ["soft , binary mattes", "Used_for", "'' hints ''"]]
pred: [('unsupervised seg-mentation', 'Used_for', 'whole objects'), ("'' hints ''", 'Used_for', 'unsupervised seg-mentation'), ("'' hints ''", 'Conjunction', 'partial scene segmentation'), ('soft , binary mattes', 'Used_for', "'' hints ''")]
trues: [['partial scene segmentation', 'Used_for', 'unsupervised seg-mentation of whole objects'], ['soft , binary mattes', 'Used_for', 'partial scene segmentation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30638297872340425, 0.254416961130742, 0.27799227799227794]
text: During normal tracking conditions when the object is visible from frame to frame , local optimization is used to track the local mode of the similarity measure in a parameter space of translation , rotation and scale .
result: The candidate relations for this text are:
Used_for
Used_for
Feature_of
Conjunction
Feature_of
Conjunction
Feature_of
The candidate entities for this text are:
local optimization
similarity measure
local optimization
tracking
translation
parameter space
translation
rotation
rotation
parameter space
rotation
scale
scale
parameter space

triple_list: [["local optimization", "Used_for", "similarity measure"], ["local optimization", "Used_for", "tracking"], ["translation", "Feature_of", "parameter space"], ["translation", "Conjunction", "rotation"], ["rotation", "Feature_of", "parameter space"], ["rotation", "Conjunction", "scale"], ["scale", "Feature_of", "parameter space"]]
pred: [('local optimization', 'Used_for', 'similarity measure'), ('local optimization', 'Used_for', 'tracking'), ('translation', 'Feature_of', 'parameter space'), ('translation', 'Conjunction', 'rotation'), ('rotation', 'Feature_of', 'parameter space'), ('rotation', 'Conjunction', 'scale'), ('scale', 'Feature_of', 'parameter space')]
trues: [['local optimization', 'Used_for', 'local mode of the similarity measure'], ['parameter space of translation , rotation and scale', 'Used_for', 'local mode of the similarity measure']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2975206611570248, 0.25263157894736843, 0.2732447817836812]
text: Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB 's binding theory , while those for text-level anaphora incorporate an adapted version of a Grosz-Sidner-style focus model .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
sentence boundaries
anaphora resolution
binding theory
anaphora resolution
binding theory
GB 's
GB 's
anaphora resolution
focus model
text-level anaphora
Grosz-Sidner-style focus model
text-level anaphora

triple_list: [["sentence boundaries", "Used_for", "anaphora resolution"], ["binding theory", "Used_for", "anaphora resolution"], ["binding theory", "Used_for", "GB 's"], ["GB 's", "Conjunction", "anaphora resolution"], ["focus model", "Used_for", "text-level anaphora"], ["Grosz-Sidner-style focus model", "Used_for", "text-level anaphora"]]
pred: [('sentence boundaries', 'Used_for', 'anaphora resolution'), ('binding theory', 'Used_for', 'anaphora resolution'), ('binding theory', 'Used_for', "GB 's"), ("GB 's", 'Conjunction', 'anaphora resolution'), ('focus model', 'Used_for', 'text-level anaphora'), ('Grosz-Sidner-style focus model', 'Used_for', 'text-level anaphora')]
trues: [['Criteria', 'Used_for', 'anaphora resolution within sentence boundaries'], ["GB 's binding theory", 'Used_for', 'Criteria'], ['those', 'Used_for', 'text-level anaphora'], ['Grosz-Sidner-style focus model', 'Part_of', 'those']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2903225806451613, 0.2491349480968858, 0.2681564245810056]
text: Metagrammatical formalisms that combine context-free phrase structure rules and metarules ( MPS grammars ) allow concise statement of generalizations about the syntax of natural languages .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
The candidate entities for this text are:
context-free phrase structure rules
Metagrammatical formalisms
metarules
Metagrammatical formalisms
context-free phrase structure rules
metarules

triple_list: [["context-free phrase structure rules", "Used_for", "Metagrammatical formalisms"], ["metarules", "Used_for", "Metagrammatical formalisms"], ["context-free phrase structure rules", "Conjunction", "metarules"]]
pred: [('context-free phrase structure rules', 'Used_for', 'Metagrammatical formalisms'), ('metarules', 'Used_for', 'Metagrammatical formalisms'), ('context-free phrase structure rules', 'Conjunction', 'metarules')]
trues: [['context-free phrase structure rules', 'Part_of', 'Metagrammatical formalisms'], ['context-free phrase structure rules', 'Conjunction', 'metarules ( MPS grammars )'], ['metarules ( MPS grammars )', 'Part_of', 'Metagrammatical formalisms']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2868525896414343, 0.2465753424657534, 0.2651933701657458]
text: The description has been implemented using the PC-KIMMO environment ( Antworth , 1990 ) and is based on a root word lexicon of about 23,000 roots words .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
PC-KIMMO environment
description
root word lexicon
description

triple_list: [["PC-KIMMO environment", "Used_for", "description"], ["root word lexicon", "Used_for", "description"]]
pred: [('PC-KIMMO environment', 'Used_for', 'description'), ('root word lexicon', 'Used_for', 'description')]
trues: [['PC-KIMMO environment', 'Used_for', 'description'], ['root word lexicon', 'Used_for', 'description']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.2924901185770751, 0.25170068027210885, 0.27056672760511885]
text: Under this framework , a joint source-channel transliteration model , also called n-gram transliteration model ( n-gram TM ) , is further proposed to model the transliteration process .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
joint source-channel transliteration model
transliteration process

triple_list: [["joint source-channel transliteration model", "Used_for", "transliteration process"]]
pred: [('joint source-channel transliteration model', 'Used_for', 'transliteration process')]
trues: [['framework', 'Used_for', 'joint source-channel transliteration model'], ['n-gram transliteration model ( n-gram TM )', 'Used_for', 'transliteration process']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.29133858267716534, 0.25, 0.2690909090909091]
text: Our extensive experimental results significantly improve over both uniform sampling and standard stratified sampling which are de-facto the industry standards .
result: The candidate relations for this text are:
Compare
Evaluate_for
Evaluate_for
Evaluate_for
The candidate entities for this text are:
uniform sampling
stratified sampling
uniform sampling
sampling
stratified sampling
sampling
uniform sampling
industry standards

triple_list: [["uniform sampling", "Compare", "stratified sampling"], ["uniform sampling", "Evaluate_for", "sampling"], ["stratified sampling", "Evaluate_for", "sampling"], ["uniform sampling", "Evaluate_for", "industry standards"]]
pred: [('uniform sampling', 'Compare', 'stratified sampling'), ('uniform sampling', 'Evaluate_for', 'sampling'), ('stratified sampling', 'Evaluate_for', 'sampling'), ('uniform sampling', 'Evaluate_for', 'industry standards')]
trues: [['uniform sampling', 'Conjunction', 'stratified sampling']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2868217054263566, 0.24915824915824916, 0.26666666666666666]
text: Branch and bound strategies have previously attempted to curb this complexity whilst maintaining global optimality .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Branch and bound strategies
complexity

triple_list: [["Branch and bound strategies", "Used_for", "complexity"]]
pred: [('Branch and bound strategies', 'Used_for', 'complexity')]
trues: [['Branch and bound strategies', 'Used_for', 'complexity'], ['global optimality', 'Feature_of', 'Branch and bound strategies']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.28957528957528955, 0.2508361204013378, 0.26881720430107525]
text: The formalism 's intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary , we intend it to allow TAGs to be used beyond their role in syntax proper .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
formalism
expressions of natural languages
logical form language
semantics
natural language
translations

triple_list: [["formalism", "Used_for", "expressions of natural languages"], ["logical form language", "Used_for", "semantics"], ["natural language", "Used_for", "translations"]]
pred: [('formalism', 'Used_for', 'expressions of natural languages'), ('logical form language', 'Used_for', 'semantics'), ('natural language', 'Used_for', 'translations')]
trues: [['logical form language', 'Used_for', 'semantics'], ['TAGs', 'Used_for', 'syntax proper']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.2900763358778626, 0.25249169435215946, 0.2699822380106572]
text: Using these ideas together , the resulting tagger gives a 97.24 % accuracy on the Penn Treebank WSJ , an error reduction of 4.4 % on the best previous single automatically learned tagging result .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
Compare
The candidate entities for this text are:
accuracy
tagger
Penn Treebank WSJ
tagger
tagger
single automatically learned tagging result

triple_list: [["accuracy", "Evaluate_for", "tagger"], ["Penn Treebank WSJ", "Evaluate_for", "tagger"], ["tagger", "Compare", "single automatically learned tagging result"]]
pred: [('accuracy', 'Evaluate_for', 'tagger'), ('Penn Treebank WSJ', 'Evaluate_for', 'tagger'), ('tagger', 'Compare', 'single automatically learned tagging result')]
trues: [['accuracy', 'Evaluate_for', 'tagger'], ['Penn Treebank WSJ', 'Evaluate_for', 'tagger'], ['error', 'Evaluate_for', 'tagger']]
metrics_sample: [0.6666666666666666, 0.6666666666666666, 0.6666666666666666]
metrics_current: [0.2943396226415094, 0.2565789473684211, 0.2741652021089631]
text: We discuss several applications of the result to the problem of distributional approximation of probabilistic context-free grammars by means of probabilistic finite automata .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
result
distributional approximation of probabilistic context-free grammars
probabilistic finite automata
distributional approximation of probabilistic context-free grammars
probabilistic finite automata
probabilistic context-free grammars

triple_list: [["result", "Used_for", "distributional approximation of probabilistic context-free grammars"], ["probabilistic finite automata", "Used_for", "distributional approximation of probabilistic context-free grammars"], ["probabilistic finite automata", "Used_for", "probabilistic context-free grammars"]]
pred: [('result', 'Used_for', 'distributional approximation of probabilistic context-free grammars'), ('probabilistic finite automata', 'Used_for', 'distributional approximation of probabilistic context-free grammars'), ('probabilistic finite automata', 'Used_for', 'probabilistic context-free grammars')]
trues: [['distributional approximation', 'Feature_of', 'probabilistic context-free grammars'], ['probabilistic finite automata', 'Used_for', 'distributional approximation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.291044776119403, 0.2549019607843137, 0.27177700348432055]
text: A central goal of this research is to merge approaches from pivot MT , interactive MT , and multilingual text authoring .
result: The candidate relations for this text are:
Conjunction
Used_for
Used_for
Used_for
The candidate entities for this text are:
approaches
pivot MT
approaches
interactive MT
approaches
multilingual text authoring
pivot MT
approaches
interactive MT
approaches

triple_list: [["approaches", "Conjunction", "pivot MT"], ["approaches", "Used_for", "pivot MT"], ["approaches", "Used_for", "interactive MT"], ["interactive MT", "Used_for", "approaches"]]
pred: [('approaches', 'Conjunction', 'pivot MT'), ('approaches', 'Used_for', 'pivot MT'), ('approaches', 'Used_for', 'interactive MT'), ('interactive MT', 'Used_for', 'approaches')]
trues: [['pivot MT', 'Conjunction', 'interactive MT'], ['interactive MT', 'Conjunction', 'multilingual text authoring']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2867647058823529, 0.2532467532467532, 0.2689655172413793]
text: The NCA is created statistically from a large corpus and recomposed under concept hierarchy constraints and frequency of occurrences .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
The candidate entities for this text are:
corpus
NCA
concept hierarchy constraints
NCA
concept hierarchy constraints
frequency of occurrences

triple_list: [["corpus", "Used_for", "NCA"], ["concept hierarchy constraints", "Used_for", "NCA"], ["concept hierarchy constraints", "Conjunction", "frequency of occurrences"]]
pred: [('corpus', 'Used_for', 'NCA'), ('concept hierarchy constraints', 'Used_for', 'NCA'), ('concept hierarchy constraints', 'Conjunction', 'frequency of occurrences')]
trues: [['concept hierarchy constraints', 'Used_for', 'NCA'], ['frequency of occurrences', 'Used_for', 'NCA']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.2872727272727273, 0.25483870967741934, 0.27008547008547007]
text: In this paper we present a new UV procedure with two major features : a ) Confidence tests are applied to decoded string hypotheses obtained from using word and garbage models that represent OOV words and noises .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
Confidence tests
decoded string hypotheses
word and garbage models
decoded string hypotheses
OOV words
word and garbage models

triple_list: [["Confidence tests", "Used_for", "decoded string hypotheses"], ["word and garbage models", "Used_for", "decoded string hypotheses"], ["OOV words", "Used_for", "word and garbage models"]]
pred: [('Confidence tests', 'Used_for', 'decoded string hypotheses'), ('word and garbage models', 'Used_for', 'decoded string hypotheses'), ('OOV words', 'Used_for', 'word and garbage models')]
trues: [['Confidence tests', 'Used_for', 'decoded string hypotheses'], ['noises', 'Conjunction', 'OOV words']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.28776978417266186, 0.2564102564102564, 0.2711864406779661]
text: In addition to the high accuracy of the model , the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
smoothing
unlexicalized parser

triple_list: [["smoothing", "Used_for", "unlexicalized parser"]]
pred: [('smoothing', 'Used_for', 'unlexicalized parser')]
trues: [['accuracy', 'Evaluate_for', 'model'], ['smoothing', 'Used_for', 'unlexicalized parser']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.2903225806451613, 0.25796178343949044, 0.2731871838111299]
text: The purpose of this research is to test the efficacy of applying automated evaluation techniques , originally devised for the evaluation of human language learners , to the output of machine translation ( MT ) systems .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
automated evaluation techniques
human language learners
automated evaluation techniques
output of machine translation ( MT ) systems
human language learners
output of machine translation ( MT ) systems

triple_list: [["automated evaluation techniques", "Used_for", "human language learners"], ["automated evaluation techniques", "Used_for", "output of machine translation ( MT ) systems"], ["human language learners", "Used_for", "output of machine translation ( MT ) systems"]]
pred: [('automated evaluation techniques', 'Used_for', 'human language learners'), ('automated evaluation techniques', 'Used_for', 'output of machine translation ( MT ) systems'), ('human language learners', 'Used_for', 'output of machine translation ( MT ) systems')]
trues: [['automated evaluation techniques', 'Used_for', 'evaluation of human language learners']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.2872340425531915, 0.2571428571428571, 0.27135678391959794]
text: It models reflection as regions containing two different layers moving over each other .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
It
reflection

triple_list: [["It", "Used_for", "reflection"]]
pred: [('It', 'Used_for', 'reflection')]
trues: [['It', 'Used_for', 'reflection']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.28975265017667845, 0.25949367088607594, 0.27378964941569284]
text: This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .
result: The candidate relations for this text are:
Used_for
Hyponym_of
Conjunction
Hyponym_of
Conjunction
Hyponym_of
Conjunction
Hyponym_of
The candidate entities for this text are:
grammar formalisms
formalism
rewriting systems
grammar formalisms
rewriting systems
dependency grammars
dependency grammars
grammar formalisms
dependency grammars
TAG
TAG
grammar formalisms
TAG
HPSG
HPSG
grammar formalisms
HPSG
LFG
grammar formalisms

triple_list: [["grammar formalisms", "Used_for", "formalism"], ["rewriting systems", "Hyponym_of", "grammar formalisms"], ["rewriting systems", "Conjunction", "dependency grammars"], ["dependency grammars", "Hyponym_of", "grammar formalisms"], ["dependency grammars", "Conjunction", "TAG"], ["TAG", "Hyponym_of", "grammar formalisms"], ["TAG", "Conjunction", "HPSG"], ["HPSG", "Hyponym_of", "grammar formalisms"], ["HPSG", "Conjunction", "LFG"]]
pred: [('grammar formalisms', 'Used_for', 'formalism'), ('rewriting systems', 'Hyponym_of', 'grammar formalisms'), ('rewriting systems', 'Conjunction', 'dependency grammars'), ('dependency grammars', 'Hyponym_of', 'grammar formalisms'), ('dependency grammars', 'Conjunction', 'TAG'), ('TAG', 'Hyponym_of', 'grammar formalisms'), ('TAG', 'Conjunction', 'HPSG'), ('HPSG', 'Hyponym_of', 'grammar formalisms'), ('HPSG', 'Conjunction', 'LFG')]
trues: [['formalism', 'Used_for', 'grammar formalisms'], ['rewriting systems', 'Hyponym_of', 'grammar formalisms'], ['rewriting systems', 'Conjunction', 'dependency grammars'], ['dependency grammars', 'Hyponym_of', 'grammar formalisms'], ['dependency grammars', 'Conjunction', 'TAG'], ['TAG', 'Hyponym_of', 'grammar formalisms'], ['TAG', 'Conjunction', 'HPSG'], ['HPSG', 'Hyponym_of', 'grammar formalisms'], ['HPSG', 'Conjunction', 'LFG'], ['LFG', 'Hyponym_of', 'grammar formalisms']]
metrics_sample: [0.8888888888888888, 0.8, 0.8421052631578948]
metrics_current: [0.3082191780821918, 0.27607361963190186, 0.2912621359223301]
text: We extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic Horn clauses .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
corpus-based sample
heuristic principles
probabilistic Horn clauses
heuristic principles

triple_list: [["corpus-based sample", "Used_for", "heuristic principles"], ["probabilistic Horn clauses", "Used_for", "heuristic principles"]]
pred: [('corpus-based sample', 'Used_for', 'heuristic principles'), ('probabilistic Horn clauses', 'Used_for', 'heuristic principles')]
trues: [['corpus-based sample', 'Used_for', 'heuristic principles'], ['probabilistic Horn clauses', 'Feature_of', 'heuristic principles']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.30952380952380953, 0.2774390243902439, 0.29260450160771706]
text: Recent progress in computer vision has been driven by high-capacity models trained on large datasets .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
high-capacity models
computer vision

triple_list: [["high-capacity models", "Used_for", "computer vision"]]
pred: [('high-capacity models', 'Used_for', 'computer vision')]
trues: [['high-capacity models', 'Used_for', 'computer vision'], ['large datasets', 'Used_for', 'high-capacity models']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.31186440677966104, 0.2787878787878788, 0.29440000000000005]
text: In this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances ( called the linguistic structure ) , a structure of purposes ( called the intentional structure ) , and the state of focus of attention ( called the attentional state ) .
result: The candidate relations for this text are:
Part_of
Conjunction
Part_of
Conjunction
Part_of
The candidate entities for this text are:
structure of the sequence of utterances
discourse structure
structure of the sequence of utterances
intentional structure
intentional structure
discourse structure
intentional structure
attentional state
attentional state
discourse structure

triple_list: [["structure of the sequence of utterances", "Part_of", "discourse structure"], ["structure of the sequence of utterances", "Conjunction", "intentional structure"], ["intentional structure", "Part_of", "discourse structure"], ["intentional structure", "Conjunction", "attentional state"], ["attentional state", "Part_of", "discourse structure"]]
pred: [('structure of the sequence of utterances', 'Part_of', 'discourse structure'), ('structure of the sequence of utterances', 'Conjunction', 'intentional structure'), ('intentional structure', 'Part_of', 'discourse structure'), ('intentional structure', 'Conjunction', 'attentional state'), ('attentional state', 'Part_of', 'discourse structure')]
trues: [['components', 'Part_of', 'discourse structure'], ['linguistic structure', 'Part_of', 'components'], ['linguistic structure', 'Conjunction', 'intentional structure'], ['intentional structure', 'Part_of', 'components'], ['intentional structure', 'Conjunction', 'attentional state'], ['attentional state', 'Part_of', 'components']]
metrics_sample: [0.2, 0.16666666666666666, 0.1818181818181818]
metrics_current: [0.31, 0.2767857142857143, 0.2924528301886793]
text: The model is evaluated on English and Czech newspaper texts , and is then validated on French broadcast news transcriptions .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
Evaluate_for
The candidate entities for this text are:
English and Czech newspaper texts
model
French broadcast news transcriptions
model
French broadcast news transcriptions
Czech newspaper texts

triple_list: [["English and Czech newspaper texts", "Evaluate_for", "model"], ["French broadcast news transcriptions", "Evaluate_for", "model"], ["French broadcast news transcriptions", "Evaluate_for", "Czech newspaper texts"]]
pred: [('English and Czech newspaper texts', 'Evaluate_for', 'model'), ('French broadcast news transcriptions', 'Evaluate_for', 'model'), ('French broadcast news transcriptions', 'Evaluate_for', 'Czech newspaper texts')]
trues: [['English and Czech newspaper texts', 'Evaluate_for', 'model'], ['French broadcast news transcriptions', 'Evaluate_for', 'model']]
metrics_sample: [0.6666666666666666, 1.0, 0.8]
metrics_current: [0.31353135313531355, 0.28106508875739644, 0.296411856474259]
text: Our system incorporates a linguistic parser/generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
Used_for
Used_for
The candidate entities for this text are:
linguistic parser/generator
system
linguistic parser/generator
LFG
parse reduction
system
parse reduction
packed parse forests
maximum-entropy model
system
maximum-entropy model
stochastic output selection

triple_list: [["linguistic parser/generator", "Used_for", "system"], ["linguistic parser/generator", "Used_for", "LFG"], ["parse reduction", "Conjunction", "system"], ["parse reduction", "Used_for", "packed parse forests"], ["maximum-entropy model", "Used_for", "system"], ["maximum-entropy model", "Used_for", "stochastic output selection"]]
pred: [('linguistic parser/generator', 'Used_for', 'system'), ('linguistic parser/generator', 'Used_for', 'LFG'), ('parse reduction', 'Conjunction', 'system'), ('parse reduction', 'Used_for', 'packed parse forests'), ('maximum-entropy model', 'Used_for', 'system'), ('maximum-entropy model', 'Used_for', 'stochastic output selection')]
trues: [['linguistic parser/generator', 'Part_of', 'system'], ['linguistic parser/generator', 'Used_for', 'LFG'], ['linguistic parser/generator', 'Conjunction', 'transfer component'], ['transfer component', 'Part_of', 'system'], ['transfer component', 'Used_for', 'parse reduction'], ['transfer component', 'Conjunction', 'maximum-entropy model'], ['packed parse forests', 'Used_for', 'parse reduction'], ['maximum-entropy model', 'Part_of', 'system'], ['maximum-entropy model', 'Used_for', 'stochastic output selection']]
metrics_sample: [0.3333333333333333, 0.2222222222222222, 0.26666666666666666]
metrics_current: [0.313915857605178, 0.27953890489913547, 0.29573170731707316]
text: Unfortunately , creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
human effort
large datasets with pixel-level labels

triple_list: [["human effort", "Used_for", "large datasets with pixel-level labels"]]
pred: [('human effort', 'Used_for', 'large datasets with pixel-level labels')]
trues: [['pixel-level labels', 'Feature_of', 'large datasets']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31290322580645163, 0.27873563218390807, 0.2948328267477203]
text: We believe that these evaluation techniques will provide information about both the human language learning process , the translation process and the development of machine translation systems .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
evaluation techniques
human language learning process
evaluation techniques
translation process
human language learning process
translation process
evaluation techniques
machine translation systems

triple_list: [["evaluation techniques", "Used_for", "human language learning process"], ["evaluation techniques", "Used_for", "translation process"], ["human language learning process", "Conjunction", "translation process"], ["evaluation techniques", "Used_for", "machine translation systems"]]
pred: [('evaluation techniques', 'Used_for', 'human language learning process'), ('evaluation techniques', 'Used_for', 'translation process'), ('human language learning process', 'Conjunction', 'translation process'), ('evaluation techniques', 'Used_for', 'machine translation systems')]
trues: [['evaluation techniques', 'Used_for', 'human language learning process'], ['evaluation techniques', 'Used_for', 'translation process'], ['evaluation techniques', 'Used_for', 'machine translation systems'], ['human language learning process', 'Conjunction', 'translation process'], ['translation process', 'Conjunction', 'machine translation systems']]
metrics_sample: [1.0, 0.8, 0.888888888888889]
metrics_current: [0.321656050955414, 0.28611898016997167, 0.3028485757121439]
text: Listen-Communicate-Show ( LCS ) is a new paradigm for human interaction with data sources .
result: The candidate relations for this text are:
Hyponym_of
The candidate entities for this text are:
Listen-Communicate-Show ( LCS )
human interaction with data sources

triple_list: [["Listen-Communicate-Show ( LCS )", "Hyponym_of", "human interaction with data sources"]]
pred: [('Listen-Communicate-Show ( LCS )', 'Hyponym_of', 'human interaction with data sources')]
trues: [['Listen-Communicate-Show ( LCS )', 'Used_for', 'human interaction with data sources']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32063492063492066, 0.2853107344632768, 0.30194319880418535]
text: The new criterion -- meaning-entailing substitutability -- fits the needs of semantic-oriented NLP applications and can be evaluated directly ( independent of an application ) at a good level of human agreement .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
meaning-entailing substitutability
semantic-oriented NLP applications

triple_list: [["meaning-entailing substitutability", "Used_for", "semantic-oriented NLP applications"]]
pred: [('meaning-entailing substitutability', 'Used_for', 'semantic-oriented NLP applications')]
trues: [['meaning-entailing substitutability', 'Used_for', 'semantic-oriented NLP applications'], ['human agreement', 'Evaluate_for', 'meaning-entailing substitutability']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3227848101265823, 0.28651685393258425, 0.30357142857142855]
text: However , they provide no guarantee of being more efficient than exhaustive search .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
they
exhaustive search

triple_list: [["they", "Compare", "exhaustive search"]]
pred: [('they', 'Compare', 'exhaustive search')]
trues: [['they', 'Compare', 'exhaustive search']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3249211356466877, 0.28851540616246496, 0.30563798219584565]
text: It is based on a weakly supervised dependency parser that can model speech syntax without relying on any annotated training corpus .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
weakly supervised dependency parser
It
speech syntax
weakly supervised dependency parser
annotated training corpus
weakly supervised dependency parser

triple_list: [["weakly supervised dependency parser", "Used_for", "It"], ["speech syntax", "Used_for", "weakly supervised dependency parser"], ["annotated training corpus", "Used_for", "weakly supervised dependency parser"]]
pred: [('weakly supervised dependency parser', 'Used_for', 'It'), ('speech syntax', 'Used_for', 'weakly supervised dependency parser'), ('annotated training corpus', 'Used_for', 'weakly supervised dependency parser')]
trues: [['weakly supervised dependency parser', 'Used_for', 'speech syntax']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.321875, 0.2877094972067039, 0.30383480825958703]
text: Our numerical hybrid local and global mode-seeking tracker is validated on challenging airborne videos with heavy occlusion and large camera motions .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
numerical hybrid local and global mode-seeking tracker
airborne videos
airborne videos
heavy occlusion and large camera motions

triple_list: [["numerical hybrid local and global mode-seeking tracker", "Used_for", "airborne videos"], ["airborne videos", "Used_for", "heavy occlusion and large camera motions"]]
pred: [('numerical hybrid local and global mode-seeking tracker', 'Used_for', 'airborne videos'), ('airborne videos', 'Used_for', 'heavy occlusion and large camera motions')]
trues: [['airborne videos', 'Evaluate_for', 'numerical hybrid local and global mode-seeking tracker'], ['heavy occlusion', 'Feature_of', 'airborne videos'], ['heavy occlusion', 'Conjunction', 'camera motions'], ['camera motions', 'Feature_of', 'airborne videos']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3198757763975155, 0.2845303867403315, 0.30116959064327486]
text: This paper presents necessary and sufficient conditions for the use of demonstrative expressions in English and discusses implications for current discourse processing algorithms .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
demonstrative expressions
English
demonstrative expressions
discourse processing algorithms

triple_list: [["demonstrative expressions", "Used_for", "English"], ["demonstrative expressions", "Used_for", "discourse processing algorithms"]]
pred: [('demonstrative expressions', 'Used_for', 'English'), ('demonstrative expressions', 'Used_for', 'discourse processing algorithms')]
trues: [['demonstrative expressions', 'Feature_of', 'English'], ['implications', 'Used_for', 'discourse processing algorithms']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31790123456790126, 0.28296703296703296, 0.2994186046511628]
text: SVMs are known to be robust to overfitting ; however , a few training examples usually do not represent well the structure of the class .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
SVMs
overfitting

triple_list: [["SVMs", "Used_for", "overfitting"]]
pred: [('SVMs', 'Used_for', 'overfitting')]
trues: [['SVMs', 'Used_for', 'overfitting']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.32, 0.28493150684931506, 0.30144927536231886]
text: In addition , we improve the learning by a metric weight constraint , so that the learned metric has a better generalization ability .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
metric weight constraint
learning

triple_list: [["metric weight constraint", "Used_for", "learning"]]
pred: [('metric weight constraint', 'Used_for', 'learning')]
trues: [['metric weight constraint', 'Used_for', 'learning'], ['generalization ability', 'Feature_of', 'learned metric']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3220858895705521, 0.28610354223433243, 0.303030303030303]
text: We also derive an efficient non-linear extension of WARCA by using the kernel trick .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
kernel trick
non-linear extension of WARCA
kernel trick
WARCA

triple_list: [["kernel trick", "Used_for", "non-linear extension of WARCA"], ["kernel trick", "Used_for", "WARCA"]]
pred: [('kernel trick', 'Used_for', 'non-linear extension of WARCA'), ('kernel trick', 'Used_for', 'WARCA')]
trues: [['kernel trick', 'Used_for', 'non-linear extension of WARCA']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3231707317073171, 0.28804347826086957, 0.3045977011494253]
text: MINPRAN 's properties are connrmed experimentally on synthetic data and compare favorably to least median of squares .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
MINPRAN
least median of squares

triple_list: [["MINPRAN", "Compare", "least median of squares"]]
pred: [('MINPRAN', 'Compare', 'least median of squares')]
trues: [['synthetic data', 'Evaluate_for', 'MINPRAN'], ['least median of squares', 'Compare', 'MINPRAN']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3221884498480243, 0.2864864864864865, 0.30329041487839775]
text: This paper gives an overall account of a prototype natural language question answering system , called Chat-80 .
result: The candidate relations for this text are:
Hyponym_of
The candidate entities for this text are:
Chat-80
natural language question answering system

triple_list: [["Chat-80", "Hyponym_of", "natural language question answering system"]]
pred: [('Chat-80', 'Hyponym_of', 'natural language question answering system')]
trues: [['Chat-80', 'Hyponym_of', 'natural language question answering system']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3242424242424242, 0.2884097035040431, 0.3052781740370898]
text: We suggest a new goal and evaluation criterion for word similarity measures .
result: The candidate relations for this text are:
Evaluate_for
The candidate entities for this text are:
word similarity measures
goal

triple_list: [["word similarity measures", "Evaluate_for", "goal"]]
pred: [('word similarity measures', 'Evaluate_for', 'goal')]
trues: [['evaluation criterion', 'Used_for', 'word similarity measures']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32326283987915405, 0.28763440860215056, 0.30440967283072545]
text: This paper investigates the utility of applying standard MT evaluation methods ( BLEU , NIST , WER and PER ) to building classifiers to predict semantic equivalence and entailment .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
MT evaluation methods
classifiers
BLEU
MT evaluation methods
BLEU
NIST
NIST
MT evaluation methods

triple_list: [["MT evaluation methods", "Used_for", "classifiers"], ["BLEU", "Used_for", "MT evaluation methods"], ["BLEU", "Conjunction", "NIST"], ["NIST", "Used_for", "MT evaluation methods"]]
pred: [('MT evaluation methods', 'Used_for', 'classifiers'), ('BLEU', 'Used_for', 'MT evaluation methods'), ('BLEU', 'Conjunction', 'NIST'), ('NIST', 'Used_for', 'MT evaluation methods')]
trues: [['MT evaluation methods', 'Used_for', 'classifiers'], ['BLEU', 'Hyponym_of', 'MT evaluation methods'], ['BLEU', 'Conjunction', 'NIST'], ['NIST', 'Hyponym_of', 'MT evaluation methods'], ['NIST', 'Conjunction', 'WER'], ['WER', 'Hyponym_of', 'MT evaluation methods'], ['WER', 'Conjunction', 'PER'], ['PER', 'Hyponym_of', 'MT evaluation methods'], ['classifiers', 'Used_for', 'semantic equivalence'], ['classifiers', 'Used_for', 'entailment'], ['semantic equivalence', 'Conjunction', 'entailment']]
metrics_sample: [0.5, 0.18181818181818182, 0.26666666666666666]
metrics_current: [0.3253731343283582, 0.2845953002610966, 0.30362116991643456]
text: Using this approach , we extract parallel data from large Chinese , Arabic , and English non-parallel newspaper corpora .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
approach
parallel data
Chinese
parallel data
Chinese
Arabic
Arabic
parallel data

triple_list: [["approach", "Used_for", "parallel data"], ["Chinese", "Used_for", "parallel data"], ["Chinese", "Conjunction", "Arabic"], ["Arabic", "Used_for", "parallel data"]]
pred: [('approach', 'Used_for', 'parallel data'), ('Chinese', 'Used_for', 'parallel data'), ('Chinese', 'Conjunction', 'Arabic'), ('Arabic', 'Used_for', 'parallel data')]
trues: [['approach', 'Used_for', 'parallel data'], ['parallel data', 'Part_of', 'Chinese , Arabic , and English non-parallel newspaper corpora']]
metrics_sample: [0.25, 0.5, 0.3333333333333333]
metrics_current: [0.32448377581120946, 0.2857142857142857, 0.30386740331491713]
text: Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification , and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification .
result: The candidate relations for this text are:
Used_for
Evaluate_for
Compare
Evaluate_for
The candidate entities for this text are:
strategies
robust deep metrics
benchmarks of person re-identification
robust deep metrics
robust deep metrics
state-of-the-art methods
benchmarks of person re-identification
state-of-the-art methods

triple_list: [["strategies", "Used_for", "robust deep metrics"], ["benchmarks of person re-identification", "Evaluate_for", "robust deep metrics"], ["robust deep metrics", "Compare", "state-of-the-art methods"], ["benchmarks of person re-identification", "Evaluate_for", "state-of-the-art methods"]]
pred: [('strategies', 'Used_for', 'robust deep metrics'), ('benchmarks of person re-identification', 'Evaluate_for', 'robust deep metrics'), ('robust deep metrics', 'Compare', 'state-of-the-art methods'), ('benchmarks of person re-identification', 'Evaluate_for', 'state-of-the-art methods')]
trues: [['robust deep metrics', 'Used_for', 'person re-identification'], ['deep model', 'Compare', 'state-of-the-art methods'], ['deep model', 'Used_for', 'person re-identification'], ['state-of-the-art methods', 'Used_for', 'person re-identification']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3206997084548105, 0.2827763496143959, 0.30054644808743164]
text: Our experiments on real data sets show that the resulting detector is more robust to the choice of training examples , and substantially improves both linear and kernel SVM when trained on 10 positive and 10 negative examples .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
detector
robustness
linear and kernel SVM
detector

triple_list: [["detector", "Used_for", "robustness"], ["linear and kernel SVM", "Used_for", "detector"]]
pred: [('detector', 'Used_for', 'robustness'), ('linear and kernel SVM', 'Used_for', 'detector')]
trues: [['real data sets', 'Evaluate_for', 'detector'], ['detector', 'Compare', 'linear and kernel SVM']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3188405797101449, 0.2813299232736573, 0.2989130434782608]
text: Here , we leverage a logistic stick-breaking representation and recent innovations in Plya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods , enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Used_for
The candidate entities for this text are:
logistic stick-breaking representation
multinomial distribution
Plya-gamma augmentation
multinomial distribution
logistic stick-breaking representation
Plya-gamma augmentation
Bayesian inference techniques
Gaussian models

triple_list: [["logistic stick-breaking representation", "Used_for", "multinomial distribution"], ["Plya-gamma augmentation", "Used_for", "multinomial distribution"], ["logistic stick-breaking representation", "Used_for", "Plya-gamma augmentation"], ["Bayesian inference techniques", "Used_for", "Gaussian models"]]
pred: [('logistic stick-breaking representation', 'Used_for', 'multinomial distribution'), ('Plya-gamma augmentation', 'Used_for', 'multinomial distribution'), ('logistic stick-breaking representation', 'Used_for', 'Plya-gamma augmentation'), ('Bayesian inference techniques', 'Used_for', 'Gaussian models')]
trues: [['logistic stick-breaking representation', 'Used_for', 'multinomial distribution'], ['Plya-gamma augmentation', 'Used_for', 'multinomial distribution'], ['latent variables', 'Part_of', 'multinomial distribution'], ['jointly Gaussian likelihoods', 'Feature_of', 'latent variables'], ['Bayesian inference techniques', 'Used_for', 'Gaussian models'], ['minimal overhead', 'Feature_of', 'Gaussian models']]
metrics_sample: [0.75, 0.5, 0.6]
metrics_current: [0.3237822349570201, 0.28463476070528965, 0.30294906166219837]
text: Computing power per area and power consumption is amongst the highest reported for a single chip .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
Computing power per area
power consumption

triple_list: [["Computing power per area", "Compare", "power consumption"]]
pred: [('Computing power per area', 'Compare', 'power consumption')]
trues: [['Computing power per area', 'Conjunction', 'power consumption']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32285714285714284, 0.28391959798994976, 0.3021390374331551]
text: In this paper , we evaluate the use of a conditional random field ( CRF ) for this task and relate results with this model to our prior work .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
conditional random field ( CRF )
task

triple_list: [["conditional random field ( CRF )", "Used_for", "task"]]
pred: [('conditional random field ( CRF )', 'Used_for', 'task')]
trues: [['conditional random field ( CRF )', 'Used_for', 'task']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3247863247863248, 0.2857142857142857, 0.304]
text: The robust principal component analysis ( robust PCA ) problem has been considered in many machine learning applications , where the goal is to decompose the data matrix to a low rank part plus a sparse residual .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
machine learning applications
robust PCA problem
low rank part plus a sparse residual
machine learning applications

triple_list: [["machine learning applications", "Used_for", "robust PCA problem"], ["low rank part plus a sparse residual", "Used_for", "machine learning applications"]]
pred: [('machine learning applications', 'Used_for', 'robust PCA problem'), ('low rank part plus a sparse residual', 'Used_for', 'machine learning applications')]
trues: [['robust principal component analysis ( robust PCA ) problem', 'Used_for', 'machine learning applications'], ['low rank part', 'Part_of', 'data matrix'], ['low rank part', 'Conjunction', 'sparse residual'], ['sparse residual', 'Part_of', 'data matrix']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32294617563739375, 0.28287841191067, 0.3015873015873016]
text: This has the advantages of efficiency on grammatical input , and robustness in the face of ungrammatical input .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
efficiency
robustness

triple_list: [["efficiency", "Compare", "robustness"]]
pred: [('efficiency', 'Compare', 'robustness')]
trues: [['ungrammatical input', 'Feature_of', 'robustness']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3220338983050847, 0.28217821782178215, 0.30079155672823216]
text: We also introduce a novel classification method based on PER which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
classification method
classification
part of speech information
classification method

triple_list: [["classification method", "Used_for", "classification"], ["part of speech information", "Used_for", "classification method"]]
pred: [('classification method', 'Used_for', 'classification'), ('part of speech information', 'Used_for', 'classification method')]
trues: [['PER', 'Used_for', 'classification method'], ['PER', 'Used_for', 'part of speech information'], ['part of speech information', 'Used_for', 'word matches and non-matches']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3202247191011236, 0.2800982800982801, 0.29882044560943644]
text: We present a novel method for discovering parallel sentences in comparable , non-parallel corpora .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
method
discovering parallel sentences

triple_list: [["method", "Used_for", "discovering parallel sentences"]]
pred: [('method', 'Used_for', 'discovering parallel sentences')]
trues: [['method', 'Used_for', 'discovering parallel sentences'], ['comparable , non-parallel corpora', 'Used_for', 'discovering parallel sentences']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.32212885154061627, 0.28117359413202936, 0.3002610966057441]
text: Our study reveals that the proposed method not only reduces an extensive system development effort but also improves the transliteration accuracy significantly .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
method
transliteration accuracy

triple_list: [["method", "Used_for", "transliteration accuracy"]]
pred: [('method', 'Used_for', 'transliteration accuracy')]
trues: [['transliteration accuracy', 'Evaluate_for', 'method']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32122905027932963, 0.2804878048780488, 0.29947916666666674]
text: To evaluate our proposed model , we build a large streaming video dataset with annotations .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
annotations
streaming video dataset

triple_list: [["annotations", "Used_for", "streaming video dataset"]]
pred: [('annotations', 'Used_for', 'streaming video dataset')]
trues: [['streaming video dataset', 'Evaluate_for', 'model']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3203342618384401, 0.2798053527980535, 0.29870129870129863]
text: We propose to incorporate a priori geometric constraints in a 3 -- D stereo reconstruction scheme to cope with the many cases where image information alone is not sufficient to accurately recover 3 -- D shape .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
geometric constraints
3 -- D stereo reconstruction scheme
geometric constraints
3 -- D shape

triple_list: [["geometric constraints", "Used_for", "3 -- D stereo reconstruction scheme"], ["geometric constraints", "Used_for", "3 -- D shape"]]
pred: [('geometric constraints', 'Used_for', '3 -- D stereo reconstruction scheme'), ('geometric constraints', 'Used_for', '3 -- D shape')]
trues: [['priori geometric constraints', 'Part_of', '3 -- D stereo reconstruction scheme'], ['image information', 'Used_for', '3 -- D shape']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3185595567867036, 0.2784503631961259, 0.2971576227390181]
text: The modified graph is then sent to the UNL-L0 deconverter and the result shown .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
UNL-L0 deconverter
graph

triple_list: [["UNL-L0 deconverter", "Used_for", "graph"]]
pred: [('UNL-L0 deconverter', 'Used_for', 'graph')]
trues: [['graph', 'Used_for', 'UNL-L0 deconverter']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31767955801104975, 0.2777777777777778, 0.2963917525773196]
text: Compared to the exhaustive procedure that thus far is the state-of-the-art for cascade evaluation , the proposed method requires fewer evaluations of the classifier functions , thereby speeding up the search .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
exhaustive procedure
method

triple_list: [["exhaustive procedure", "Compare", "method"]]
pred: [('exhaustive procedure', 'Compare', 'method')]
trues: [['exhaustive procedure', 'Used_for', 'cascade evaluation'], ['exhaustive procedure', 'Compare', 'method'], ['method', 'Used_for', 'search']]
metrics_sample: [1.0, 0.3333333333333333, 0.5]
metrics_current: [0.31955922865013775, 0.27817745803357313, 0.2974358974358975]
text: We report experiments conducted on a multilingual corpus to estimate the number of analogies among the sentences that it contains .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
multilingual corpus
number of analogies

triple_list: [["multilingual corpus", "Used_for", "number of analogies"]]
pred: [('multilingual corpus', 'Used_for', 'number of analogies')]
trues: [['multilingual corpus', 'Evaluate_for', 'analogies']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31868131868131866, 0.27751196172248804, 0.2966751918158568]
text: Amorph recognizes NE items in two stages : dictionary lookup and rule application .
result: The candidate relations for this text are:
Used_for
Conjunction
Used_for
The candidate entities for this text are:
dictionary lookup
Amorph
dictionary lookup
rule application
rule application
Amorph

triple_list: [["dictionary lookup", "Used_for", "Amorph"], ["dictionary lookup", "Conjunction", "rule application"], ["rule application", "Used_for", "Amorph"]]
pred: [('dictionary lookup', 'Used_for', 'Amorph'), ('dictionary lookup', 'Conjunction', 'rule application'), ('rule application', 'Used_for', 'Amorph')]
trues: [['Amorph', 'Used_for', 'NE items'], ['dictionary lookup', 'Part_of', 'Amorph'], ['dictionary lookup', 'Conjunction', 'rule application'], ['rule application', 'Part_of', 'Amorph']]
metrics_sample: [0.3333333333333333, 0.25, 0.28571428571428575]
metrics_current: [0.3188010899182561, 0.2772511848341232, 0.2965779467680608]
text: The objective of this work is to recognize faces using video sequences both for training and recognition input , in a realistic , unconstrained setup in which lighting , pose and user motion pattern have a wide variability and face images are of low resolution .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Used_for
Conjunction
The candidate entities for this text are:
video sequences
faces
video sequences
recognition
video sequences
training
lighting
pose
pose
user motion pattern

triple_list: [["video sequences", "Used_for", "faces"], ["video sequences", "Used_for", "recognition"], ["video sequences", "Used_for", "training"], ["lighting", "Used_for", "pose"], ["pose", "Conjunction", "user motion pattern"]]
pred: [('video sequences', 'Used_for', 'faces'), ('video sequences', 'Used_for', 'recognition'), ('video sequences', 'Used_for', 'training'), ('lighting', 'Used_for', 'pose'), ('pose', 'Conjunction', 'user motion pattern')]
trues: [['lighting', 'Conjunction', 'pose'], ['pose', 'Conjunction', 'user motion pattern'], ['resolution', 'Feature_of', 'face images']]
metrics_sample: [0.2, 0.3333333333333333, 0.25]
metrics_current: [0.3172043010752688, 0.2776470588235294, 0.2961104140526976]
text: In the experiments , we used a variety of methods for preparing a segmented corpus and compared the language models by their speech recognition accuracies .
result: The candidate relations for this text are:
Used_for
Used_for
Compare
Evaluate_for
The candidate entities for this text are:
methods
segmented corpus
segmented corpus
language models
speech recognition accuracies
language models
speech recognition accuracies
segmented corpus

triple_list: [["methods", "Used_for", "segmented corpus"], ["segmented corpus", "Used_for", "language models"], ["speech recognition accuracies", "Compare", "language models"], ["speech recognition accuracies", "Evaluate_for", "segmented corpus"]]
pred: [('methods', 'Used_for', 'segmented corpus'), ('segmented corpus', 'Used_for', 'language models'), ('speech recognition accuracies', 'Compare', 'language models'), ('speech recognition accuracies', 'Evaluate_for', 'segmented corpus')]
trues: [['methods', 'Used_for', 'preparing a segmented corpus'], ['speech recognition accuracies', 'Evaluate_for', 'language models']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31382978723404253, 0.27634660421545665, 0.29389788293897884]
text: We argue that it is necessary to draw a line between generalizable semantic principles and domain-specific semantic information .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
generalizable semantic principles
domain-specific semantic information

triple_list: [["generalizable semantic principles", "Compare", "domain-specific semantic information"]]
pred: [('generalizable semantic principles', 'Compare', 'domain-specific semantic information')]
trues: [['generalizable semantic principles', 'Compare', 'domain-specific semantic information']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3156498673740053, 0.2780373831775701, 0.2956521739130435]
text: In the case of natural speech signals , the method is shown to have separation accuracy better than the state-of-the-art methods .
result: The candidate relations for this text are:
Compare
Evaluate_for
Evaluate_for
The candidate entities for this text are:
method
natural speech signals
method
separation accuracy
state-of-the-art methods
separation accuracy

triple_list: [["method", "Compare", "natural speech signals"], ["method", "Evaluate_for", "separation accuracy"], ["state-of-the-art methods", "Evaluate_for", "separation accuracy"]]
pred: [('method', 'Compare', 'natural speech signals'), ('method', 'Evaluate_for', 'separation accuracy'), ('state-of-the-art methods', 'Evaluate_for', 'separation accuracy')]
trues: [['method', 'Used_for', 'natural speech signals'], ['method', 'Compare', 'methods'], ['separation accuracy', 'Evaluate_for', 'method'], ['separation accuracy', 'Evaluate_for', 'methods'], ['methods', 'Used_for', 'natural speech signals']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3131578947368421, 0.2748267898383372, 0.29274292742927427]
text: The results show that the features in terms of which we formulate our heuristic principles have significant predictive power , and that rules that closely resemble our Horn clauses can be learnt automatically from these features .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
features
heuristic principles
features
rules

triple_list: [["features", "Used_for", "heuristic principles"], ["features", "Used_for", "rules"]]
pred: [('features', 'Used_for', 'heuristic principles'), ('features', 'Used_for', 'rules')]
trues: [['features', 'Feature_of', 'heuristic principles']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31151832460732987, 0.27419354838709675, 0.29166666666666663]
text: A domain independent model is proposed for the automated interpretation of nominal compounds in English .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
domain independent model
automated interpretation of nominal compounds

triple_list: [["domain independent model", "Used_for", "automated interpretation of nominal compounds"]]
pred: [('domain independent model', 'Used_for', 'automated interpretation of nominal compounds')]
trues: [['domain independent model', 'Used_for', 'automated interpretation of nominal compounds'], ['English', 'Feature_of', 'nominal compounds']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3133159268929504, 0.27522935779816515, 0.29304029304029305]
text: We compare our results to state of the art methods such as mi-cro phase shifting and modulated phase shifting .
result: The candidate relations for this text are:
Compare
Conjunction
The candidate entities for this text are:
mi-cro phase shifting
methods
mi-cro phase shifting
modulated phase shifting

triple_list: [["mi-cro phase shifting", "Compare", "methods"], ["mi-cro phase shifting", "Conjunction", "modulated phase shifting"]]
pred: [('mi-cro phase shifting', 'Compare', 'methods'), ('mi-cro phase shifting', 'Conjunction', 'modulated phase shifting')]
trues: [['mi-cro phase shifting', 'Hyponym_of', 'state of the art methods'], ['mi-cro phase shifting', 'Conjunction', 'modulated phase shifting'], ['modulated phase shifting', 'Hyponym_of', 'state of the art methods']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.3142857142857143, 0.275626423690205, 0.2936893203883495]
text: This research is part of a larger study of anaphoric expressions , the results of which will be incorporated into a natural language generation system .
result: The candidate relations for this text are:
Part_of
Used_for
The candidate entities for this text are:
anaphoric expressions
research
natural language generation system
anaphoric expressions

triple_list: [["anaphoric expressions", "Part_of", "research"], ["natural language generation system", "Used_for", "anaphoric expressions"]]
pred: [('anaphoric expressions', 'Part_of', 'research'), ('natural language generation system', 'Used_for', 'anaphoric expressions')]
trues: [['anaphoric expressions', 'Used_for', 'natural language generation system']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31266149870801035, 0.275, 0.2926239419588875]
text: In comparison with previous works , in this paper it is assumed that the signals are not i.i.d. in each epoch , but obey a first-order autoregressive model .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
first-order autoregressive model
signals

triple_list: [["first-order autoregressive model", "Used_for", "signals"]]
pred: [('first-order autoregressive model', 'Used_for', 'signals')]
trues: [['first-order autoregressive model', 'Used_for', 'signals']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.31443298969072164, 0.2766439909297052, 0.2943305186972256]
text: While sentence extraction as an approach to summarization has been shown to work in documents of certain genres , because of the conversational nature of email communication where utterances are made in relation to one made previously , sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
sentence extraction
summarization
sentence extraction
email communication

triple_list: [["sentence extraction", "Used_for", "summarization"], ["sentence extraction", "Used_for", "email communication"]]
pred: [('sentence extraction', 'Used_for', 'summarization'), ('sentence extraction', 'Used_for', 'email communication')]
trues: [['sentence extraction', 'Used_for', 'summarization']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3153846153846154, 0.27828054298642535, 0.2956730769230769]
text: We propose a novel probabilistic framework for learning visual models of 3D object categories by combining appearance information and geometric constraints .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
probabilistic framework
visual models of 3D object categories
appearance information
geometric constraints
appearance information
geometric constraints
geometric constraints
visual models of 3D object categories

triple_list: [["probabilistic framework", "Used_for", "visual models of 3D object categories"], ["appearance information", "Used_for", "geometric constraints"], ["appearance information", "Conjunction", "geometric constraints"], ["geometric constraints", "Used_for", "visual models of 3D object categories"]]
pred: [('probabilistic framework', 'Used_for', 'visual models of 3D object categories'), ('appearance information', 'Used_for', 'geometric constraints'), ('appearance information', 'Conjunction', 'geometric constraints'), ('geometric constraints', 'Used_for', 'visual models of 3D object categories')]
trues: [['probabilistic framework', 'Used_for', 'visual models of 3D object categories'], ['appearance information', 'Used_for', 'probabilistic framework'], ['appearance information', 'Conjunction', 'geometric constraints'], ['geometric constraints', 'Used_for', 'probabilistic framework']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.31725888324873097, 0.2802690582959641, 0.2976190476190476]
text: We investigate the problem of learning to predict moves in the board game of Go from game records of expert players .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
game records
learning
game records
board game of Go

triple_list: [["game records", "Used_for", "learning"], ["game records", "Used_for", "board game of Go"]]
pred: [('game records', 'Used_for', 'learning'), ('game records', 'Used_for', 'board game of Go')]
trues: [['game records of expert players', 'Used_for', 'board game of Go']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31565656565656564, 0.2796420581655481, 0.29655990510083036]
text: Experiment results on ACE corpora show that this spectral clustering based approach outperforms the other clustering methods .
result: The candidate relations for this text are:
Compare
Evaluate_for
Evaluate_for
The candidate entities for this text are:
spectral clustering based approach
clustering methods
ACE corpora
spectral clustering based approach
ACE corpora
clustering methods

triple_list: [["spectral clustering based approach", "Compare", "clustering methods"], ["ACE corpora", "Evaluate_for", "spectral clustering based approach"], ["ACE corpora", "Evaluate_for", "clustering methods"]]
pred: [('spectral clustering based approach', 'Compare', 'clustering methods'), ('ACE corpora', 'Evaluate_for', 'spectral clustering based approach'), ('ACE corpora', 'Evaluate_for', 'clustering methods')]
trues: [['ACE corpora', 'Evaluate_for', 'spectral clustering based approach'], ['ACE corpora', 'Evaluate_for', 'clustering methods'], ['spectral clustering based approach', 'Compare', 'clustering methods']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3208020050125313, 0.28444444444444444, 0.3015312131919905]
text: A demonstration ( in UNIX ) for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis ( ICALL ) , including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples .
result: The candidate relations for this text are:
Used_for
Part_of
Part_of
Conjunction
Part_of
The candidate entities for this text are:
components
Applied Natural Language Processing
components
intelligent computer-assisted morphological analysis ( ICALL )
intelligent computer-assisted morphological analysis ( ICALL )
disambiguated morphological analysis
intelligent computer-assisted morphological analysis ( ICALL )
lemmatized indexing
aligned bilingual corpus of word examples
lemmatized indexing

triple_list: [["components", "Used_for", "Applied Natural Language Processing"], ["components", "Part_of", "intelligent computer-assisted morphological analysis ( ICALL )"], ["intelligent computer-assisted morphological analysis ( ICALL )", "Part_of", "disambiguated morphological analysis"], ["intelligent computer-assisted morphological analysis ( ICALL )", "Conjunction", "lemmatized indexing"], ["aligned bilingual corpus of word examples", "Part_of", "lemmatized indexing"]]
pred: [('components', 'Used_for', 'Applied Natural Language Processing'), ('components', 'Part_of', 'intelligent computer-assisted morphological analysis ( ICALL )'), ('intelligent computer-assisted morphological analysis ( ICALL )', 'Part_of', 'disambiguated morphological analysis'), ('intelligent computer-assisted morphological analysis ( ICALL )', 'Conjunction', 'lemmatized indexing'), ('aligned bilingual corpus of word examples', 'Part_of', 'lemmatized indexing')]
trues: [['components', 'Used_for', 'intelligent computer-assisted morphological analysis ( ICALL )'], ['disambiguated morphological analysis', 'Hyponym_of', 'components'], ['disambiguated morphological analysis', 'Conjunction', 'lemmatized indexing'], ['disambiguated morphological analysis', 'Used_for', 'aligned bilingual corpus'], ['lemmatized indexing', 'Hyponym_of', 'components'], ['lemmatized indexing', 'Used_for', 'aligned bilingual corpus']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31683168316831684, 0.2807017543859649, 0.2976744186046511]
text: We explain this distinction and we show how this model may be applied to the interpretation of compounds in real texts , provided that complementary semantic information are retrieved .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
model
interpretation of compounds
complementary semantic information
model

triple_list: [["model", "Used_for", "interpretation of compounds"], ["complementary semantic information", "Used_for", "model"]]
pred: [('model', 'Used_for', 'interpretation of compounds'), ('complementary semantic information', 'Used_for', 'model')]
trues: [['model', 'Used_for', 'interpretation of compounds']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.31773399014778325, 0.28227571115973743, 0.2989571263035921]
text: We present a novel approach for automatically acquiring English topic signatures .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
approach
English topic signatures

triple_list: [["approach", "Used_for", "English topic signatures"]]
pred: [('approach', 'Used_for', 'English topic signatures')]
trues: [['approach', 'Used_for', 'automatically acquiring English topic signatures']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31695331695331697, 0.2816593886462882, 0.29826589595375724]
text: It uses a powerful pattern-matching language to classify grs into frames hierarchically in a way that mirrors inheritance-based lexica .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
pattern-matching language
It
pattern-matching language
frames

triple_list: [["pattern-matching language", "Used_for", "It"], ["pattern-matching language", "Used_for", "frames"]]
pred: [('pattern-matching language', 'Used_for', 'It'), ('pattern-matching language', 'Used_for', 'frames')]
trues: [['pattern-matching language', 'Used_for', 'It'], ['pattern-matching language', 'Used_for', 'grs']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.31784841075794623, 0.2826086956521739, 0.2991944764096663]
text: In this paper , we present our work on the detection of question-answer pairs in an email conversation for the task of email summarization .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
detection of question-answer pairs
email summarization
email conversation
detection of question-answer pairs

triple_list: [["detection of question-answer pairs", "Used_for", "email summarization"], ["email conversation", "Used_for", "detection of question-answer pairs"]]
pred: [('detection of question-answer pairs', 'Used_for', 'email summarization'), ('email conversation', 'Used_for', 'detection of question-answer pairs')]
trues: [['detection of question-answer pairs', 'Used_for', 'email summarization'], ['email conversation', 'Used_for', 'detection of question-answer pairs']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.32116788321167883, 0.2857142857142857, 0.302405498281787]
text: This paper outlines Plume as it currently exists and describes our detailed design for extending Plume to handle passives , relative clauses , and interrogatives in a general manner .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
Plume
passives
Plume
relative clauses

triple_list: [["Plume", "Used_for", "passives"], ["Plume", "Used_for", "relative clauses"]]
pred: [('Plume', 'Used_for', 'passives'), ('Plume', 'Used_for', 'relative clauses')]
trues: [['Plume', 'Used_for', 'passives'], ['Plume', 'Used_for', 'relative clauses'], ['Plume', 'Used_for', 'interrogatives'], ['passives', 'Conjunction', 'relative clauses'], ['relative clauses', 'Conjunction', 'interrogatives']]
metrics_sample: [1.0, 0.4, 0.5714285714285715]
metrics_current: [0.324455205811138, 0.28693790149892934, 0.3045454545454545]
text: The intentional structure captures the discourse-relevant purposes , expressed in each of the linguistic segments as well as relationships among them .
result: The candidate relations for this text are:
Feature_of
The candidate entities for this text are:
discourse-relevant purposes
intentional structure

triple_list: [["discourse-relevant purposes", "Feature_of", "intentional structure"]]
pred: [('discourse-relevant purposes', 'Feature_of', 'intentional structure')]
trues: [['intentional structure', 'Used_for', 'discourse-relevant purposes']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32367149758454106, 0.2863247863247863, 0.3038548752834467]
text: Utterance Verification ( UV ) is a critical function of an Automatic Speech Recognition ( ASR ) System working on real applications where spontaneous speech , out-of-vocabulary ( OOV ) words and acoustic noises are present .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
Utterance Verification ( UV )
Automatic Speech Recognition ( ASR ) System
spontaneous speech
Automatic Speech Recognition ( ASR ) System
spontaneous speech
out-of-vocabulary ( OOV ) words
out-of-vocabulary ( OOV ) words
Automatic Speech Recognition ( ASR ) System

triple_list: [["Utterance Verification ( UV )", "Used_for", "Automatic Speech Recognition ( ASR ) System"], ["spontaneous speech", "Used_for", "Automatic Speech Recognition ( ASR ) System"], ["spontaneous speech", "Conjunction", "out-of-vocabulary ( OOV ) words"], ["out-of-vocabulary ( OOV ) words", "Used_for", "Automatic Speech Recognition ( ASR ) System"]]
pred: [('Utterance Verification ( UV )', 'Used_for', 'Automatic Speech Recognition ( ASR ) System'), ('spontaneous speech', 'Used_for', 'Automatic Speech Recognition ( ASR ) System'), ('spontaneous speech', 'Conjunction', 'out-of-vocabulary ( OOV ) words'), ('out-of-vocabulary ( OOV ) words', 'Used_for', 'Automatic Speech Recognition ( ASR ) System')]
trues: [['Utterance Verification ( UV )', 'Hyponym_of', 'Automatic Speech Recognition ( ASR ) System']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32057416267942584, 0.2857142857142857, 0.30214205186020293]
text: Here we develop an approach for 1 distance that begins with an explicit and exactly distance-preserving embedding of the points into 2 2 .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
2 2
approach

triple_list: [["2 2", "Used_for", "approach"]]
pred: [('2 2', 'Used_for', 'approach')]
trues: [['approach', 'Used_for', '1 distance']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3198090692124105, 0.2851063829787234, 0.30146231721034866]
text: This distribution has numerous applications in computer Go , including serving as an efficient stand-alone Go player .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
distribution
computer Go
distribution
stand-alone Go player

triple_list: [["distribution", "Used_for", "computer Go"], ["distribution", "Used_for", "stand-alone Go player"]]
pred: [('distribution', 'Used_for', 'computer Go'), ('distribution', 'Used_for', 'stand-alone Go player')]
trues: [['distribution', 'Used_for', 'computer Go']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.32066508313539194, 0.28662420382165604, 0.30269058295964124]
text: We test our algorithm on the detection task and the viewpoint classification task by using '' car '' category from both the Savarese et al. 2007 and PASCAL VOC 2006 datasets .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Conjunction
Hyponym_of
Hyponym_of
The candidate entities for this text are:
algorithm
detection task
algorithm
viewpoint classification task
detection task
viewpoint classification task
'' car '' category
Savarese et al. 2007
'' car '' category
PASCAL VOC 2006 datasets
Savarese et al. 2007
PASCAL VOC 2006 datasets

triple_list: [["algorithm", "Used_for", "detection task"], ["algorithm", "Used_for", "viewpoint classification task"], ["detection task", "Used_for", "viewpoint classification task"], ["'' car '' category", "Conjunction", "Savarese et al. 2007"], ["'' car '' category", "Hyponym_of", "Savarese et al. 2007"], ["'' car '' category", "Hyponym_of", "PASCAL VOC 2006 datasets"]]
pred: [('algorithm', 'Used_for', 'detection task'), ('algorithm', 'Used_for', 'viewpoint classification task'), ('detection task', 'Used_for', 'viewpoint classification task'), ("'' car '' category", 'Conjunction', 'Savarese et al. 2007'), ("'' car '' category", 'Hyponym_of', 'Savarese et al. 2007'), ("'' car '' category", 'Hyponym_of', 'PASCAL VOC 2006 datasets')]
trues: [['algorithm', 'Used_for', 'detection task'], ['algorithm', 'Used_for', 'viewpoint classification task'], ['detection task', 'Conjunction', 'viewpoint classification task'], ['PASCAL VOC 2006 datasets', 'Evaluate_for', 'algorithm']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.32084309133489464, 0.28842105263157897, 0.30376940133037694]
text: Given an object model and a black-box measure of similarity between the model and candidate targets , we consider visual object tracking as a numerical optimization problem .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
object model
visual object tracking
black-box measure of similarity
visual object tracking

triple_list: [["object model", "Used_for", "visual object tracking"], ["black-box measure of similarity", "Used_for", "visual object tracking"]]
pred: [('object model', 'Used_for', 'visual object tracking'), ('black-box measure of similarity', 'Used_for', 'visual object tracking')]
trues: [['numerical optimization problem', 'Used_for', 'visual object tracking']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31934731934731936, 0.28781512605042014, 0.30276243093922645]
text: For example , nucleotides in a DNA sequence , children 's names in a given state and year , and text documents are all commonly modeled with multinomial distributions .
result: The candidate relations for this text are:
Used_for
Hyponym_of
Conjunction
Hyponym_of
Conjunction
Hyponym_of
The candidate entities for this text are:
multinomial distributions
nucleotides
nucleotides
DNA sequence
nucleotides
children 's names
children 's names
multinomial distributions
children 's names
text documents
text documents
multinomial distributions

triple_list: [["multinomial distributions", "Used_for", "nucleotides"], ["nucleotides", "Hyponym_of", "DNA sequence"], ["nucleotides", "Conjunction", "children 's names"], ["children 's names", "Hyponym_of", "multinomial distributions"], ["children 's names", "Conjunction", "text documents"], ["text documents", "Hyponym_of", "multinomial distributions"]]
pred: [('multinomial distributions', 'Used_for', 'nucleotides'), ('nucleotides', 'Hyponym_of', 'DNA sequence'), ('nucleotides', 'Conjunction', "children 's names"), ("children 's names", 'Hyponym_of', 'multinomial distributions'), ("children 's names", 'Conjunction', 'text documents'), ('text documents', 'Hyponym_of', 'multinomial distributions')]
trues: [['multinomial distributions', 'Used_for', 'nucleotides in a DNA sequence'], ['multinomial distributions', 'Used_for', 'text documents']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31494252873563217, 0.28661087866108786, 0.30010952902519167]
text: In this paper , we describe a search procedure for statistical machine translation ( MT ) based on dynamic programming ( DP ) .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
search procedure
statistical machine translation ( MT )
dynamic programming ( DP )
search procedure

triple_list: [["search procedure", "Used_for", "statistical machine translation ( MT )"], ["dynamic programming ( DP )", "Used_for", "search procedure"]]
pred: [('search procedure', 'Used_for', 'statistical machine translation ( MT )'), ('dynamic programming ( DP )', 'Used_for', 'search procedure')]
trues: [['search procedure', 'Used_for', 'statistical machine translation ( MT )'], ['dynamic programming ( DP )', 'Used_for', 'statistical machine translation ( MT )']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3157894736842105, 0.2875, 0.3009814612868048]
text: The result is a discrete motor control representation of the continuous pen motion , via the quantized levels of the model parameters .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
discrete motor control representation
continuous pen motion
quantized levels of the model parameters
discrete motor control representation

triple_list: [["discrete motor control representation", "Used_for", "continuous pen motion"], ["quantized levels of the model parameters", "Used_for", "discrete motor control representation"]]
pred: [('discrete motor control representation', 'Used_for', 'continuous pen motion'), ('quantized levels of the model parameters', 'Used_for', 'discrete motor control representation')]
trues: [['discrete motor control representation', 'Used_for', 'continuous pen motion']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.31662870159453305, 0.288981288981289, 0.30217391304347824]
text: We investigate several voting - and arbiter-based combination strategies over a diverse pool of unsupervised WSD systems .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
voting - and arbiter-based combination strategies
unsupervised WSD systems

triple_list: [["voting - and arbiter-based combination strategies", "Used_for", "unsupervised WSD systems"]]
pred: [('voting - and arbiter-based combination strategies', 'Used_for', 'unsupervised WSD systems')]
trues: [['voting - and arbiter-based combination strategies', 'Used_for', 'unsupervised WSD systems']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3181818181818182, 0.29045643153526973, 0.3036876355748373]
text: We also address a more general problem of matrix rank degeneration & non-isolated minima in the low-rank matrix optimization by using new type of regularizer which approximately enforces the or-thonormality of the learned matrix very efficiently .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
regularizer
low-rank matrix optimization
regularizer
matrix rank degeneration
regularizer
non-isolated minima

triple_list: [["regularizer", "Used_for", "low-rank matrix optimization"], ["regularizer", "Used_for", "matrix rank degeneration"], ["regularizer", "Used_for", "non-isolated minima"]]
pred: [('regularizer', 'Used_for', 'low-rank matrix optimization'), ('regularizer', 'Used_for', 'matrix rank degeneration'), ('regularizer', 'Used_for', 'non-isolated minima')]
trues: [['matrix rank degeneration', 'Conjunction', 'non-isolated minima'], ['matrix rank degeneration', 'Feature_of', 'low-rank matrix optimization'], ['non-isolated minima', 'Feature_of', 'low-rank matrix optimization'], ['regularizer', 'Used_for', 'low-rank matrix optimization'], ['regularizer', 'Used_for', 'or-thonormality'], ['or-thonormality', 'Feature_of', 'learned matrix']]
metrics_sample: [0.3333333333333333, 0.16666666666666666, 0.2222222222222222]
metrics_current: [0.3182844243792325, 0.2889344262295082, 0.30290010741138557]
text: Thus , our method can be applied with great benefit to language pairs for which only scarce resources are available .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
method
language pairs

triple_list: [["method", "Used_for", "language pairs"]]
pred: [('method', 'Used_for', 'language pairs')]
trues: [['scarce resources', 'Used_for', 'method']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31756756756756754, 0.2883435582822086, 0.3022508038585209]
text: In this paper , we explore geometric structures of 3D lines in ray space for improving light field triangulation and stereo matching .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
geometric structures of 3D lines
light field triangulation
geometric structures of 3D lines
stereo matching
ray space
geometric structures of 3D lines

triple_list: [["geometric structures of 3D lines", "Used_for", "light field triangulation"], ["geometric structures of 3D lines", "Used_for", "stereo matching"], ["ray space", "Used_for", "geometric structures of 3D lines"]]
pred: [('geometric structures of 3D lines', 'Used_for', 'light field triangulation'), ('geometric structures of 3D lines', 'Used_for', 'stereo matching'), ('ray space', 'Used_for', 'geometric structures of 3D lines')]
trues: [['geometric structures of 3D lines', 'Used_for', 'light field triangulation'], ['geometric structures of 3D lines', 'Used_for', 'stereo matching'], ['ray space', 'Feature_of', 'geometric structures of 3D lines'], ['light field triangulation', 'Conjunction', 'stereo matching']]
metrics_sample: [0.6666666666666666, 0.5, 0.5714285714285715]
metrics_current: [0.319910514541387, 0.29006085192697767, 0.30425531914893617]
text: The strong description and modeling properties of differential features make them useful tools that can be efficiently used as constraints for 3 -- D reconstruction .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
differential features
3 -- D reconstruction
differential features
3 -- D reconstruction

triple_list: [["differential features", "Used_for", "3 -- D reconstruction"], ["differential features", "Used_for", "3 -- D reconstruction"]]
pred: [('differential features', 'Used_for', '3 -- D reconstruction'), ('differential features', 'Used_for', '3 -- D reconstruction')]
trues: [['them', 'Used_for', '3 -- D reconstruction']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31919642857142855, 0.2894736842105263, 0.3036093418259023]
text: Our method has two major components : a ) a pattern extraction scheme for efficiently harvesting patterns of given size and shape from expert game records and b ) a Bayesian learning algorithm ( in two variants ) that learns a distribution over the values of a move given a board position based on the local pattern context .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
pattern extraction scheme
patterns
pattern extraction scheme
expert game records
Bayesian learning algorithm
move
Bayesian learning algorithm
distribution
distribution
move
board position
distribution

triple_list: [["pattern extraction scheme", "Used_for", "patterns"], ["pattern extraction scheme", "Used_for", "expert game records"], ["Bayesian learning algorithm", "Used_for", "move"], ["Bayesian learning algorithm", "Conjunction", "distribution"], ["distribution", "Used_for", "move"]]
pred: [('pattern extraction scheme', 'Used_for', 'patterns'), ('pattern extraction scheme', 'Used_for', 'expert game records'), ('Bayesian learning algorithm', 'Used_for', 'move'), ('Bayesian learning algorithm', 'Conjunction', 'distribution'), ('distribution', 'Used_for', 'move')]
trues: [['pattern extraction scheme', 'Part_of', 'method'], ['pattern extraction scheme', 'Conjunction', 'Bayesian learning algorithm'], ['Bayesian learning algorithm', 'Part_of', 'method']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31567328918322296, 0.28772635814889336, 0.30105263157894735]
text: There are four language pairs currently supported by GLOSSER : English-Bulgarian , English-Estonian , English-Hungarian and French-Dutch .
result: The candidate relations for this text are:
Hyponym_of
Conjunction
Hyponym_of
Conjunction
Hyponym_of
Conjunction
Hyponym_of
The candidate entities for this text are:
English-Bulgarian
GLOSSER
English-Bulgarian
English-Estonian
English-Estonian
GLOSSER
English-Estonian
English-Hungarian
English-Hungarian
GLOSSER
English-Hungarian
French-Dutch
French-Dutch
GLOSSER

triple_list: [["English-Bulgarian", "Hyponym_of", "GLOSSER"], ["English-Bulgarian", "Conjunction", "English-Estonian"], ["English-Estonian", "Hyponym_of", "GLOSSER"], ["English-Estonian", "Conjunction", "English-Hungarian"], ["English-Hungarian", "Hyponym_of", "GLOSSER"], ["English-Hungarian", "Conjunction", "French-Dutch"], ["French-Dutch", "Hyponym_of", "GLOSSER"]]
pred: [('English-Bulgarian', 'Hyponym_of', 'GLOSSER'), ('English-Bulgarian', 'Conjunction', 'English-Estonian'), ('English-Estonian', 'Hyponym_of', 'GLOSSER'), ('English-Estonian', 'Conjunction', 'English-Hungarian'), ('English-Hungarian', 'Hyponym_of', 'GLOSSER'), ('English-Hungarian', 'Conjunction', 'French-Dutch'), ('French-Dutch', 'Hyponym_of', 'GLOSSER')]
trues: [['language pairs', 'Used_for', 'GLOSSER'], ['English-Bulgarian', 'Hyponym_of', 'language pairs'], ['English-Bulgarian', 'Conjunction', 'English-Estonian'], ['English-Estonian', 'Hyponym_of', 'language pairs'], ['English-Estonian', 'Conjunction', 'English-Hungarian'], ['English-Hungarian', 'Hyponym_of', 'language pairs'], ['English-Hungarian', 'Conjunction', 'French-Dutch'], ['French-Dutch', 'Hyponym_of', 'language pairs']]
metrics_sample: [0.42857142857142855, 0.375, 0.39999999999999997]
metrics_current: [0.3173913043478261, 0.2891089108910891, 0.30259067357512953]
text: Inherent ambiguities in the computation of features are addressed by using a data-dependent bandwidth for density estimation using kernels .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
data-dependent bandwidth
density estimation
kernels
density estimation

triple_list: [["data-dependent bandwidth", "Used_for", "density estimation"], ["kernels", "Used_for", "density estimation"]]
pred: [('data-dependent bandwidth', 'Used_for', 'density estimation'), ('kernels', 'Used_for', 'density estimation')]
trues: [['ambiguities', 'Feature_of', 'computation of features'], ['data-dependent bandwidth', 'Used_for', 'ambiguities'], ['data-dependent bandwidth', 'Used_for', 'density estimation'], ['kernels', 'Used_for', 'density estimation']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3203463203463203, 0.2907662082514735, 0.3048403707518022]
text: We present results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
Bayesian Network
addressee identification
Bayesian Network
Naive Bayes classifiers
Naive Bayes classifiers
addressee identification
four-participants face-to-face meetings
addressee identification
Bayesian Network
Naive Bayes classifiers

triple_list: [["Bayesian Network", "Used_for", "addressee identification"], ["Bayesian Network", "Used_for", "Naive Bayes classifiers"], ["Naive Bayes classifiers", "Conjunction", "addressee identification"], ["four-participants face-to-face meetings", "Used_for", "addressee identification"], ["Bayesian Network", "Used_for", "Naive Bayes classifiers"]]
pred: [('Bayesian Network', 'Used_for', 'addressee identification'), ('Bayesian Network', 'Used_for', 'Naive Bayes classifiers'), ('Naive Bayes classifiers', 'Conjunction', 'addressee identification'), ('four-participants face-to-face meetings', 'Used_for', 'addressee identification'), ('Bayesian Network', 'Used_for', 'Naive Bayes classifiers')]
trues: [['Bayesian Network', 'Used_for', 'addressee identification in four-participants face-to-face meetings'], ['Naive Bayes classifiers', 'Used_for', 'addressee identification in four-participants face-to-face meetings'], ['Naive Bayes classifiers', 'Conjunction', 'Bayesian Network']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31759656652360513, 0.2890625, 0.30265848670756645]
text: We present a new method for detecting interest points using histogram information .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
method
detecting interest points
histogram information
method

triple_list: [["method", "Used_for", "detecting interest points"], ["histogram information", "Used_for", "method"]]
pred: [('method', 'Used_for', 'detecting interest points'), ('histogram information', 'Used_for', 'method')]
trues: [['method', 'Used_for', 'detecting interest points'], ['histogram information', 'Used_for', 'detecting interest points']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.31837606837606836, 0.2898832684824903, 0.3034623217922607]
text: We propose a multi-task end-to-end Joint Classification-Regression Recurrent Neural Network to better explore the action type and temporal localiza-tion information .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
multi-task end-to-end Joint Classification-Regression Recurrent Neural Network
action type
multi-task end-to-end Joint Classification-Regression Recurrent Neural Network
temporal localiza-tion information
action type
temporal localiza-tion information
Joint Classification-Regression Recurrent Neural Network
action type

triple_list: [["multi-task end-to-end Joint Classification-Regression Recurrent Neural Network", "Used_for", "action type"], ["multi-task end-to-end Joint Classification-Regression Recurrent Neural Network", "Used_for", "temporal localiza-tion information"], ["action type", "Conjunction", "temporal localiza-tion information"], ["Joint Classification-Regression Recurrent Neural Network", "Used_for", "action type"]]
pred: [('multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'Used_for', 'action type'), ('multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'Used_for', 'temporal localiza-tion information'), ('action type', 'Conjunction', 'temporal localiza-tion information'), ('Joint Classification-Regression Recurrent Neural Network', 'Used_for', 'action type')]
trues: [['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'Used_for', 'action type'], ['multi-task end-to-end Joint Classification-Regression Recurrent Neural Network', 'Used_for', 'temporal localiza-tion information'], ['action type', 'Conjunction', 'temporal localiza-tion information']]
metrics_sample: [0.75, 1.0, 0.8571428571428571]
metrics_current: [0.3220338983050847, 0.2940038684719536, 0.30738119312436807]
text: In this paper , a novel framework for machine transliteration/backtransliteration that allows us to carry out direct orthographical mapping ( DOM ) between two different languages is presented .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
framework
machine transliteration/backtransliteration
orthographical mapping ( DOM )
framework

triple_list: [["framework", "Used_for", "machine transliteration/backtransliteration"], ["orthographical mapping ( DOM )", "Used_for", "framework"]]
pred: [('framework', 'Used_for', 'machine transliteration/backtransliteration'), ('orthographical mapping ( DOM )', 'Used_for', 'framework')]
trues: [['framework', 'Used_for', 'machine transliteration/backtransliteration'], ['machine transliteration/backtransliteration', 'Used_for', 'direct orthographical mapping ( DOM )']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3227848101265823, 0.2947976878612717, 0.30815709969788524]
text: We evaluate several proposals for constraining them , basing our assessment on computational tractability and explanatory adequacy .
result: The candidate relations for this text are:
Evaluate_for
Conjunction
Evaluate_for
The candidate entities for this text are:
computational tractability
proposals
computational tractability
explanatory adequacy
explanatory adequacy
proposals

triple_list: [["computational tractability", "Evaluate_for", "proposals"], ["computational tractability", "Conjunction", "explanatory adequacy"], ["explanatory adequacy", "Evaluate_for", "proposals"]]
pred: [('computational tractability', 'Evaluate_for', 'proposals'), ('computational tractability', 'Conjunction', 'explanatory adequacy'), ('explanatory adequacy', 'Evaluate_for', 'proposals')]
trues: [['computational tractability and explanatory adequacy', 'Evaluate_for', 'them']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32075471698113206, 0.29423076923076924, 0.3069207622868606]
text: The approach is validated with real image data , and with experimental results from a CID/DRAM analog array prototype in 0.5 cents m CMOS .
result: The candidate relations for this text are:
Used_for
Evaluate_for
The candidate entities for this text are:
real image data
approach
CID/DRAM analog array prototype
approach

triple_list: [["real image data", "Used_for", "approach"], ["CID/DRAM analog array prototype", "Evaluate_for", "approach"]]
pred: [('real image data', 'Used_for', 'approach'), ('CID/DRAM analog array prototype', 'Evaluate_for', 'approach')]
trues: [['real image data', 'Evaluate_for', 'approach']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31941544885177453, 0.29366602687140114, 0.306]
text: Although the study of clustering is centered around an intuitively compelling goal , it has been very difficult to develop a unified framework for reasoning about it at a technical level , and profoundly diverse approaches to clustering abound in the research community .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
it
clustering

triple_list: [["it", "Used_for", "clustering"]]
pred: [('it', 'Used_for', 'clustering')]
trues: [['unified framework', 'Used_for', 'reasoning']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31875, 0.29310344827586204, 0.3053892215568862]
text: This model is meant to account for productive rules of interpretation which are inferred from the morpho-syntactic and semantic characteristics of the nominal constituents .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
model
productive rules of interpretation
morpho-syntactic and semantic characteristics
nominal constituents

triple_list: [["model", "Used_for", "productive rules of interpretation"], ["morpho-syntactic and semantic characteristics", "Used_for", "nominal constituents"]]
pred: [('model', 'Used_for', 'productive rules of interpretation'), ('morpho-syntactic and semantic characteristics', 'Used_for', 'nominal constituents')]
trues: [['model', 'Used_for', 'productive rules of interpretation'], ['morpho-syntactic and semantic characteristics', 'Used_for', 'productive rules of interpretation'], ['morpho-syntactic and semantic characteristics', 'Feature_of', 'nominal constituents']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.31950207468879666, 0.29333333333333333, 0.3058589870903674]
text: Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
MBR decoding
statistical MT
MBR decoding
loss functions

triple_list: [["MBR decoding", "Used_for", "statistical MT"], ["MBR decoding", "Used_for", "loss functions"]]
pred: [('MBR decoding', 'Used_for', 'statistical MT'), ('MBR decoding', 'Used_for', 'loss functions')]
trues: [['MBR decoding', 'Used_for', 'statistical MT'], ['MBR decoding', 'Used_for', 'loss functions']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.32231404958677684, 0.29601518026565465, 0.3086053412462908]
text: Our core motivation is that minimizing a weighted rank loss is a natural criterion for many problems in computer vision such as person re-identification .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
weighted rank loss
problems in computer vision
weighted rank loss
person re-identification

triple_list: [["weighted rank loss", "Used_for", "problems in computer vision"], ["weighted rank loss", "Used_for", "person re-identification"]]
pred: [('weighted rank loss', 'Used_for', 'problems in computer vision'), ('weighted rank loss', 'Used_for', 'person re-identification')]
trues: [['weighted rank loss', 'Used_for', 'computer vision'], ['weighted rank loss', 'Used_for', 'person re-identification'], ['person re-identification', 'Hyponym_of', 'computer vision']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.3230452674897119, 0.2962264150943396, 0.30905511811023617]
text: Our results show that MT evaluation techniques are able to produce useful features for paraphrase classification and to a lesser extent entailment .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
MT evaluation techniques
features
MT evaluation techniques
paraphrase classification
MT evaluation techniques
entailment
features
entailment

triple_list: [["MT evaluation techniques", "Used_for", "features"], ["MT evaluation techniques", "Used_for", "paraphrase classification"], ["MT evaluation techniques", "Conjunction", "entailment"], ["features", "Used_for", "entailment"]]
pred: [('MT evaluation techniques', 'Used_for', 'features'), ('MT evaluation techniques', 'Used_for', 'paraphrase classification'), ('MT evaluation techniques', 'Conjunction', 'entailment'), ('features', 'Used_for', 'entailment')]
trues: [['MT evaluation techniques', 'Used_for', 'features'], ['MT evaluation techniques', 'Used_for', 'paraphrase classification'], ['MT evaluation techniques', 'Used_for', 'entailment'], ['paraphrase classification', 'Conjunction', 'entailment']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.32448979591836735, 0.29775280898876405, 0.31054687499999994]
text: The result theoretically justifies the effectiveness of features in robust PCA .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
features
robust PCA

triple_list: [["features", "Used_for", "robust PCA"]]
pred: [('features', 'Used_for', 'robust PCA')]
trues: [['features', 'Feature_of', 'robust PCA']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32382892057026474, 0.297196261682243, 0.3099415204678363]
text: Extension to affine projection enables reconstruction without estimating cameras .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
affine projection
reconstruction

triple_list: [["affine projection", "Used_for", "reconstruction"]]
pred: [('affine projection', 'Used_for', 'reconstruction')]
trues: [['affine projection', 'Used_for', 'reconstruction']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3252032520325203, 0.29850746268656714, 0.31128404669260695]
text: Our approach is among the first to propose a generative proba-bilistic framework for 3D object categorization .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
generative proba-bilistic framework
3D object categorization

triple_list: [["generative proba-bilistic framework", "Used_for", "3D object categorization"]]
pred: [('generative proba-bilistic framework', 'Used_for', '3D object categorization')]
trues: [['generative proba-bilistic framework', 'Used_for', '3D object categorization']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3265720081135903, 0.29981378026070765, 0.312621359223301]
text: Therefore , the study presented in this paper may be useful in inspiring new designs of deep models for person re-identification .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
deep models
person re-identification

triple_list: [["deep models", "Used_for", "person re-identification"]]
pred: [('deep models', 'Used_for', 'person re-identification')]
trues: [['deep models', 'Used_for', 'person re-identification']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.32793522267206476, 0.30111524163568776, 0.313953488372093]
text: Specifically , we show how to incorporate a simple prior on the distribution of natural images into support vector machines .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
prior
support vector machines
prior
distribution of natural images

triple_list: [["prior", "Used_for", "support vector machines"], ["prior", "Used_for", "distribution of natural images"]]
pred: [('prior', 'Used_for', 'support vector machines'), ('prior', 'Used_for', 'distribution of natural images')]
trues: [['prior on the distribution of natural images', 'Used_for', 'support vector machines']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32661290322580644, 0.300556586270872, 0.3130434782608696]
text: We further demonstrate synthesis from a freely translating virtual camera .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
freely translating virtual camera
synthesis

triple_list: [["freely translating virtual camera", "Used_for", "synthesis"]]
pred: [('freely translating virtual camera', 'Used_for', 'synthesis')]
trues: [['translating virtual camera', 'Used_for', 'synthesis']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32595573440643866, 0.3, 0.31243972999035685]
text: The proposed detectors are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to rotation , illumination variation , and blur .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Conjunction
Used_for
Conjunction
Used_for
The candidate entities for this text are:
detectors
large-scale structures
detectors
distinctive textured patterns
detectors
invariance
invariance
large-scale structures
invariance
detectors
blur
detectors

triple_list: [["detectors", "Used_for", "large-scale structures"], ["detectors", "Used_for", "distinctive textured patterns"], ["detectors", "Used_for", "invariance"], ["invariance", "Conjunction", "large-scale structures"], ["invariance", "Used_for", "detectors"], ["blur", "Conjunction", "detectors"]]
pred: [('detectors', 'Used_for', 'large-scale structures'), ('detectors', 'Used_for', 'distinctive textured patterns'), ('detectors', 'Used_for', 'invariance'), ('invariance', 'Conjunction', 'large-scale structures'), ('invariance', 'Used_for', 'detectors'), ('blur', 'Conjunction', 'detectors')]
trues: [['detectors', 'Used_for', 'large-scale structures'], ['detectors', 'Used_for', 'distinctive textured patterns'], ['detectors', 'Used_for', 'rotation'], ['detectors', 'Used_for', 'illumination variation'], ['detectors', 'Used_for', 'blur'], ['large-scale structures', 'Conjunction', 'distinctive textured patterns'], ['rotation', 'Conjunction', 'illumination variation'], ['illumination variation', 'Conjunction', 'blur']]
metrics_sample: [0.3333333333333333, 0.25, 0.28571428571428575]
metrics_current: [0.3260437375745527, 0.29927007299270075, 0.3120837297811608]
text: A critical step in encoding sound for neuronal processing occurs when the analog pressure wave is coded into discrete nerve-action potentials .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
analog pressure wave
nerve-action potentials

triple_list: [["analog pressure wave", "Used_for", "nerve-action potentials"]]
pred: [('analog pressure wave', 'Used_for', 'nerve-action potentials')]
trues: [['encoding sound', 'Used_for', 'neuronal processing'], ['discrete nerve-action potentials', 'Used_for', 'analog pressure wave']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3253968253968254, 0.29818181818181816, 0.31119544592030357]
text: We present a novel model-based approach to recover the shapes and the poses of transparent objects from known motion .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
model-based approach
transparent objects
known motion
transparent objects

triple_list: [["model-based approach", "Used_for", "transparent objects"], ["known motion", "Used_for", "transparent objects"]]
pred: [('model-based approach', 'Used_for', 'transparent objects'), ('known motion', 'Used_for', 'transparent objects')]
trues: [['model-based approach', 'Used_for', 'shapes and the poses of transparent objects'], ['known motion', 'Used_for', 'shapes and the poses of transparent objects']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3241106719367589, 0.2971014492753623, 0.31001890359168244]
text: Using the IEMOCAP database , discrete ( categorical ) and continuous ( attribute ) emotional assessments evaluated by the actors and na  ve listeners are compared .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
IEMOCAP database
discrete ( categorical ) and continuous ( attribute ) emotional assessments

triple_list: [["IEMOCAP database", "Used_for", "discrete ( categorical ) and continuous ( attribute ) emotional assessments"]]
pred: [('IEMOCAP database', 'Used_for', 'discrete ( categorical ) and continuous ( attribute ) emotional assessments')]
trues: [['IEMOCAP database', 'Used_for', 'discrete ( categorical ) and continuous ( attribute ) emotional assessments']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3254437869822485, 0.298372513562387, 0.3113207547169811]
text: We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : SLIPPER , a rule-based learning algorithm , and TiMBL , a memory-based system .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
Used_for
Hyponym_of
Conjunction
Hyponym_of
The candidate entities for this text are:
predicates
clauses
predicates
features
features
dataset
SLIPPER
machine learning algorithms
SLIPPER
rule-based learning algorithm
TiMBL
machine learning algorithms
TiMBL
memory-based system

triple_list: [["predicates", "Used_for", "clauses"], ["predicates", "Used_for", "features"], ["features", "Conjunction", "dataset"], ["SLIPPER", "Used_for", "machine learning algorithms"], ["SLIPPER", "Used_for", "rule-based learning algorithm"], ["TiMBL", "Hyponym_of", "machine learning algorithms"], ["TiMBL", "Conjunction", "memory-based system"], ["memory-based system", "Hyponym_of", "machine learning algorithms"]]
pred: [('predicates', 'Used_for', 'clauses'), ('predicates', 'Used_for', 'features'), ('features', 'Conjunction', 'dataset'), ('SLIPPER', 'Used_for', 'machine learning algorithms'), ('SLIPPER', 'Used_for', 'rule-based learning algorithm'), ('TiMBL', 'Hyponym_of', 'machine learning algorithms'), ('TiMBL', 'Conjunction', 'memory-based system'), ('memory-based system', 'Hyponym_of', 'machine learning algorithms')]
trues: [['SLIPPER', 'Hyponym_of', 'rule-based learning algorithm'], ['rule-based learning algorithm', 'Part_of', 'machine learning algorithms'], ['rule-based learning algorithm', 'Compare', 'memory-based system'], ['TiMBL', 'Hyponym_of', 'memory-based system'], ['memory-based system', 'Part_of', 'machine learning algorithms']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32038834951456313, 0.2956989247311828, 0.30754892823858343]
text: Based on these results , we present an ECA that uses verbal and nonverbal grounding acts to update dialogue state .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
ECA
dialogue state
verbal and nonverbal grounding acts
ECA
verbal and nonverbal grounding acts
dialogue state

triple_list: [["ECA", "Used_for", "dialogue state"], ["verbal and nonverbal grounding acts", "Used_for", "ECA"], ["verbal and nonverbal grounding acts", "Used_for", "dialogue state"]]
pred: [('ECA', 'Used_for', 'dialogue state'), ('verbal and nonverbal grounding acts', 'Used_for', 'ECA'), ('verbal and nonverbal grounding acts', 'Used_for', 'dialogue state')]
trues: [['verbal and nonverbal grounding acts', 'Used_for', 'ECA'], ['verbal and nonverbal grounding acts', 'Used_for', 'dialogue state']]
metrics_sample: [0.6666666666666666, 1.0, 0.8]
metrics_current: [0.3223938223938224, 0.2982142857142857, 0.3098330241187385]
text: Sentence boundary detection in speech is important for enriching speech recognition output , making it easier for humans to read and downstream modules to process .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
Sentence boundary detection
speech
Sentence boundary detection
speech recognition output

triple_list: [["Sentence boundary detection", "Used_for", "speech"], ["Sentence boundary detection", "Used_for", "speech recognition output"]]
pred: [('Sentence boundary detection', 'Used_for', 'speech'), ('Sentence boundary detection', 'Used_for', 'speech recognition output')]
trues: [['Sentence boundary detection', 'Used_for', 'speech recognition output'], ['speech', 'Used_for', 'Sentence boundary detection']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3230769230769231, 0.298932384341637, 0.31053604436229204]
text: Extended CF grammars ( grammars with regular expressions at the right hand side ) can be parsed with a simple modification of the LR-parser for normal CF grammars .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
Extended CF grammars
LR-parser
LR-parser
normal CF grammars

triple_list: [["Extended CF grammars", "Used_for", "LR-parser"], ["LR-parser", "Used_for", "normal CF grammars"]]
pred: [('Extended CF grammars', 'Used_for', 'LR-parser'), ('LR-parser', 'Used_for', 'normal CF grammars')]
trues: [['regular expressions', 'Feature_of', 'grammars'], ['LR-parser', 'Used_for', 'Extended CF grammars'], ['LR-parser', 'Used_for', 'CF grammars']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3218390804597701, 0.2973451327433628, 0.3091076356945722]
text: This paper deals with the problem of generating the fundamental frequency ( F0 ) contour of speech from a text input for text-to-speech synthesis .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
text
fundamental frequency ( F0 ) contour
fundamental frequency ( F0 ) contour
text-to-speech synthesis

triple_list: [["text", "Used_for", "fundamental frequency ( F0 ) contour"], ["fundamental frequency ( F0 ) contour", "Used_for", "text-to-speech synthesis"]]
pred: [('text', 'Used_for', 'fundamental frequency ( F0 ) contour'), ('fundamental frequency ( F0 ) contour', 'Used_for', 'text-to-speech synthesis')]
trues: [['fundamental frequency ( F0 ) contour of speech', 'Used_for', 'text-to-speech synthesis'], ['text input', 'Used_for', 'fundamental frequency ( F0 ) contour of speech']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32061068702290074, 0.2962962962962963, 0.307974335472044]
text: A language learning experiment showed that assessors can differentiate native from non-native language essays in less than 100 words .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
language learning experiment
non-native language essays

triple_list: [["language learning experiment", "Used_for", "non-native language essays"]]
pred: [('language learning experiment', 'Used_for', 'non-native language essays')]
trues: [['language learning', 'Evaluate_for', 'assessors']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.32, 0.29577464788732394, 0.3074107959743824]
text: With the aid of a logic-based grammar formalism called extraposition grammars , Chat-80 translates English questions into the Prolog subset of logic .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
logic-based grammar formalism
Chat-80
extraposition grammars
Chat-80
Prolog subset of logic
Chat-80

triple_list: [["logic-based grammar formalism", "Used_for", "Chat-80"], ["extraposition grammars", "Used_for", "Chat-80"], ["Prolog subset of logic", "Used_for", "Chat-80"]]
pred: [('logic-based grammar formalism', 'Used_for', 'Chat-80'), ('extraposition grammars', 'Used_for', 'Chat-80'), ('Prolog subset of logic', 'Used_for', 'Chat-80')]
trues: [['extraposition grammars', 'Hyponym_of', 'logic-based grammar formalism'], ['extraposition grammars', 'Used_for', 'Chat-80']]
metrics_sample: [0.3333333333333333, 0.5, 0.4]
metrics_current: [0.32007575757575757, 0.2964912280701754, 0.3078324225865209]
text: FERRET utilizes a novel approach to Q/A known as predictive questioning which attempts to identify the questions ( and answers ) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
approach
Q/A
predictive questioning
approach

triple_list: [["approach", "Used_for", "Q/A"], ["predictive questioning", "Used_for", "approach"]]
pred: [('approach', 'Used_for', 'Q/A'), ('predictive questioning', 'Used_for', 'approach')]
trues: [['approach', 'Used_for', 'FERRET'], ['approach', 'Used_for', 'Q/A']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.32075471698113206, 0.2972027972027972, 0.30852994555353896]
text: We propose a convex problem to incorporate side information in robust PCA and show that the low rank matrix can be exactly recovered via the proposed method under certain conditions .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
convex problem
side information
side information
robust PCA

triple_list: [["convex problem", "Used_for", "side information"], ["side information", "Used_for", "robust PCA"]]
pred: [('convex problem', 'Used_for', 'side information'), ('side information', 'Used_for', 'robust PCA')]
trues: [['convex problem', 'Used_for', 'side information'], ['side information', 'Part_of', 'robust PCA'], ['method', 'Used_for', 'low rank matrix']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.32142857142857145, 0.29739130434782607, 0.30894308943089427]
text: We describe how to estimate these indicators and how to integrate them into a retrieval approach based on language models .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
indicators
retrieval approach
language models
retrieval approach
language models
indicators
them
retrieval approach

triple_list: [["indicators", "Used_for", "retrieval approach"], ["language models", "Used_for", "retrieval approach"], ["language models", "Conjunction", "indicators"], ["them", "Used_for", "retrieval approach"]]
pred: [('indicators', 'Used_for', 'retrieval approach'), ('language models', 'Used_for', 'retrieval approach'), ('language models', 'Conjunction', 'indicators'), ('them', 'Used_for', 'retrieval approach')]
trues: [['them', 'Part_of', 'retrieval approach'], ['language models', 'Used_for', 'them']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31902985074626866, 0.29636048526863085, 0.307277628032345]
text: We evaluate the demands that task dialogues collected using this technique , place upon a prototype Natural Language system .
result: The candidate relations for this text are:
Used_for
Evaluate_for
The candidate entities for this text are:
task dialogues
Natural Language system
Natural Language system
demands

triple_list: [["task dialogues", "Used_for", "Natural Language system"], ["Natural Language system", "Evaluate_for", "demands"]]
pred: [('task dialogues', 'Used_for', 'Natural Language system'), ('Natural Language system', 'Evaluate_for', 'demands')]
trues: [['task dialogues', 'Used_for', 'prototype Natural Language system'], ['technique', 'Used_for', 'task dialogues']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31784386617100374, 0.29533678756476683, 0.306177260519248]
text: The experimental tests are carried out on the Verbmobil task ( German-English , 8000-word vocabulary ) , which is a limited-domain spoken-language task .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Verbmobil task
limited-domain spoken-language task

triple_list: [["Verbmobil task", "Used_for", "limited-domain spoken-language task"]]
pred: [('Verbmobil task', 'Used_for', 'limited-domain spoken-language task')]
trues: [['Verbmobil task', 'Hyponym_of', 'limited-domain spoken-language task']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3172541743970315, 0.29482758620689653, 0.30563002680965146]
text: However , acquiring the necessary training data is typically a tedious and time consuming task that involves manual interaction , active sensing devices and/or synthetic scenes .
result: The candidate relations for this text are:
Used_for
Conjunction
Used_for
Conjunction
Used_for
The candidate entities for this text are:
manual interaction
training data
manual interaction
active sensing devices
active sensing devices
training data
active sensing devices
synthetic scenes
synthetic scenes
training data

triple_list: [["manual interaction", "Used_for", "training data"], ["manual interaction", "Conjunction", "active sensing devices"], ["active sensing devices", "Used_for", "training data"], ["active sensing devices", "Conjunction", "synthetic scenes"], ["synthetic scenes", "Used_for", "training data"]]
pred: [('manual interaction', 'Used_for', 'training data'), ('manual interaction', 'Conjunction', 'active sensing devices'), ('active sensing devices', 'Used_for', 'training data'), ('active sensing devices', 'Conjunction', 'synthetic scenes'), ('synthetic scenes', 'Used_for', 'training data')]
trues: [['manual interaction', 'Used_for', 'task'], ['manual interaction', 'Conjunction', 'active sensing devices'], ['active sensing devices', 'Used_for', 'task'], ['active sensing devices', 'Conjunction', 'synthetic scenes'], ['synthetic scenes', 'Used_for', 'task']]
metrics_sample: [0.4, 0.4, 0.4000000000000001]
metrics_current: [0.3180147058823529, 0.29572649572649573, 0.3064658990256865]
text: Second , based on the information resulting from the dictionary lookup stage , a set of rules is applied to the segmented strings in order to identify NE items .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
rules
NE items
segmented strings
rules

triple_list: [["rules", "Used_for", "NE items"], ["segmented strings", "Used_for", "rules"]]
pred: [('rules', 'Used_for', 'NE items'), ('segmented strings', 'Used_for', 'rules')]
trues: [['dictionary lookup stage', 'Used_for', 'rules'], ['rules', 'Used_for', 'NE items']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.31868131868131866, 0.29642248722316866, 0.30714916151809357]
text: This motor control representation enables successful word spotting and matching of cursive scripts .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
motor control representation
word spotting
motor control representation
cursive scripts

triple_list: [["motor control representation", "Used_for", "word spotting"], ["motor control representation", "Used_for", "cursive scripts"]]
pred: [('motor control representation', 'Used_for', 'word spotting'), ('motor control representation', 'Used_for', 'cursive scripts')]
trues: [['motor control representation', 'Used_for', 'word spotting'], ['motor control representation', 'Used_for', 'matching of cursive scripts'], ['word spotting', 'Conjunction', 'matching of cursive scripts']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.3193430656934307, 0.2966101694915254, 0.30755711775043937]
text: OA improved phase locking in the auditory nerve ( AN ) and raised ASR accuracy for features derived from AN fibers ( ANFs ) .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
OA
phase locking
OA
ASR accuracy

triple_list: [["OA", "Used_for", "phase locking"], ["OA", "Used_for", "ASR accuracy"]]
pred: [('OA', 'Used_for', 'phase locking'), ('OA', 'Used_for', 'ASR accuracy')]
trues: [['OA', 'Used_for', 'phase locking in the auditory nerve ( AN )'], ['OA', 'Used_for', 'features'], ['ASR accuracy', 'Evaluate_for', 'features'], ['AN fibers ( ANFs )', 'Used_for', 'features']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3181818181818182, 0.2946127946127946, 0.30594405594405594]
text: Turkish is an agglutinative language with word structures formed by productive affixations of derivational and inflectional suffixes to root words .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
Turkish
agglutinative language
productive affixations
word structures

triple_list: [["Turkish", "Used_for", "agglutinative language"], ["productive affixations", "Used_for", "word structures"]]
pred: [('Turkish', 'Used_for', 'agglutinative language'), ('productive affixations', 'Used_for', 'word structures')]
trues: [['Turkish', 'Hyponym_of', 'agglutinative language'], ['word structures', 'Feature_of', 'agglutinative language'], ['productive affixations of derivational and inflectional suffixes', 'Part_of', 'word structures']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3170289855072464, 0.2931323283082077, 0.30461270670147955]
text: We rigorously establish the correctness of the methodology and show by experimentation using LSH that it is competitive in practice with available alternatives .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
it
alternatives

triple_list: [["it", "Compare", "alternatives"]]
pred: [('it', 'Compare', 'alternatives')]
trues: [['it', 'Compare', 'alternatives']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3182640144665461, 0.29431438127090304, 0.3058210251954822]
text: A random modulation scheme produces near-Bernoulli statistics even for highly correlated inputs .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
random modulation scheme
highly correlated inputs

triple_list: [["random modulation scheme", "Used_for", "highly correlated inputs"]]
pred: [('random modulation scheme', 'Used_for', 'highly correlated inputs')]
trues: [['random modulation scheme', 'Used_for', 'near-Bernoulli statistics'], ['highly correlated inputs', 'Used_for', 'random modulation scheme']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3176895306859206, 0.29333333333333333, 0.30502599653379553]
text: In this paper We experimentally evaluate a trainable sentence planner for a spoken dialogue system by eliciting subjective human judgments .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
trainable sentence planner
spoken dialogue system
subjective human judgments
trainable sentence planner

triple_list: [["trainable sentence planner", "Used_for", "spoken dialogue system"], ["subjective human judgments", "Used_for", "trainable sentence planner"]]
pred: [('trainable sentence planner', 'Used_for', 'spoken dialogue system'), ('subjective human judgments', 'Used_for', 'trainable sentence planner')]
trues: [['trainable sentence planner', 'Used_for', 'spoken dialogue system'], ['subjective human judgments', 'Evaluate_for', 'trainable sentence planner']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3183453237410072, 0.29401993355481726, 0.305699481865285]
text: We propose a novel approach to associate objects across multiple PTZ cameras that can be used to perform camera handoff in wide-area surveillance scenarios .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
approach
objects
approach
camera handoff

triple_list: [["approach", "Used_for", "objects"], ["approach", "Used_for", "camera handoff"]]
pred: [('approach', 'Used_for', 'objects'), ('approach', 'Used_for', 'camera handoff')]
trues: [['approach', 'Used_for', 'camera handoff in wide-area surveillance scenarios']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3172043010752688, 0.2935323383084577, 0.30490956072351416]
text: Contrary to most of the existing mixture of viewpoints models , our model establishes explicit correspondences of parts across different viewpoints of the object class .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
model
mixture of viewpoints models

triple_list: [["model", "Used_for", "mixture of viewpoints models"]]
pred: [('model', 'Used_for', 'mixture of viewpoints models')]
trues: [['model', 'Compare', 'mixture of viewpoints models']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31663685152057247, 0.29304635761589404, 0.3043852106620808]
text: We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
Tree Adjoining Grammars
Head Grammars

triple_list: [["Tree Adjoining Grammars", "Compare", "Head Grammars"]]
pred: [('Tree Adjoining Grammars', 'Compare', 'Head Grammars')]
trues: [['Tree Adjoining Grammars', 'Hyponym_of', 'grammatical formalisms'], ['Tree Adjoining Grammars', 'Compare', 'Head Grammars'], ['Head Grammars', 'Hyponym_of', 'grammatical formalisms']]
metrics_sample: [1.0, 0.3333333333333333, 0.5]
metrics_current: [0.31785714285714284, 0.2932454695222405, 0.3050556983718937]
text: In particular , we make extensive use of Pustejovsky 's principles concerning the predicative information associated with nominals .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Pustejovsky 's principles
predicative information associated with nominals

triple_list: [["Pustejovsky 's principles", "Used_for", "predicative information associated with nominals"]]
pred: [("Pustejovsky 's principles", 'Used_for', 'predicative information associated with nominals')]
trues: [['nominals', 'Feature_of', 'predicative information']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3172905525846702, 0.29276315789473684, 0.3045337895637297]
text: We show promising results in both the detection and viewpoint classification tasks on these two challenging datasets .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
Conjunction
Evaluate_for
Evaluate_for
The candidate entities for this text are:
detection
tasks
detection
viewpoint classification tasks
detection
viewpoint classification tasks
detection
datasets
viewpoint classification tasks
datasets

triple_list: [["detection", "Evaluate_for", "tasks"], ["detection", "Evaluate_for", "viewpoint classification tasks"], ["detection", "Conjunction", "viewpoint classification tasks"], ["detection", "Evaluate_for", "datasets"], ["viewpoint classification tasks", "Evaluate_for", "datasets"]]
pred: [('detection', 'Evaluate_for', 'tasks'), ('detection', 'Evaluate_for', 'viewpoint classification tasks'), ('detection', 'Conjunction', 'viewpoint classification tasks'), ('detection', 'Evaluate_for', 'datasets'), ('viewpoint classification tasks', 'Evaluate_for', 'datasets')]
trues: [['datasets', 'Evaluate_for', 'detection and viewpoint classification tasks']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31448763250883394, 0.2922824302134647, 0.3029787234042553]
text: Both classifiers perform the best when conversational context and utterance features are combined with speaker 's gaze information .
result: The candidate relations for this text are:
Conjunction
Used_for
Used_for
The candidate entities for this text are:
conversational context
utterance features
conversational context
speaker 's gaze information
utterance features
speaker 's gaze information

triple_list: [["conversational context", "Conjunction", "utterance features"], ["conversational context", "Used_for", "speaker 's gaze information"], ["utterance features", "Used_for", "speaker 's gaze information"]]
pred: [('conversational context', 'Conjunction', 'utterance features'), ('conversational context', 'Used_for', "speaker 's gaze information"), ('utterance features', 'Used_for', "speaker 's gaze information")]
trues: [['conversational context', 'Used_for', 'classifiers'], ['conversational context', 'Conjunction', 'utterance features'], ['utterance features', 'Used_for', 'classifiers'], ["speaker 's gaze information", 'Used_for', 'classifiers'], ["speaker 's gaze information", 'Conjunction', 'utterance features']]
metrics_sample: [0.3333333333333333, 0.2, 0.25]
metrics_current: [0.3145869947275923, 0.2915309446254072, 0.30262045646661034]
text: This paper describes a method for incorporating priming into an incremental probabilistic parser .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
method
incorporating priming
incorporating priming
incremental probabilistic parser

triple_list: [["method", "Used_for", "incorporating priming"], ["incorporating priming", "Used_for", "incremental probabilistic parser"]]
pred: [('method', 'Used_for', 'incorporating priming'), ('incorporating priming', 'Used_for', 'incremental probabilistic parser')]
trues: [['method', 'Used_for', 'priming'], ['priming', 'Used_for', 'incremental probabilistic parser']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3134851138353765, 0.2905844155844156, 0.3016006739679865]
text: We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 video sequences with extreme illumination , pose and head motion variation .
result: The candidate relations for this text are:
Used_for
Evaluate_for
Evaluate_for
Conjunction
Evaluate_for
The candidate entities for this text are:
method
recognition system
individuals
recognition system
individuals
video sequences
video sequences
recognition system
pose
recognition system

triple_list: [["method", "Used_for", "recognition system"], ["individuals", "Evaluate_for", "recognition system"], ["individuals", "Evaluate_for", "video sequences"], ["video sequences", "Conjunction", "recognition system"], ["pose", "Evaluate_for", "recognition system"]]
pred: [('method', 'Used_for', 'recognition system'), ('individuals', 'Evaluate_for', 'recognition system'), ('individuals', 'Evaluate_for', 'video sequences'), ('video sequences', 'Conjunction', 'recognition system'), ('pose', 'Evaluate_for', 'recognition system')]
trues: [['method', 'Used_for', 'fully automatic recognition system'], ['video sequences', 'Evaluate_for', 'fully automatic recognition system'], ['illumination', 'Feature_of', 'video sequences'], ['illumination', 'Conjunction', 'pose'], ['pose', 'Feature_of', 'video sequences'], ['pose', 'Conjunction', 'head motion variation'], ['head motion variation', 'Feature_of', 'video sequences']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3107638888888889, 0.28731942215088285, 0.29858215179316094]
text: As a Monte Carlo approach , ASA stochastically samples the parameter space , in contrast to local deterministic search .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
Monte Carlo approach
local deterministic search

triple_list: [["Monte Carlo approach", "Compare", "local deterministic search"]]
pred: [('Monte Carlo approach', 'Compare', 'local deterministic search')]
trues: [['ASA', 'Hyponym_of', 'Monte Carlo approach'], ['ASA', 'Compare', 'local deterministic search']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31022530329289427, 0.2864, 0.29783693843594006]
text: Fast algorithms for nearest neighbor ( NN ) search have in large part focused on 2 distance .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
2 distance
Fast algorithms for nearest neighbor ( NN ) search

triple_list: [["2 distance", "Used_for", "Fast algorithms for nearest neighbor ( NN ) search"]]
pred: [('2 distance', 'Used_for', 'Fast algorithms for nearest neighbor ( NN ) search')]
trues: [['Fast algorithms', 'Used_for', 'nearest neighbor ( NN ) search']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3096885813148789, 0.28594249201277955, 0.2973421926910299]
text: A generative framework is used for learning a model that captures the relative position of parts within each of the discretized viewpoints .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
generative framework
model
generative framework
relative position of parts

triple_list: [["generative framework", "Used_for", "model"], ["generative framework", "Used_for", "relative position of parts"]]
pred: [('generative framework', 'Used_for', 'model'), ('generative framework', 'Used_for', 'relative position of parts')]
trues: [['generative framework', 'Used_for', 'model']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3103448275862069, 0.28708133971291866, 0.2982601491300746]
text: Many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
discrete data
modeling problems

triple_list: [["discrete data", "Used_for", "modeling problems"]]
pred: [('discrete data', 'Used_for', 'modeling problems')]
trues: [['discrete data', 'Used_for', 'modeling problems'], ['multinomial or categorical distributions', 'Used_for', 'modeling problems']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.31153184165232356, 0.287758346581876, 0.2991735537190083]
text: We introduce a method to accelerate the evaluation of object detection cascades with the help of a divide-and-conquer procedure in the space of candidate regions .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
method
evaluation of object detection cascades
divide-and-conquer procedure
evaluation of object detection cascades

triple_list: [["method", "Used_for", "evaluation of object detection cascades"], ["divide-and-conquer procedure", "Used_for", "evaluation of object detection cascades"]]
pred: [('method', 'Used_for', 'evaluation of object detection cascades'), ('divide-and-conquer procedure', 'Used_for', 'evaluation of object detection cascades')]
trues: [['method', 'Used_for', 'evaluation of object detection cascades'], ['divide-and-conquer procedure', 'Used_for', 'method'], ['space of candidate regions', 'Feature_of', 'divide-and-conquer procedure']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.31217838765008576, 0.2879746835443038, 0.29958847736625516]
text: Unlike other techniques that handle large outlier percentages , MINPRAN does not rely on a known error bound for the good data .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
techniques
MINPRAN

triple_list: [["techniques", "Compare", "MINPRAN"]]
pred: [('techniques', 'Compare', 'MINPRAN')]
trues: [['techniques', 'Used_for', 'large outlier percentages'], ['techniques', 'Compare', 'MINPRAN']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3133561643835616, 0.2886435331230284, 0.30049261083743845]
text: Joint matrix triangularization is often used for estimating the joint eigenstructure of a set M of matrices , with applications in signal processing and machine learning .
result: The candidate relations for this text are:
Used_for
Hyponym_of
Conjunction
Hyponym_of
The candidate entities for this text are:
Joint matrix triangularization
joint eigenstructure
signal processing
machine learning
signal processing
machine learning
signal processing
applications
machine learning
applications

triple_list: [["Joint matrix triangularization", "Used_for", "joint eigenstructure"], ["signal processing", "Hyponym_of", "machine learning"], ["signal processing", "Conjunction", "machine learning"], ["signal processing", "Hyponym_of", "applications"], ["machine learning", "Hyponym_of", "applications"]]
pred: [('Joint matrix triangularization', 'Used_for', 'joint eigenstructure'), ('signal processing', 'Hyponym_of', 'machine learning'), ('signal processing', 'Conjunction', 'machine learning'), ('signal processing', 'Hyponym_of', 'applications'), ('machine learning', 'Hyponym_of', 'applications')]
trues: [['Joint matrix triangularization', 'Used_for', 'joint eigenstructure'], ['joint eigenstructure', 'Used_for', 'signal processing'], ['joint eigenstructure', 'Used_for', 'machine learning'], ['signal processing', 'Conjunction', 'machine learning']]
metrics_sample: [0.4, 0.5, 0.4444444444444445]
metrics_current: [0.3140916808149406, 0.28996865203761757, 0.3015484922575387]
text: The system is implemented entirely in Prolog , a programming language based on logic .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Prolog
system

triple_list: [["Prolog", "Used_for", "system"]]
pred: [('Prolog', 'Used_for', 'system')]
trues: [['Prolog', 'Used_for', 'system'], ['Prolog', 'Hyponym_of', 'programming language'], ['logic', 'Used_for', 'programming language']]
metrics_sample: [1.0, 0.3333333333333333, 0.5]
metrics_current: [0.3152542372881356, 0.29017160686427457, 0.3021933387489846]
text: In previous work , we have developed hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers that integrate textual and prosodic knowledge sources for detecting sentence boundaries .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
hidden Markov model ( HMM )
detecting sentence boundaries
maximum entropy ( Maxent )
detecting sentence boundaries
maximum entropy ( Maxent )
hidden Markov model ( HMM )
textual and prosodic knowledge sources
detecting sentence boundaries
textual and prosodic knowledge sources
hidden Markov model ( HMM )

triple_list: [["hidden Markov model ( HMM )", "Used_for", "detecting sentence boundaries"], ["maximum entropy ( Maxent )", "Used_for", "detecting sentence boundaries"], ["maximum entropy ( Maxent )", "Conjunction", "hidden Markov model ( HMM )"], ["textual and prosodic knowledge sources", "Used_for", "detecting sentence boundaries"], ["textual and prosodic knowledge sources", "Used_for", "hidden Markov model ( HMM )"]]
pred: [('hidden Markov model ( HMM )', 'Used_for', 'detecting sentence boundaries'), ('maximum entropy ( Maxent )', 'Used_for', 'detecting sentence boundaries'), ('maximum entropy ( Maxent )', 'Conjunction', 'hidden Markov model ( HMM )'), ('textual and prosodic knowledge sources', 'Used_for', 'detecting sentence boundaries'), ('textual and prosodic knowledge sources', 'Used_for', 'hidden Markov model ( HMM )')]
trues: [['hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers', 'Used_for', 'detecting sentence boundaries'], ['textual and prosodic knowledge sources', 'Used_for', 'hidden Markov model ( HMM ) and maximum entropy ( Maxent ) classifiers']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3126050420168067, 0.2892690513219285, 0.30048465266558966]
text: Hitherto , smooth motion has been encouraged using a trajectory basis , yielding a hard combinatorial problem with time complexity growing exponentially in the number of frames .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
trajectory basis
smooth motion
trajectory basis
hard combinatorial problem

triple_list: [["trajectory basis", "Used_for", "smooth motion"], ["trajectory basis", "Used_for", "hard combinatorial problem"]]
pred: [('trajectory basis', 'Used_for', 'smooth motion'), ('trajectory basis', 'Used_for', 'hard combinatorial problem')]
trues: [['trajectory basis', 'Used_for', 'smooth motion'], ['time complexity', 'Evaluate_for', 'hard combinatorial problem']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3132328308207705, 0.289922480620155, 0.3011272141706924]
text: This model mimics the way in which images are processed in the visual pathway , rendering a feasible alternative for the implementation of early vision applications in standard technologies .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
model
early vision applications
standard technologies
early vision applications

triple_list: [["model", "Used_for", "early vision applications"], ["standard technologies", "Used_for", "early vision applications"]]
pred: [('model', 'Used_for', 'early vision applications'), ('standard technologies', 'Used_for', 'early vision applications')]
trues: [['visual pathway', 'Used_for', 'images']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3121869782971619, 0.2894736842105263, 0.3004016064257028]
text: An experimental evaluation of summarization quality shows a close correlation between the automatic parse-based evaluation and a manual evaluation of generated strings .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
The candidate entities for this text are:
automatic parse-based evaluation
summarization quality
manual evaluation of generated strings
summarization quality

triple_list: [["automatic parse-based evaluation", "Evaluate_for", "summarization quality"], ["manual evaluation of generated strings", "Evaluate_for", "summarization quality"]]
pred: [('automatic parse-based evaluation', 'Evaluate_for', 'summarization quality'), ('manual evaluation of generated strings', 'Evaluate_for', 'summarization quality')]
trues: [['summarization quality', 'Evaluate_for', 'automatic parse-based evaluation'], ['automatic parse-based evaluation', 'Compare', 'manual evaluation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31114808652246256, 0.28858024691358025, 0.29943955164131303]
text: It also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ACE relation major types .
result: The candidate relations for this text are:
Compare
Evaluate_for
Evaluate_for
The candidate entities for this text are:
method
dependency tree kernels
ACE relation major types
method
ACE relation major types
dependency tree kernels

triple_list: [["method", "Compare", "dependency tree kernels"], ["ACE relation major types", "Evaluate_for", "method"], ["ACE relation major types", "Evaluate_for", "dependency tree kernels"]]
pred: [('method', 'Compare', 'dependency tree kernels'), ('ACE relation major types', 'Evaluate_for', 'method'), ('ACE relation major types', 'Evaluate_for', 'dependency tree kernels')]
trues: [['method', 'Compare', 'dependency tree kernels']]
metrics_sample: [0.3333333333333333, 1.0, 0.5]
metrics_current: [0.31125827814569534, 0.2896764252696456, 0.30007980845969673]
text: This paper describes a novel system for acquiring adjectival subcategorization frames ( scfs ) and associated frequency information from English corpus data .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
system
adjectival subcategorization frames ( scfs )
system
frequency information
English corpus data
system

triple_list: [["system", "Used_for", "adjectival subcategorization frames ( scfs )"], ["system", "Used_for", "frequency information"], ["English corpus data", "Used_for", "system"]]
pred: [('system', 'Used_for', 'adjectival subcategorization frames ( scfs )'), ('system', 'Used_for', 'frequency information'), ('English corpus data', 'Used_for', 'system')]
trues: [['system', 'Used_for', 'acquiring adjectival subcategorization frames ( scfs )']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30971993410214166, 0.28923076923076924, 0.29912490055688146]
text: On the internal side , liaisons are established between elements of the text and the graph by using broadly available resources such as a LO-English or better a L0-UNL dictionary , a morphosyntactic parser of L0 , and a canonical graph2tree transformation .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Used_for
Conjunction
The candidate entities for this text are:
LO-English
liaisons
LO-English
elements of the text
L0-UNL dictionary
liaisons
L0-UNL dictionary
elements of the text
morphosyntactic parser of L0
liaisons
morphosyntactic parser of L0
elements of the text

triple_list: [["LO-English", "Used_for", "liaisons"], ["LO-English", "Used_for", "elements of the text"], ["L0-UNL dictionary", "Used_for", "liaisons"], ["L0-UNL dictionary", "Used_for", "elements of the text"], ["morphosyntactic parser of L0", "Conjunction", "elements of the text"]]
pred: [('LO-English', 'Used_for', 'liaisons'), ('LO-English', 'Used_for', 'elements of the text'), ('L0-UNL dictionary', 'Used_for', 'liaisons'), ('L0-UNL dictionary', 'Used_for', 'elements of the text'), ('morphosyntactic parser of L0', 'Conjunction', 'elements of the text')]
trues: [['resources', 'Used_for', 'liaisons'], ['LO-English or better a L0-UNL dictionary', 'Hyponym_of', 'resources'], ['LO-English or better a L0-UNL dictionary', 'Conjunction', 'morphosyntactic parser of L0'], ['morphosyntactic parser of L0', 'Hyponym_of', 'resources'], ['morphosyntactic parser of L0', 'Conjunction', 'canonical graph2tree transformation'], ['canonical graph2tree transformation', 'Hyponym_of', 'resources']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30718954248366015, 0.2865853658536585, 0.29652996845425866]
text: Online action detection goes one step further and is more challenging , which identifies the action type and localizes the action positions on the fly from the untrimmed stream .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Online action detection
action type

triple_list: [["Online action detection", "Used_for", "action type"]]
pred: [('Online action detection', 'Used_for', 'action type')]
trues: [['Online action detection', 'Used_for', 'action type'], ['Online action detection', 'Used_for', 'action positions'], ['action type', 'Conjunction', 'action positions'], ['untrimmed stream', 'Used_for', 'Online action detection']]
metrics_sample: [1.0, 0.25, 0.4]
metrics_current: [0.3083197389885807, 0.2863636363636364, 0.2969363707776905]
text: Thus , the limitations of the automatic metrics used within MT are also discussed in this regard .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
automatic metrics
MT

triple_list: [["automatic metrics", "Used_for", "MT"]]
pred: [('automatic metrics', 'Used_for', 'MT')]
trues: [['automatic metrics', 'Evaluate_for', 'MT']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30781758957654726, 0.2859304084720121, 0.29647058823529415]
text: Establishing a `` best '' correspondence between the '' UNL-tree + L0 '' and the '' MS-L0 structure '' , a lattice , may be done using the dictionary and trying to align the tree and the selected trajectory with as few crossing liaisons as possible .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
dictionary
correspondence
dictionary
tree
dictionary
trajectory

triple_list: [["dictionary", "Used_for", "correspondence"], ["dictionary", "Used_for", "tree"], ["dictionary", "Used_for", "trajectory"]]
pred: [('dictionary', 'Used_for', 'correspondence'), ('dictionary', 'Used_for', 'tree'), ('dictionary', 'Used_for', 'trajectory')]
trues: [['UNL-tree + L0', 'Conjunction', 'MS-L0 structure'], ['dictionary', 'Used_for', 'lattice']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3063209076175041, 0.2850678733031674, 0.29531250000000003]
text: We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars ( LFG ) to the domain of sentence condensation .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
ambiguity packing
application
stochastic disambiguation techniques
application
LFG
application

triple_list: [["ambiguity packing", "Used_for", "application"], ["stochastic disambiguation techniques", "Used_for", "application"], ["LFG", "Used_for", "application"]]
pred: [('ambiguity packing', 'Used_for', 'application'), ('stochastic disambiguation techniques', 'Used_for', 'application'), ('LFG', 'Used_for', 'application')]
trues: [['ambiguity packing and stochastic disambiguation techniques', 'Used_for', 'Lexical-Functional Grammars ( LFG )'], ['ambiguity packing and stochastic disambiguation techniques', 'Used_for', 'sentence condensation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30483870967741933, 0.28421052631578947, 0.29416342412451363]
text: We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
The candidate entities for this text are:
it
statistical machine translation system
it
extracted data

triple_list: [["it", "Evaluate_for", "statistical machine translation system"], ["it", "Evaluate_for", "extracted data"]]
pred: [('it', 'Evaluate_for', 'statistical machine translation system'), ('it', 'Evaluate_for', 'extracted data')]
trues: [['it', 'Used_for', 'statistical machine translation system']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30385852090032156, 0.28378378378378377, 0.29347826086956524]
text: Machine transliteration/back-transliteration plays an important role in many multilingual speech and language applications .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Machine transliteration/back-transliteration
multilingual speech and language applications

triple_list: [["Machine transliteration/back-transliteration", "Used_for", "multilingual speech and language applications"]]
pred: [('Machine transliteration/back-transliteration', 'Used_for', 'multilingual speech and language applications')]
trues: [['Machine transliteration/back-transliteration', 'Used_for', 'multilingual speech and language applications']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.30497592295345105, 0.2848575712143928, 0.29457364341085274]
text: We analyzed eye gaze , head nods and attentional focus in the context of a direction-giving task .
result: The candidate relations for this text are:
Conjunction
Feature_of
The candidate entities for this text are:
eye gaze
head nods
head nods
direction-giving task

triple_list: [["eye gaze", "Conjunction", "head nods"], ["head nods", "Feature_of", "direction-giving task"]]
pred: [('eye gaze', 'Conjunction', 'head nods'), ('head nods', 'Feature_of', 'direction-giving task')]
trues: [['eye gaze', 'Conjunction', 'head nods'], ['eye gaze', 'Part_of', 'direction-giving task'], ['head nods', 'Conjunction', 'attentional focus'], ['head nods', 'Part_of', 'direction-giving task'], ['attentional focus', 'Part_of', 'direction-giving task']]
metrics_sample: [0.5, 0.2, 0.28571428571428575]
metrics_current: [0.3056, 0.28422619047619047, 0.2945258288357749]
text: These methods diier from previous work in the area in two ways : the use of an action model formalism which is better suited to the needs of a re-active agent , and successful implementation of noise-handling mechanisms .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
action model formalism
re-active agent

triple_list: [["action model formalism", "Used_for", "re-active agent"]]
pred: [('action model formalism', 'Used_for', 're-active agent')]
trues: [['action model formalism', 'Used_for', 'methods'], ['action model formalism', 'Used_for', 're-active agent'], ['noise-handling mechanisms', 'Used_for', 'methods']]
metrics_sample: [1.0, 0.3333333333333333, 0.5]
metrics_current: [0.30670926517571884, 0.28444444444444444, 0.29515757109915447]
text: Background modeling is an important component of many vision systems .
result: The candidate relations for this text are:
Part_of
The candidate entities for this text are:
Background modeling
vision systems

triple_list: [["Background modeling", "Part_of", "vision systems"]]
pred: [('Background modeling', 'Part_of', 'vision systems')]
trues: [['Background modeling', 'Part_of', 'vision systems']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3078149920255183, 0.28550295857988167, 0.29623944742900993]
text: In order to perform an exhaustive comparison , we also evaluate a hand-crafted template-based generation component , two rule-based sentence planners , and two baseline sentence planners .
result: The candidate relations for this text are:
Evaluate_for
Conjunction
Evaluate_for
Conjunction
Evaluate_for
Conjunction
Evaluate_for
The candidate entities for this text are:
hand-crafted template-based generation component
exhaustive comparison
hand-crafted template-based generation component
rule-based sentence planners
rule-based sentence planners
exhaustive comparison
rule-based sentence planners
baseline sentence planners
baseline sentence planners
exhaustive comparison
baseline sentence planners
rule-based sentence planners

triple_list: [["hand-crafted template-based generation component", "Evaluate_for", "exhaustive comparison"], ["hand-crafted template-based generation component", "Conjunction", "rule-based sentence planners"], ["rule-based sentence planners", "Evaluate_for", "exhaustive comparison"], ["rule-based sentence planners", "Conjunction", "baseline sentence planners"], ["baseline sentence planners", "Evaluate_for", "exhaustive comparison"], ["baseline sentence planners", "Conjunction", "rule-based sentence planners"]]
pred: [('hand-crafted template-based generation component', 'Evaluate_for', 'exhaustive comparison'), ('hand-crafted template-based generation component', 'Conjunction', 'rule-based sentence planners'), ('rule-based sentence planners', 'Evaluate_for', 'exhaustive comparison'), ('rule-based sentence planners', 'Conjunction', 'baseline sentence planners'), ('baseline sentence planners', 'Evaluate_for', 'exhaustive comparison'), ('baseline sentence planners', 'Conjunction', 'rule-based sentence planners')]
trues: [['hand-crafted template-based generation component', 'Conjunction', 'rule-based sentence planners'], ['rule-based sentence planners', 'Conjunction', 'baseline sentence planners']]
metrics_sample: [0.3333333333333333, 1.0, 0.5]
metrics_current: [0.3080568720379147, 0.28761061946902655, 0.2974828375286041]
text: Our main result is a first-order upper bound on the distance between any approximate joint triangularizer of the matrices in M ' and any exact joint triangularizer of the matrices in M .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
first-order upper bound
distance

triple_list: [["first-order upper bound", "Used_for", "distance"]]
pred: [('first-order upper bound', 'Used_for', 'distance')]
trues: [['approximate joint triangularizer', 'Conjunction', 'exact joint triangularizer']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30757097791798105, 0.28718703976435933, 0.297029702970297]
text: We also found that OA is crucial for auditory processing by onset neurons ( ONs ) in the next neuronal stage , the auditory brainstem .
result: The candidate relations for this text are:
Used_for
Part_of
The candidate entities for this text are:
OA
auditory processing
auditory brainstem
ONs

triple_list: [["OA", "Used_for", "auditory processing"], ["auditory brainstem", "Part_of", "ONs"]]
pred: [('OA', 'Used_for', 'auditory processing'), ('auditory brainstem', 'Part_of', 'ONs')]
trues: [['OA', 'Used_for', 'auditory processing'], ['onset neurons ( ONs )', 'Used_for', 'OA']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3081761006289308, 0.2878120411160059, 0.2976461655277145]
text: We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
gray-level band-pass white noise patterns
robustness

triple_list: [["gray-level band-pass white noise patterns", "Used_for", "robustness"]]
pred: [('gray-level band-pass white noise patterns', 'Used_for', 'robustness')]
trues: [['robustness', 'Evaluate_for', 'gray-level band-pass white noise patterns'], ['indirect lighting', 'Feature_of', 'robustness'], ['indirect lighting', 'Conjunction', 'scene discontinuities'], ['scene discontinuities', 'Feature_of', 'robustness']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3076923076923077, 0.28613138686131384, 0.29652042360060515]
text: A method for producing such phrases from a word-aligned corpora is proposed .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
word-aligned corpora
method

triple_list: [["word-aligned corpora", "Used_for", "method"]]
pred: [('word-aligned corpora', 'Used_for', 'method')]
trues: [['method', 'Used_for', 'phrases'], ['word-aligned corpora', 'Evaluate_for', 'method']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3072100313479624, 0.2852983988355167, 0.2958490566037736]
text: In order to build robust automatic abstracting systems , there is a need for better training resources than are currently available .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
training resources
automatic abstracting systems

triple_list: [["training resources", "Used_for", "automatic abstracting systems"]]
pred: [('training resources', 'Used_for', 'automatic abstracting systems')]
trues: [['training resources', 'Used_for', 'automatic abstracting systems']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3082942097026604, 0.2863372093023256, 0.29691032403918616]
text: Full digital resolution is maintained even with low-resolution analog-to-digital conversion , owing to random statistics in the analog summation of binary products .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
analog-to-digital conversion
Full digital resolution

triple_list: [["analog-to-digital conversion", "Used_for", "Full digital resolution"]]
pred: [('analog-to-digital conversion', 'Used_for', 'Full digital resolution')]
trues: [['random statistics', 'Part_of', 'analog summation of binary products']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3078125, 0.28592162554426703, 0.29646350639578634]
text: We demonstrate our approach with multiple PTZ camera sequences in typical outdoor surveillance settings and show a comparison with state-of-the-art approaches .
result: The candidate relations for this text are:
Used_for
Used_for
Compare
The candidate entities for this text are:
approach
PTZ camera sequences
outdoor surveillance settings
approach
approach
state-of-the-art approaches

triple_list: [["approach", "Used_for", "PTZ camera sequences"], ["outdoor surveillance settings", "Used_for", "approach"], ["approach", "Compare", "state-of-the-art approaches"]]
pred: [('approach', 'Used_for', 'PTZ camera sequences'), ('outdoor surveillance settings', 'Used_for', 'approach'), ('approach', 'Compare', 'state-of-the-art approaches')]
trues: [['approach', 'Used_for', 'outdoor surveillance settings'], ['approach', 'Compare', 'state-of-the-art approaches'], ['multiple PTZ camera sequences', 'Used_for', 'approach']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.30793157076205285, 0.2861271676300578, 0.2966292134831461]
text: Human action recognition from well-segmented 3D skeleton data has been intensively studied and attracting an increasing attention .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
3D skeleton data
Human action recognition

triple_list: [["3D skeleton data", "Used_for", "Human action recognition"]]
pred: [('3D skeleton data', 'Used_for', 'Human action recognition')]
trues: [['well-segmented 3D skeleton data', 'Used_for', 'Human action recognition']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30745341614906835, 0.2857142857142857, 0.2961854899027674]
text: This paper describes the impact of using different-quality references on evaluation .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
different-quality references
evaluation

triple_list: [["different-quality references", "Used_for", "evaluation"]]
pred: [('different-quality references', 'Used_for', 'evaluation')]
trues: [['different-quality references', 'Used_for', 'evaluation']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3085271317829457, 0.28674351585014407, 0.29723674383868554]
text: Multi-layer perceptrons ( MLPs ) performed much better than standard Gaussian mixture models ( GMMs ) for both our ANF-based and ON-based auditory features .
result: The candidate relations for this text are:
Compare
Evaluate_for
Evaluate_for
The candidate entities for this text are:
Multi-layer perceptrons ( MLPs )
Gaussian mixture models ( GMMs )
ANF-based and ON-based auditory features
Multi-layer perceptrons ( MLPs )
ANF-based and ON-based auditory features
Gaussian mixture models ( GMMs )

triple_list: [["Multi-layer perceptrons ( MLPs )", "Compare", "Gaussian mixture models ( GMMs )"], ["ANF-based and ON-based auditory features", "Evaluate_for", "Multi-layer perceptrons ( MLPs )"], ["ANF-based and ON-based auditory features", "Evaluate_for", "Gaussian mixture models ( GMMs )"]]
pred: [('Multi-layer perceptrons ( MLPs )', 'Compare', 'Gaussian mixture models ( GMMs )'), ('ANF-based and ON-based auditory features', 'Evaluate_for', 'Multi-layer perceptrons ( MLPs )'), ('ANF-based and ON-based auditory features', 'Evaluate_for', 'Gaussian mixture models ( GMMs )')]
trues: [['Multi-layer perceptrons ( MLPs )', 'Compare', 'Gaussian mixture models ( GMMs )'], ['Multi-layer perceptrons ( MLPs )', 'Used_for', 'ANF-based and ON-based auditory features'], ['Gaussian mixture models ( GMMs )', 'Used_for', 'ANF-based and ON-based auditory features']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.30864197530864196, 0.28694404591104733, 0.2973977695167286]
text: The unique properties of tree-adjoining grammars ( TAG ) present a challenge for the application of TAGs beyond the limited confines of syntax , for instance , to the task of semantic interpretation or automatic translation of natural language .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Conjunction
The candidate entities for this text are:
tree-adjoining grammars ( TAG )
syntax
tree-adjoining grammars ( TAG )
semantic interpretation
tree-adjoining grammars ( TAG )
automatic translation of natural language
semantic interpretation
automatic translation of natural language

triple_list: [["tree-adjoining grammars ( TAG )", "Used_for", "syntax"], ["tree-adjoining grammars ( TAG )", "Used_for", "semantic interpretation"], ["tree-adjoining grammars ( TAG )", "Used_for", "automatic translation of natural language"], ["semantic interpretation", "Conjunction", "automatic translation of natural language"]]
pred: [('tree-adjoining grammars ( TAG )', 'Used_for', 'syntax'), ('tree-adjoining grammars ( TAG )', 'Used_for', 'semantic interpretation'), ('tree-adjoining grammars ( TAG )', 'Used_for', 'automatic translation of natural language'), ('semantic interpretation', 'Conjunction', 'automatic translation of natural language')]
trues: [['TAGs', 'Used_for', 'semantic interpretation'], ['TAGs', 'Used_for', 'automatic translation of natural language'], ['semantic interpretation', 'Conjunction', 'automatic translation of natural language']]
metrics_sample: [0.25, 0.3333333333333333, 0.28571428571428575]
metrics_current: [0.30828220858895705, 0.28714285714285714, 0.2973372781065089]
text: A statistical translation model is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the NIST evaluation metric .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
statistical translation model
phrases
statistical translation model
training method
phrases
training method
NIST evaluation metric
translation accuracy

triple_list: [["statistical translation model", "Used_for", "phrases"], ["statistical translation model", "Used_for", "training method"], ["phrases", "Conjunction", "training method"], ["NIST evaluation metric", "Used_for", "translation accuracy"]]
pred: [('statistical translation model', 'Used_for', 'phrases'), ('statistical translation model', 'Used_for', 'training method'), ('phrases', 'Conjunction', 'training method'), ('NIST evaluation metric', 'Used_for', 'translation accuracy')]
trues: [['statistical translation model', 'Used_for', 'phrases'], ['maximization of translation accuracy', 'Used_for', 'training method'], ['NIST evaluation metric', 'Evaluate_for', 'statistical translation model']]
metrics_sample: [0.25, 0.3333333333333333, 0.28571428571428575]
metrics_current: [0.3079268292682927, 0.28733997155049784, 0.2972774098601913]
text: This reflects contemporary methods for unsupervised object discovery from groups of images , and it allows us to define intuitive evaluation met-rics for our sets of segmentations based on the accurate and parsimonious delineation of scene objects .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
unsupervised object discovery
segmentations
evaluation met-rics
segmentations

triple_list: [["unsupervised object discovery", "Used_for", "segmentations"], ["evaluation met-rics", "Used_for", "segmentations"]]
pred: [('unsupervised object discovery', 'Used_for', 'segmentations'), ('evaluation met-rics', 'Used_for', 'segmentations')]
trues: [['contemporary methods', 'Used_for', 'unsupervised object discovery']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3069908814589666, 0.2869318181818182, 0.2966226138032306]
text: In this paper , we propose a new method for the modeling and subtraction of such scenes .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
method
modeling and subtraction of such scenes

triple_list: [["method", "Used_for", "modeling and subtraction of such scenes"]]
pred: [('method', 'Used_for', 'modeling and subtraction of such scenes')]
trues: [['method', 'Used_for', 'modeling and subtraction of such scenes']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.30804248861911987, 0.28794326241134754, 0.29765395894428154]
text: Indeed , automatic evaluations need high-quality data that allow the comparison of both automatic and human translations .
result: The candidate relations for this text are:
Used_for
Compare
The candidate entities for this text are:
high-quality data
automatic evaluations
automatic and human translations
high-quality data

triple_list: [["high-quality data", "Used_for", "automatic evaluations"], ["automatic and human translations", "Compare", "high-quality data"]]
pred: [('high-quality data', 'Used_for', 'automatic evaluations'), ('automatic and human translations', 'Compare', 'high-quality data')]
trues: [['high-quality data', 'Evaluate_for', 'automatic evaluations']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3071104387291982, 0.28753541076487255, 0.29700073152889545]
text: We present a new part-of-speech tagger that demonstrates the following ideas : ( i ) explicit use of both preceding and following tag contexts via a dependency network representation , ( ii ) broad use of lexical features , including jointly conditioning on multiple consecutive words , ( iii ) effective use of priors in conditional loglinear models , and ( iv ) fine-grained modeling of unknown word features .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
part-of-speech tagger
ideas
ideas
part-of-speech tagger
dependency network representation
ideas
lexical features
ideas
lexical features
jointly conditioning on multiple consecutive words
jointly conditioning on multiple consecutive words
ideas
priors
conditional loglinear models
priors

triple_list: [["part-of-speech tagger", "Used_for", "ideas"], ["ideas", "Used_for", "part-of-speech tagger"], ["dependency network representation", "Used_for", "ideas"], ["lexical features", "Used_for", "ideas"], ["lexical features", "Conjunction", "jointly conditioning on multiple consecutive words"], ["jointly conditioning on multiple consecutive words", "Used_for", "ideas"], ["priors", "Used_for", "conditional loglinear models"], ["priors", "Used_for", "ideas"]]
pred: [('part-of-speech tagger', 'Used_for', 'ideas'), ('ideas', 'Used_for', 'part-of-speech tagger'), ('dependency network representation', 'Used_for', 'ideas'), ('lexical features', 'Used_for', 'ideas'), ('lexical features', 'Conjunction', 'jointly conditioning on multiple consecutive words'), ('jointly conditioning on multiple consecutive words', 'Used_for', 'ideas'), ('priors', 'Used_for', 'conditional loglinear models'), ('priors', 'Used_for', 'ideas')]
trues: [['tag contexts', 'Used_for', 'part-of-speech tagger'], ['dependency network representation', 'Used_for', 'tag contexts'], ['lexical features', 'Used_for', 'part-of-speech tagger'], ['priors in conditional loglinear models', 'Used_for', 'part-of-speech tagger'], ['fine-grained modeling of unknown word features', 'Used_for', 'part-of-speech tagger']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3034379671150972, 0.28551336146272854, 0.2942028985507246]
text: We use novel priors , generate sparse and dense detection maps and our results show high detection rate with rejection to pathological motion and occlusion .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
priors
detection
detection maps
detection
detection maps
detection

triple_list: [["priors", "Used_for", "detection"], ["detection maps", "Used_for", "detection"], ["detection maps", "Used_for", "detection"]]
pred: [('priors', 'Used_for', 'detection'), ('detection maps', 'Used_for', 'detection'), ('detection maps', 'Used_for', 'detection')]
trues: [['priors', 'Used_for', 'sparse and dense detection maps'], ['pathological motion', 'Conjunction', 'occlusion']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3025335320417288, 0.2847124824684432, 0.29335260115606937]
text: First , it uses several kinds of dictionaries to segment and tag Japanese character strings .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
dictionaries
Japanese character strings
dictionaries
segment and tag Japanese character strings

triple_list: [["dictionaries", "Used_for", "Japanese character strings"], ["dictionaries", "Used_for", "segment and tag Japanese character strings"]]
pred: [('dictionaries', 'Used_for', 'Japanese character strings'), ('dictionaries', 'Used_for', 'segment and tag Japanese character strings')]
trues: [['dictionaries', 'Used_for', 'it'], ['dictionaries', 'Used_for', 'Japanese character strings']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3031203566121842, 0.2853146853146853, 0.2939481268011527]
text: On this challenging data set our system consistently demonstrated a nearly perfect recognition rate ( over 99.7 % on all three databases ) , significantly out-performing state-of-the-art commercial software and methods from the literature .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
Compare
Evaluate_for
Evaluate_for
The candidate entities for this text are:
recognition rate
system
recognition rate
databases
system
commercial software
system
methods
recognition rate
databases

triple_list: [["recognition rate", "Evaluate_for", "system"], ["recognition rate", "Evaluate_for", "databases"], ["system", "Compare", "commercial software"], ["system", "Evaluate_for", "methods"], ["recognition rate", "Evaluate_for", "databases"]]
pred: [('recognition rate', 'Evaluate_for', 'system'), ('recognition rate', 'Evaluate_for', 'databases'), ('system', 'Compare', 'commercial software'), ('system', 'Evaluate_for', 'methods'), ('recognition rate', 'Evaluate_for', 'databases')]
trues: [['data set', 'Evaluate_for', 'system'], ['system', 'Compare', 'commercial software'], ['system', 'Compare', 'methods'], ['recognition rate', 'Evaluate_for', 'system'], ['commercial software', 'Conjunction', 'methods']]
metrics_sample: [0.5, 0.4, 0.4444444444444445]
metrics_current: [0.30428360413589367, 0.2861111111111111, 0.29491768074445235]
text: This model was shown to be more appropriate for blind separation of natural speech signals .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
model
blind separation of natural speech signals

triple_list: [["model", "Used_for", "blind separation of natural speech signals"]]
pred: [('model', 'Used_for', 'blind separation of natural speech signals')]
trues: [['model', 'Used_for', 'blind separation of natural speech signals .']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30383480825958703, 0.2857142857142857, 0.29449606862044314]
text: We present a strong detector based on combining a set of weak detectors .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
weak detectors
detector

triple_list: [["weak detectors", "Used_for", "detector"]]
pred: [('weak detectors', 'Used_for', 'detector')]
trues: [['detectors', 'Used_for', 'detector']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30338733431516934, 0.2853185595567867, 0.2940756602426838]
text: Translations are produced by means of a beam-search decoder .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
beam-search decoder
Translations

triple_list: [["beam-search decoder", "Used_for", "Translations"]]
pred: [('beam-search decoder', 'Used_for', 'Translations')]
trues: [['beam-search decoder', 'Used_for', 'Translations']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3044117647058823, 0.2863070539419087, 0.29508196721311475]
text: For LR ( 0 ) grammars , our algorithm is closely related to the recursive ascent parsers recently discovered by Kruse-man Aretz [ 1 ] and Roberts [ 2 ] .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
algorithm
LR ( 0 ) grammars
recursive ascent parsers
LR ( 0 ) grammars

triple_list: [["algorithm", "Used_for", "LR ( 0 ) grammars"], ["recursive ascent parsers", "Used_for", "LR ( 0 ) grammars"]]
pred: [('algorithm', 'Used_for', 'LR ( 0 ) grammars'), ('recursive ascent parsers', 'Used_for', 'LR ( 0 ) grammars')]
trues: [['algorithm', 'Used_for', 'LR ( 0 ) grammars'], ['algorithm', 'Conjunction', 'recursive ascent parsers']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.30498533724340177, 0.2868965517241379, 0.2956645344705046]
text: We evaluate across two corpora ( conversational telephone speech and broadcast news speech ) on both human transcriptions and speech recognition output .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
Conjunction
Evaluate_for
Evaluate_for
The candidate entities for this text are:
corpora
human transcriptions
corpora
speech recognition output
human transcriptions
speech recognition output
conversational telephone speech
corpora
broadcast news speech
corpora

triple_list: [["corpora", "Evaluate_for", "human transcriptions"], ["corpora", "Evaluate_for", "speech recognition output"], ["human transcriptions", "Conjunction", "speech recognition output"], ["conversational telephone speech", "Evaluate_for", "corpora"], ["broadcast news speech", "Evaluate_for", "corpora"]]
pred: [('corpora', 'Evaluate_for', 'human transcriptions'), ('corpora', 'Evaluate_for', 'speech recognition output'), ('human transcriptions', 'Conjunction', 'speech recognition output'), ('conversational telephone speech', 'Evaluate_for', 'corpora'), ('broadcast news speech', 'Evaluate_for', 'corpora')]
trues: [['corpora', 'Evaluate_for', 'human transcriptions'], ['corpora', 'Evaluate_for', 'speech recognition output'], ['conversational telephone speech', 'Hyponym_of', 'corpora'], ['conversational telephone speech', 'Conjunction', 'broadcast news speech'], ['broadcast news speech', 'Hyponym_of', 'corpora'], ['human transcriptions', 'Conjunction', 'speech recognition output']]
metrics_sample: [0.6, 0.5, 0.5454545454545454]
metrics_current: [0.3071324599708879, 0.28864569083447333, 0.29760225669957685]
text: The fact that Turkish is an agglutinating free word order language presents a challenge for language theories .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Turkish
language theories

triple_list: [["Turkish", "Used_for", "language theories"]]
pred: [('Turkish', 'Used_for', 'language theories')]
trues: [['Turkish', 'Hyponym_of', 'agglutinating free word order language']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3066860465116279, 0.28825136612021857, 0.2971830985915493]
text: Labeled data is replaced by a few hand-crafted rules that encode basic syntactic knowledge .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
hand-crafted rules
syntactic knowledge

triple_list: [["hand-crafted rules", "Used_for", "syntactic knowledge"]]
pred: [('hand-crafted rules', 'Used_for', 'syntactic knowledge')]
trues: [['hand-crafted rules', 'Used_for', 'syntactic knowledge']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3076923076923077, 0.2892223738062756, 0.2981715893108298]
text: Kernel space embedding decouples the training and prediction costs from the data dimension and enables us to plug inarbitrary distance measures which are more natural for the features .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
Kernel space embedding
training and prediction costs
distance measures
Kernel space embedding

triple_list: [["Kernel space embedding", "Used_for", "training and prediction costs"], ["distance measures", "Used_for", "Kernel space embedding"]]
pred: [('Kernel space embedding', 'Used_for', 'training and prediction costs'), ('distance measures', 'Used_for', 'Kernel space embedding')]
trues: [['Kernel space embedding', 'Used_for', 'inarbitrary distance measures']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3068017366136035, 0.2888283378746594, 0.2975438596491228]
text: In this paper , we discuss language model adaptation methods given a word list and a raw corpus .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
The candidate entities for this text are:
language model adaptation methods
word list
language model adaptation methods
raw corpus
word list
raw corpus

triple_list: [["language model adaptation methods", "Used_for", "word list"], ["language model adaptation methods", "Used_for", "raw corpus"], ["word list", "Conjunction", "raw corpus"]]
pred: [('language model adaptation methods', 'Used_for', 'word list'), ('language model adaptation methods', 'Used_for', 'raw corpus'), ('word list', 'Conjunction', 'raw corpus')]
trues: [['word list', 'Used_for', 'language model adaptation methods'], ['word list', 'Conjunction', 'raw corpus'], ['raw corpus', 'Used_for', 'language model adaptation methods']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.3069164265129683, 0.28900949796472186, 0.29769392033542974]
text: Evaluation on the ACE 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous best-reported feature-based methods on the 24 ACE relation subtypes .
result: The candidate relations for this text are:
Evaluate_for
Compare
Evaluate_for
The candidate entities for this text are:
ACE 2003 corpus
convolution kernel
convolution kernel
feature-based methods
24 ACE relation subtypes
convolution kernel

triple_list: [["ACE 2003 corpus", "Evaluate_for", "convolution kernel"], ["convolution kernel", "Compare", "feature-based methods"], ["24 ACE relation subtypes", "Evaluate_for", "convolution kernel"]]
pred: [('ACE 2003 corpus', 'Evaluate_for', 'convolution kernel'), ('convolution kernel', 'Compare', 'feature-based methods'), ('24 ACE relation subtypes', 'Evaluate_for', 'convolution kernel')]
trues: [['ACE 2003 corpus', 'Evaluate_for', 'convolution kernel over parse trees'], ['feature-based methods', 'Compare', 'convolution kernel over parse trees']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30559540889526543, 0.2882273342354533, 0.2966573816155989]
text: Language resource quality is crucial in NLP .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Language resource quality
NLP

triple_list: [["Language resource quality", "Used_for", "NLP"]]
pred: [('Language resource quality', 'Used_for', 'NLP')]
trues: [['Language resource quality', 'Feature_of', 'NLP']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30515759312320917, 0.28783783783783784, 0.29624478442280944]
text: We demonstrate that for certain field structured extraction tasks , such as classified advertisements and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
field structured extraction tasks
classified advertisements
field structured extraction tasks
bibliographic citations
prior knowledge
models

triple_list: [["field structured extraction tasks", "Used_for", "classified advertisements"], ["field structured extraction tasks", "Used_for", "bibliographic citations"], ["prior knowledge", "Used_for", "models"]]
pred: [('field structured extraction tasks', 'Used_for', 'classified advertisements'), ('field structured extraction tasks', 'Used_for', 'bibliographic citations'), ('prior knowledge', 'Used_for', 'models')]
trues: [['classified advertisements', 'Hyponym_of', 'field structured extraction tasks'], ['classified advertisements', 'Conjunction', 'bibliographic citations'], ['bibliographic citations', 'Hyponym_of', 'field structured extraction tasks'], ['prior knowledge', 'Used_for', 'field structured extraction tasks']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30385164051355207, 0.2862903225806452, 0.29480968858131484]
text: Experiments on the PASCAL VOC 2006 dataset show an acceleration of more than 50 % by our method compared to standard cascade evaluation .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
Compare
The candidate entities for this text are:
PASCAL VOC 2006 dataset
method
PASCAL VOC 2006 dataset
cascades
method
cascades

triple_list: [["PASCAL VOC 2006 dataset", "Evaluate_for", "method"], ["PASCAL VOC 2006 dataset", "Evaluate_for", "cascades"], ["method", "Compare", "cascades"]]
pred: [('PASCAL VOC 2006 dataset', 'Evaluate_for', 'method'), ('PASCAL VOC 2006 dataset', 'Evaluate_for', 'cascades'), ('method', 'Compare', 'cascades')]
trues: [['PASCAL VOC 2006 dataset', 'Evaluate_for', 'method'], ['PASCAL VOC 2006 dataset', 'Evaluate_for', 'cascade evaluation'], ['cascade evaluation', 'Compare', 'method']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.3039772727272727, 0.28647925033467203, 0.2949689869055824]
text: The perception of transparent objects from images is known to be a very hard problem in vision .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
images
perception of transparent objects

triple_list: [["images", "Used_for", "perception of transparent objects"]]
pred: [('images', 'Used_for', 'perception of transparent objects')]
trues: [['images', 'Used_for', 'perception of transparent objects']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3049645390070922, 0.2874331550802139, 0.2959394356503785]
text: In practice , the current deep embedding methods use the Euclidean distance for the training and test .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
Euclidean distance
deep embedding methods
Euclidean distance
training and test

triple_list: [["Euclidean distance", "Used_for", "deep embedding methods"], ["Euclidean distance", "Used_for", "training and test"]]
pred: [('Euclidean distance', 'Used_for', 'deep embedding methods'), ('Euclidean distance', 'Used_for', 'training and test')]
trues: [['Euclidean distance', 'Used_for', 'deep embedding methods']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3055162659123055, 0.2883845126835781, 0.2967032967032967]
text: The combined approach is formulated in the framework of Bayesian statistics to account for the robust-ness requirement in image understanding .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
Bayesian statistics
approach
Bayesian statistics
image understanding

triple_list: [["Bayesian statistics", "Used_for", "approach"], ["Bayesian statistics", "Used_for", "image understanding"]]
pred: [('Bayesian statistics', 'Used_for', 'approach'), ('Bayesian statistics', 'Used_for', 'image understanding')]
trues: [['approach', 'Used_for', 'robust-ness requirement in image understanding'], ['Bayesian statistics', 'Used_for', 'approach']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.306064880112835, 0.28894806924101196, 0.29726027397260274]
text: Our approach is based on the iterative deformation of a 3 -- D surface mesh to minimize an objective function .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
3 -- D surface mesh
approach

triple_list: [["3 -- D surface mesh", "Used_for", "approach"]]
pred: [('3 -- D surface mesh', 'Used_for', 'approach')]
trues: [['iterative deformation of a 3 -- D surface mesh', 'Used_for', 'approach'], ['iterative deformation of a 3 -- D surface mesh', 'Used_for', 'objective function']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3056338028169014, 0.28818061088977426, 0.2966507177033493]
text: MINPRAN , a new robust operator , nds good ts in data sets where more than 50 % of the points are outliers .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
MINPRAN
robust operator

triple_list: [["MINPRAN", "Used_for", "robust operator"]]
pred: [('MINPRAN', 'Used_for', 'robust operator')]
trues: [['MINPRAN', 'Hyponym_of', 'robust operator']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30520393811533053, 0.28779840848806365, 0.2962457337883959]
text: Among other experiments , we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the KITTI2012 dataset by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data .
result: The candidate relations for this text are:
Used_for
Evaluate_for
Evaluate_for
Evaluate_for
The candidate entities for this text are:
approach
confidence measures
KITTI2012 dataset
confidence measures
laser ground truth data
confidence measures
laser ground truth data
approach

triple_list: [["approach", "Used_for", "confidence measures"], ["KITTI2012 dataset", "Evaluate_for", "confidence measures"], ["laser ground truth data", "Evaluate_for", "confidence measures"], ["laser ground truth data", "Evaluate_for", "approach"]]
pred: [('approach', 'Used_for', 'confidence measures'), ('KITTI2012 dataset', 'Evaluate_for', 'confidence measures'), ('laser ground truth data', 'Evaluate_for', 'confidence measures'), ('laser ground truth data', 'Evaluate_for', 'approach')]
trues: [['approach', 'Used_for', 'learned confidence measures'], ['KITTI2012 dataset', 'Evaluate_for', 'learned confidence measures'], ['automatically generated training data', 'Used_for', 'them'], ['laser ground truth data', 'Compare', 'automatically generated training data']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3034965034965035, 0.2862796833773087, 0.29463679565512557]
text: A new algorithm is proposed for novel view generation in one-to-one teleconferencing applications .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
algorithm
novel view generation

triple_list: [["algorithm", "Used_for", "novel view generation"]]
pred: [('algorithm', 'Used_for', 'novel view generation')]
trues: [['algorithm', 'Used_for', 'novel view generation'], ['novel view generation', 'Used_for', 'one-to-one teleconferencing applications']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.30446927374301674, 0.2868421052631579, 0.2953929539295393]
text: In both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .
result: The candidate relations for this text are:
Used_for
Compare
Used_for
The candidate entities for this text are:
unsupervised methods
domains
unsupervised methods
supervised methods
supervised methods
labeled examples

triple_list: [["unsupervised methods", "Used_for", "domains"], ["unsupervised methods", "Compare", "supervised methods"], ["supervised methods", "Used_for", "labeled examples"]]
pred: [('unsupervised methods', 'Used_for', 'domains'), ('unsupervised methods', 'Compare', 'supervised methods'), ('supervised methods', 'Used_for', 'labeled examples')]
trues: [['unsupervised methods', 'Compare', 'supervised methods'], ['accuracies', 'Evaluate_for', 'unsupervised methods'], ['accuracies', 'Evaluate_for', 'supervised methods'], ['unlabeled examples', 'Used_for', 'unsupervised methods'], ['labeled examples', 'Used_for', 'supervised methods'], ['labeled data', 'Used_for', 'semi-supervised methods']]
metrics_sample: [0.3333333333333333, 0.16666666666666666, 0.2222222222222222]
metrics_current: [0.3045897079276773, 0.2859007832898172, 0.2949494949494949]
text: Some of the extracts were expert human translations , others were machine translation outputs .
result: The candidate relations for this text are:
Conjunction
The candidate entities for this text are:
expert human translations
machine translation outputs

triple_list: [["expert human translations", "Conjunction", "machine translation outputs"]]
pred: [('expert human translations', 'Conjunction', 'machine translation outputs')]
trues: [['machine translation outputs', 'Conjunction', 'expert human translations']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30416666666666664, 0.28552803129074317, 0.2945527908540686]
text: A model-based approach to on-line cursive handwriting analysis and recognition is presented and evaluated .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
model-based approach
on-line cursive handwriting analysis and recognition

triple_list: [["model-based approach", "Used_for", "on-line cursive handwriting analysis and recognition"]]
pred: [('model-based approach', 'Used_for', 'on-line cursive handwriting analysis and recognition')]
trues: [['model-based approach', 'Used_for', 'on-line cursive handwriting analysis and recognition']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.30513176144244103, 0.2864583333333333, 0.2955003357958361]
text: This poster paper describes a full scale two-level morphological description ( Karttunen , 1983 ; Koskenniemi , 1983 ) of Turkish word structures .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
full scale two-level morphological description
Turkish word structures

triple_list: [["full scale two-level morphological description", "Used_for", "Turkish word structures"]]
pred: [('full scale two-level morphological description', 'Used_for', 'Turkish word structures')]
trues: [['full scale two-level morphological description', 'Used_for', 'Turkish word structures']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3060941828254848, 0.28738621586475943, 0.29644533869885986]
text: Examples are given that demonstrate the robustness of the new algorithm to spatial and temporal artefacts for long stereo video streams .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
long stereo video streams
algorithm

triple_list: [["long stereo video streams", "Used_for", "algorithm"]]
pred: [('long stereo video streams', 'Used_for', 'algorithm')]
trues: [['robustness', 'Evaluate_for', 'algorithm'], ['algorithm', 'Used_for', 'spatial and temporal artefacts'], ['spatial and temporal artefacts', 'Used_for', 'long stereo video streams']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30567081604426005, 0.2862694300518135, 0.2956521739130435]
text: In this paper , we study the problem of online action detection from the streaming skeleton data .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
streaming skeleton data
online action detection

triple_list: [["streaming skeleton data", "Used_for", "online action detection"]]
pred: [('streaming skeleton data', 'Used_for', 'online action detection')]
trues: [['streaming skeleton data', 'Used_for', 'online action detection']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.30662983425414364, 0.2871927554980595, 0.29659318637274545]
text: In this paper , we present an unlexicalized parser for German which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2 , higher than previously reported results on the NEGRA corpus .
result: The candidate relations for this text are:
Used_for
Evaluate_for
Evaluate_for
Compare
The candidate entities for this text are:
unlexicalized parser
German
labelled bracket F-score
unlexicalized parser
NEGRA corpus
unlexicalized parser
unlexicalized parser
smoothing and suffix analysis

triple_list: [["unlexicalized parser", "Used_for", "German"], ["labelled bracket F-score", "Evaluate_for", "unlexicalized parser"], ["NEGRA corpus", "Evaluate_for", "unlexicalized parser"], ["unlexicalized parser", "Compare", "smoothing and suffix analysis"]]
pred: [('unlexicalized parser', 'Used_for', 'German'), ('labelled bracket F-score', 'Evaluate_for', 'unlexicalized parser'), ('NEGRA corpus', 'Evaluate_for', 'unlexicalized parser'), ('unlexicalized parser', 'Compare', 'smoothing and suffix analysis')]
trues: [['unlexicalized parser', 'Used_for', 'German'], ['smoothing', 'Used_for', 'unlexicalized parser'], ['smoothing', 'Conjunction', 'suffix analysis'], ['suffix analysis', 'Used_for', 'unlexicalized parser'], ['labelled bracket F-score', 'Evaluate_for', 'unlexicalized parser'], ['NEGRA corpus', 'Evaluate_for', 'unlexicalized parser']]
metrics_sample: [0.75, 0.5, 0.6]
metrics_current: [0.3090659340659341, 0.2888318356867779, 0.298606502986065]
text: To our knowledge , this is the first a posteriori bound for joint matrix decomposition .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
a posteriori bound
joint matrix decomposition

triple_list: [["a posteriori bound", "Used_for", "joint matrix decomposition"]]
pred: [('a posteriori bound', 'Used_for', 'joint matrix decomposition')]
trues: [['posteriori bound', 'Used_for', 'joint matrix decomposition']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30864197530864196, 0.28846153846153844, 0.2982107355864811]
text: We show that the trainable sentence planner performs better than the rule-based systems and the baselines , and as well as the hand-crafted system .
result: The candidate relations for this text are:
Compare
Evaluate_for
Evaluate_for
Evaluate_for
Conjunction
Evaluate_for
The candidate entities for this text are:
trainable sentence planner
rule-based systems
rule-based systems
baselines
baselines
hand-crafted system
baselines
hand-crafted system
hand-crafted system
trainable sentence planner

triple_list: [["trainable sentence planner", "Compare", "rule-based systems"], ["rule-based systems", "Evaluate_for", "baselines"], ["baselines", "Evaluate_for", "hand-crafted system"], ["baselines", "Conjunction", "hand-crafted system"], ["hand-crafted system", "Evaluate_for", "trainable sentence planner"]]
pred: [('trainable sentence planner', 'Compare', 'rule-based systems'), ('rule-based systems', 'Evaluate_for', 'baselines'), ('baselines', 'Evaluate_for', 'hand-crafted system'), ('baselines', 'Conjunction', 'hand-crafted system'), ('hand-crafted system', 'Evaluate_for', 'trainable sentence planner')]
trues: [['trainable sentence planner', 'Compare', 'rule-based systems'], ['trainable sentence planner', 'Compare', 'baselines'], ['trainable sentence planner', 'Compare', 'hand-crafted system'], ['rule-based systems', 'Conjunction', 'baselines'], ['baselines', 'Conjunction', 'hand-crafted system']]
metrics_sample: [0.4, 0.4, 0.4000000000000001]
metrics_current: [0.30926430517711173, 0.289171974522293, 0.2988808426596445]
text: Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results , proposing an objective measure for evaluating feature vector quality .
result: The candidate relations for this text are:
Used_for
Evaluate_for
The candidate entities for this text are:
distributional word feature vectors
word similarity
objective measure
feature vector quality

triple_list: [["distributional word feature vectors", "Used_for", "word similarity"], ["objective measure", "Evaluate_for", "feature vector quality"]]
pred: [('distributional word feature vectors', 'Used_for', 'word similarity'), ('objective measure', 'Evaluate_for', 'feature vector quality')]
trues: [['semantic criterion', 'Evaluate_for', 'distributional word feature vectors'], ['distributional word feature vectors', 'Used_for', 'word similarity'], ['measure', 'Evaluate_for', 'feature vector quality']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.30978260869565216, 0.2893401015228426, 0.2992125984251968]
text: A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
tool
linguistic annotation of scfs
corpus data
tool

triple_list: [["tool", "Used_for", "linguistic annotation of scfs"], ["corpus data", "Used_for", "tool"]]
pred: [('tool', 'Used_for', 'linguistic annotation of scfs'), ('corpus data', 'Used_for', 'tool')]
trues: [['tool', 'Used_for', 'linguistic annotation of scfs'], ['training and test data', 'Used_for', 'subcategorization acquisition']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.31029810298102983, 0.289873417721519, 0.2997382198952879]
text: We show how this can efficiently be combined with random-projection based methods for 2 NN search , such as locality-sensitive hashing ( LSH ) or random projection trees .
result: The candidate relations for this text are:
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
random-projection based methods
2 NN search
locality-sensitive hashing ( LSH )
random projection trees
locality-sensitive hashing ( LSH )
2 NN search
random projection trees
2 NN search

triple_list: [["random-projection based methods", "Used_for", "2 NN search"], ["locality-sensitive hashing ( LSH )", "Conjunction", "random projection trees"], ["locality-sensitive hashing ( LSH )", "Used_for", "2 NN search"], ["random projection trees", "Used_for", "2 NN search"]]
pred: [('random-projection based methods', 'Used_for', '2 NN search'), ('locality-sensitive hashing ( LSH )', 'Conjunction', 'random projection trees'), ('locality-sensitive hashing ( LSH )', 'Used_for', '2 NN search'), ('random projection trees', 'Used_for', '2 NN search')]
trues: [['this', 'Conjunction', 'random-projection based methods'], ['random-projection based methods', 'Used_for', 'NN search'], ['locality-sensitive hashing ( LSH )', 'Hyponym_of', 'random-projection based methods'], ['locality-sensitive hashing ( LSH )', 'Conjunction', 'random projection trees'], ['random projection trees', 'Hyponym_of', 'random-projection based methods']]
metrics_sample: [0.25, 0.2, 0.22222222222222224]
metrics_current: [0.30997304582210244, 0.2893081761006289, 0.29928432010409894]
text: This paper presents a phrase-based statistical machine translation method , based on non-contiguous phrases , i.e. phrases with gaps .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
phrase-based statistical machine translation method
phrase-based statistical machine translation
non-contiguous phrases
phrase-based statistical machine translation

triple_list: [["phrase-based statistical machine translation method", "Used_for", "phrase-based statistical machine translation"], ["non-contiguous phrases", "Used_for", "phrase-based statistical machine translation"]]
pred: [('phrase-based statistical machine translation method', 'Used_for', 'phrase-based statistical machine translation'), ('non-contiguous phrases', 'Used_for', 'phrase-based statistical machine translation')]
trues: [['non-contiguous phrases', 'Used_for', 'phrase-based statistical machine translation method']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30913978494623656, 0.2889447236180904, 0.29870129870129875]
text: The surface realizations of morphological constructions are constrained and modified by a number of phonetic rules such as vowel harmony .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
phonetic rules
surface realizations of morphological constructions

triple_list: [["phonetic rules", "Used_for", "surface realizations of morphological constructions"]]
pred: [('phonetic rules', 'Used_for', 'surface realizations of morphological constructions')]
trues: [['phonetic rules', 'Used_for', 'surface realizations of morphological constructions'], ['vowel harmony', 'Hyponym_of', 'phonetic rules']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3100671140939597, 0.2894736842105263, 0.29941672067401165]
text: This posterior encodes sparse se-lectional preferences between a head word and its dependents .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
posterior
sparse se-lectional preferences

triple_list: [["posterior", "Used_for", "sparse se-lectional preferences"]]
pred: [('posterior', 'Used_for', 'sparse se-lectional preferences')]
trues: [['posterior', 'Used_for', 'sparse se-lectional preferences']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3109919571045576, 0.2903629536921151, 0.3003236245954693]
text: While Plume is well adapted to simple declarative and imperative utterances , it handles passives , relative clauses and interrogatives in an ad hoc manner leading to patchy syntactic coverage .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Plume
declarative and imperative utterances

triple_list: [["Plume", "Used_for", "declarative and imperative utterances"]]
pred: [('Plume', 'Used_for', 'declarative and imperative utterances')]
trues: [['Plume', 'Used_for', 'declarative and imperative utterances'], ['it', 'Used_for', 'passives'], ['it', 'Used_for', 'relative clauses'], ['it', 'Used_for', 'interrogatives'], ['passives', 'Conjunction', 'relative clauses'], ['relative clauses', 'Conjunction', 'interrogatives']]
metrics_sample: [1.0, 0.16666666666666666, 0.2857142857142857]
metrics_current: [0.3119143239625167, 0.2894409937888199, 0.3002577319587629]
text: In contrast , object pose estimation using these approaches has received relatively less attention .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
approaches
object pose estimation

triple_list: [["approaches", "Compare", "object pose estimation"]]
pred: [('approaches', 'Compare', 'object pose estimation')]
trues: [['approaches', 'Used_for', 'object pose estimation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3114973262032086, 0.2890818858560794, 0.2998712998712999]
text: Our approach to the Multi-lingual Evaluation Task ( MET ) for Japanese text is to consider the given task as a morphological analysis problem in Japanese .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
approach
Multi-lingual Evaluation Task ( MET )
Japanese text
approach

triple_list: [["approach", "Used_for", "Multi-lingual Evaluation Task ( MET )"], ["Japanese text", "Used_for", "approach"]]
pred: [('approach', 'Used_for', 'Multi-lingual Evaluation Task ( MET )'), ('Japanese text', 'Used_for', 'approach')]
trues: [['approach', 'Used_for', 'Multi-lingual Evaluation Task ( MET )'], ['Multi-lingual Evaluation Task ( MET )', 'Used_for', 'Japanese text'], ['morphological analysis problem', 'Used_for', 'task'], ['Japanese', 'Used_for', 'morphological analysis problem']]
metrics_sample: [0.5, 0.25, 0.3333333333333333]
metrics_current: [0.312, 0.28888888888888886, 0.30000000000000004]
text: In this paper , we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
approach
semantic label maps
images
semantic label maps

triple_list: [["approach", "Used_for", "semantic label maps"], ["images", "Used_for", "semantic label maps"]]
pred: [('approach', 'Used_for', 'semantic label maps'), ('images', 'Used_for', 'semantic label maps')]
trues: [['approach', 'Used_for', 'pixel-accurate semantic label maps'], ['pixel-accurate semantic label maps', 'Used_for', 'images'], ['images', 'Part_of', 'modern computer games']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31117021276595747, 0.2878228782287823, 0.2990415335463259]
text: To recover from these inevitable tracking failures , we consider object detection as a global optimization problem and solve it via Adaptive Simulated Annealing ( ASA ) , a method that avoids becoming trapped at local modes and is much faster than exhaustive search .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
object detection
global optimization problem
Adaptive Simulated Annealing ( ASA )
object detection
method
Adaptive Simulated Annealing ( ASA )

triple_list: [["object detection", "Used_for", "global optimization problem"], ["Adaptive Simulated Annealing ( ASA )", "Used_for", "object detection"], ["method", "Used_for", "Adaptive Simulated Annealing ( ASA )"]]
pred: [('object detection', 'Used_for', 'global optimization problem'), ('Adaptive Simulated Annealing ( ASA )', 'Used_for', 'object detection'), ('method', 'Used_for', 'Adaptive Simulated Annealing ( ASA )')]
trues: [['global optimization problem', 'Used_for', 'object detection'], ['Adaptive Simulated Annealing ( ASA )', 'Used_for', 'it'], ['method', 'Compare', 'exhaustive search']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30993377483443707, 0.2867647058823529, 0.2978994271164863]
text: These mattes are implied by a set of hypothesized object boundary fragments in the scene .
result: The candidate relations for this text are:
Part_of
The candidate entities for this text are:
hypothesized object boundary fragments
mattes

triple_list: [["hypothesized object boundary fragments", "Part_of", "mattes"]]
pred: [('hypothesized object boundary fragments', 'Part_of', 'mattes')]
trues: [['hypothesized object boundary fragments', 'Used_for', 'mattes']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30952380952380953, 0.2864137086903305, 0.29752066115702486]
text: This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
convolution kernel
syntactic structure information
convolution kernel
relation extraction

triple_list: [["convolution kernel", "Used_for", "syntactic structure information"], ["convolution kernel", "Used_for", "relation extraction"]]
pred: [('convolution kernel', 'Used_for', 'syntactic structure information'), ('convolution kernel', 'Used_for', 'relation extraction')]
trues: [['convolution kernel over parse trees', 'Used_for', 'syntactic structure information'], ['syntactic structure information', 'Used_for', 'relation extraction']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3087071240105541, 0.2857142857142857, 0.2967660114140774]
text: Such a triangulation provides a piecewise-linear interpolant useful for light field super-resolution .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
triangulation
light field super-resolution

triple_list: [["triangulation", "Used_for", "light field super-resolution"]]
pred: [('triangulation', 'Used_for', 'light field super-resolution')]
trues: [['triangulation', 'Used_for', 'piecewise-linear interpolant'], ['piecewise-linear interpolant', 'Used_for', 'light field super-resolution']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.308300395256917, 0.2850182704019488, 0.29620253164556964]
text: It is demonstrated qualitatively and quantitatively on a dataset of scenes and is suitable for current work in unsupervised object discovery without top-down knowledge .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
It
unsupervised object discovery
It
top-down knowledge

triple_list: [["It", "Used_for", "unsupervised object discovery"], ["It", "Used_for", "top-down knowledge"]]
pred: [('It', 'Used_for', 'unsupervised object discovery'), ('It', 'Used_for', 'top-down knowledge')]
trues: [['It', 'Used_for', 'unsupervised object discovery'], ['dataset of scenes', 'Evaluate_for', 'It']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3088042049934297, 0.28554070473876064, 0.2967171717171717]
text: We explored possible ways to obtain a compact lexicon , consistent with CCG principles , from a treebank which is an order of magnitude smaller than Penn WSJ .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
treebank
compact lexicon

triple_list: [["treebank", "Used_for", "compact lexicon"]]
pred: [('treebank', 'Used_for', 'compact lexicon')]
trues: [['compact lexicon', 'Part_of', 'treebank'], ['treebank', 'Compare', 'Penn WSJ']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3083989501312336, 0.28484848484848485, 0.2961562696912413]
text: The experiments show that the system is able to detect scf types with 70 % precision and 66 % recall rate .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
The candidate entities for this text are:
precision
system
recall rate
system

triple_list: [["precision", "Evaluate_for", "system"], ["recall rate", "Evaluate_for", "system"]]
pred: [('precision', 'Evaluate_for', 'system'), ('recall rate', 'Evaluate_for', 'system')]
trues: [['precision', 'Evaluate_for', 'system'], ['precision', 'Conjunction', 'recall'], ['recall', 'Evaluate_for', 'system']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.3089005235602094, 0.28502415458937197, 0.2964824120603015]
text: Experiments on the TREC Blog track test set show that both groups of credibility indicators significantly improve retrieval effectiveness ; the best performance is achieved when combining them .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
The candidate entities for this text are:
TREC Blog track test set
credibility indicators
retrieval effectiveness
credibility indicators

triple_list: [["TREC Blog track test set", "Evaluate_for", "credibility indicators"], ["retrieval effectiveness", "Evaluate_for", "credibility indicators"]]
pred: [('TREC Blog track test set', 'Evaluate_for', 'credibility indicators'), ('retrieval effectiveness', 'Evaluate_for', 'credibility indicators')]
trues: [['TREC Blog track test set', 'Evaluate_for', 'credibility indicators'], ['retrieval effectiveness', 'Evaluate_for', 'credibility indicators']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.31070496083550914, 0.28674698795180725, 0.2982456140350877]
text: Our experiments clearly indicate the potential of this dynamic representation for complete cursive handwriting recognition .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
dynamic representation
complete cursive handwriting recognition
complete cursive handwriting recognition
dynamic representation

triple_list: [["dynamic representation", "Used_for", "complete cursive handwriting recognition"], ["complete cursive handwriting recognition", "Used_for", "dynamic representation"]]
pred: [('dynamic representation', 'Used_for', 'complete cursive handwriting recognition'), ('complete cursive handwriting recognition', 'Used_for', 'dynamic representation')]
trues: [['dynamic representation', 'Used_for', 'cursive handwriting recognition']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3098958333333333, 0.28640192539109505, 0.29768605378361473]
text: It would also be effective as a move selector and move sorter for game tree search and as a training tool for Go players .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
It
move selector
It
move sorter
move selector
move sorter
It
training tool

triple_list: [["It", "Used_for", "move selector"], ["It", "Used_for", "move sorter"], ["move selector", "Conjunction", "move sorter"], ["It", "Used_for", "training tool"]]
pred: [('It', 'Used_for', 'move selector'), ('It', 'Used_for', 'move sorter'), ('move selector', 'Conjunction', 'move sorter'), ('It', 'Used_for', 'training tool')]
trues: [['It', 'Used_for', 'move selector'], ['It', 'Used_for', 'move sorter'], ['It', 'Used_for', 'training tool'], ['move selector', 'Conjunction', 'move sorter'], ['move selector', 'Used_for', 'game tree search'], ['move sorter', 'Used_for', 'game tree search'], ['training tool', 'Used_for', 'Go players']]
metrics_sample: [1.0, 0.5714285714285714, 0.7272727272727273]
metrics_current: [0.3134715025906736, 0.28878281622911695, 0.30062111801242236]
text: We integrate a spoken language understanding system with intelligent mobile agents that mediate between users and information sources .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
The candidate entities for this text are:
spoken language understanding system
mobile agents
intelligent mobile agents
users
intelligent mobile agents
information sources
information sources
mobile agents

triple_list: [["spoken language understanding system", "Used_for", "mobile agents"], ["intelligent mobile agents", "Used_for", "users"], ["intelligent mobile agents", "Conjunction", "information sources"], ["information sources", "Used_for", "mobile agents"]]
pred: [('spoken language understanding system', 'Used_for', 'mobile agents'), ('intelligent mobile agents', 'Used_for', 'users'), ('intelligent mobile agents', 'Conjunction', 'information sources'), ('information sources', 'Used_for', 'mobile agents')]
trues: [['intelligent mobile agents', 'Part_of', 'spoken language understanding system']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3118556701030928, 0.28843861740166865, 0.2996904024767802]
text: This paper presents a novel representation for three-dimensional objects in terms of affine-invariant image patches and their spatial relationships .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
representation
three-dimensional objects
affine-invariant image patches
representation

triple_list: [["representation", "Used_for", "three-dimensional objects"], ["affine-invariant image patches", "Used_for", "representation"]]
pred: [('representation', 'Used_for', 'three-dimensional objects'), ('affine-invariant image patches', 'Used_for', 'representation')]
trues: [['representation', 'Used_for', 'three-dimensional objects'], ['affine-invariant image patches', 'Feature_of', 'three-dimensional objects'], ['spatial relationships', 'Feature_of', 'affine-invariant image patches']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.31233933161953725, 0.28859857482185275, 0.3]
text: The two main contributions of this paper are : i ) a new type of three-plane graph for dense-stereo dynamic-programming , that encourages correct occlusion labeling ; ii ) a compact geometric derivation for novel-view synthesis by direct projection of the minimum-cost surface .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
three-plane graph
dense-stereo dynamic-programming
geometric derivation
novel-view synthesis
direct projection
minimum-cost surface

triple_list: [["three-plane graph", "Used_for", "dense-stereo dynamic-programming"], ["geometric derivation", "Used_for", "novel-view synthesis"], ["direct projection", "Used_for", "minimum-cost surface"]]
pred: [('three-plane graph', 'Used_for', 'dense-stereo dynamic-programming'), ('geometric derivation', 'Used_for', 'novel-view synthesis'), ('direct projection', 'Used_for', 'minimum-cost surface')]
trues: [['three-plane graph', 'Used_for', 'dense-stereo dynamic-programming'], ['dense-stereo dynamic-programming', 'Used_for', 'occlusion labeling'], ['compact geometric derivation', 'Used_for', 'novel-view synthesis'], ['direct projection of the minimum-cost surface', 'Used_for', 'compact geometric derivation']]
metrics_sample: [0.3333333333333333, 0.25, 0.28571428571428575]
metrics_current: [0.31241997439180536, 0.28841607565011823, 0.299938537185003]
text: We provide a unified account of sentence-level and text-level anaphora within the framework of a dependency-based grammar model .
result: The candidate relations for this text are:
Part_of
The candidate entities for this text are:
sentence-level anaphora
text-level anaphora

triple_list: [["sentence-level anaphora", "Part_of", "text-level anaphora"]]
pred: [('sentence-level anaphora', 'Part_of', 'text-level anaphora')]
trues: [['dependency-based grammar model', 'Used_for', 'sentence-level and text-level anaphora']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31202046035805625, 0.28807556080283353, 0.2995702885205648]
text: From this point of view , selecting suitable positive ( i.e. intra-class ) training samples within a local range is critical for training the CNN embedding , especially when the data has large intra-class variations .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
positive ( i.e. intra-class ) training samples
CNN embedding

triple_list: [["positive ( i.e. intra-class ) training samples", "Used_for", "CNN embedding"]]
pred: [('positive ( i.e. intra-class ) training samples', 'Used_for', 'CNN embedding')]
trues: [['intra-class variations', 'Feature_of', 'data']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3116219667943806, 0.28773584905660377, 0.299202942979767]
text: To associate a sequence of the Fujisaki-model parameters with a text input based on statistical learning , this paper proposes extending this model to a context-dependent one .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
statistical learning
Fujisaki-model parameters
statistical learning
text input

triple_list: [["statistical learning", "Used_for", "Fujisaki-model parameters"], ["statistical learning", "Used_for", "text input"]]
pred: [('statistical learning', 'Used_for', 'Fujisaki-model parameters'), ('statistical learning', 'Used_for', 'text input')]
trues: [['text input', 'Used_for', 'Fujisaki-model parameters'], ['statistical learning', 'Used_for', 'Fujisaki-model parameters']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.31210191082802546, 0.28823529411764703, 0.2996941896024464]
text: This paper highlights a particular class of miscommunication -- reference problems -- by describing a case study and techniques for avoiding failures of reference .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
case study
techniques

triple_list: [["case study", "Used_for", "techniques"]]
pred: [('case study', 'Used_for', 'techniques')]
trues: [['reference problems', 'Hyponym_of', 'miscommunication'], ['techniques', 'Used_for', 'failures of reference']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31170483460559795, 0.2875586854460094, 0.29914529914529914]
text: This paper considers the problem of reconstructing the motion of a 3D articulated tree from 2D point correspondences subject to some temporal prior .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
2D point correspondences
reconstructing the motion of a 3D articulated tree
temporal prior
2D point correspondences

triple_list: [["2D point correspondences", "Used_for", "reconstructing the motion of a 3D articulated tree"], ["temporal prior", "Used_for", "2D point correspondences"]]
pred: [('2D point correspondences', 'Used_for', 'reconstructing the motion of a 3D articulated tree'), ('temporal prior', 'Used_for', '2D point correspondences')]
trues: [['2D point correspondences', 'Used_for', 'reconstructing the motion of a 3D articulated tree']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.31218274111675126, 0.28839390386869873, 0.2998171846435101]
text: This paper presents a critical discussion of the various approaches that have been used in the evaluation of Natural Language systems .
result: The candidate relations for this text are:
Evaluate_for
The candidate entities for this text are:
Natural Language systems
approaches

triple_list: [["Natural Language systems", "Evaluate_for", "approaches"]]
pred: [('Natural Language systems', 'Evaluate_for', 'approaches')]
trues: [['approaches', 'Used_for', 'evaluation of Natural Language systems']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.311787072243346, 0.28805620608899296, 0.2994522215459525]
text: Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing ( [ 1 ] [ 2 ] ) .
result: The candidate relations for this text are:
Part_of
The candidate entities for this text are:
Recognition of proper nouns
Japanese text processing

triple_list: [["Recognition of proper nouns", "Part_of", "Japanese text processing"]]
pred: [('Recognition of proper nouns', 'Part_of', 'Japanese text processing')]
trues: [['Recognition of proper nouns', 'Part_of', 'morphological analysis'], ['proper nouns', 'Part_of', 'Japanese text'], ['morphological analysis', 'Used_for', 'Japanese text processing']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31139240506329113, 0.2870478413068845, 0.2987249544626594]
text: These models simulate the reading time advantage for parallel structures found in human data , and also yield a small increase in overall parsing accuracy .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
models
reading time advantage
models
parsing accuracy

triple_list: [["models", "Used_for", "reading time advantage"], ["models", "Used_for", "parsing accuracy"]]
pred: [('models', 'Used_for', 'reading time advantage'), ('models', 'Used_for', 'parsing accuracy')]
trues: [['parallel structures', 'Part_of', 'human data']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3106060606060606, 0.2867132867132867, 0.2981818181818182]
text: The task of machine translation ( MT ) evaluation is closely related to the task of sentence-level semantic equivalence classification .
result: The candidate relations for this text are:
Conjunction
The candidate entities for this text are:
machine translation ( MT ) evaluation
task of sentence-level semantic equivalence classification

triple_list: [["machine translation ( MT ) evaluation", "Conjunction", "task of sentence-level semantic equivalence classification"]]
pred: [('machine translation ( MT ) evaluation', 'Conjunction', 'task of sentence-level semantic equivalence classification')]
trues: [['machine translation ( MT ) evaluation', 'Conjunction', 'sentence-level semantic equivalence classification']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31021437578814626, 0.28637951105937137, 0.2978208232445521]
text: The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases , referring expressions , and interruptions .
result: The candidate relations for this text are:
Conjunction
The candidate entities for this text are:
cue phrases
referring expressions

triple_list: [["cue phrases", "Conjunction", "referring expressions"]]
pred: [('cue phrases', 'Conjunction', 'referring expressions')]
trues: [['cue phrases', 'Hyponym_of', 'discourse phenomena'], ['cue phrases', 'Conjunction', 'referring expressions'], ['referring expressions', 'Hyponym_of', 'discourse phenomena'], ['referring expressions', 'Conjunction', 'interruptions'], ['interruptions', 'Hyponym_of', 'discourse phenomena']]
metrics_sample: [1.0, 0.2, 0.33333333333333337]
metrics_current: [0.3110831234256927, 0.28587962962962965, 0.29794933655006034]
text: An important area of learning in autonomous agents is the ability to learn domain-speciic models of actions to be used by planning systems .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
learning
autonomous agents
domain-speciic models
planning systems

triple_list: [["learning", "Used_for", "autonomous agents"], ["domain-speciic models", "Used_for", "planning systems"]]
pred: [('learning', 'Used_for', 'autonomous agents'), ('domain-speciic models', 'Used_for', 'planning systems')]
trues: [['learning in autonomous agents', 'Used_for', 'domain-speciic models of actions'], ['planning systems', 'Used_for', 'domain-speciic models of actions']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3103015075376884, 0.2852193995381062, 0.2972322503008423]
text: In this paper , we present our approach for using information extraction annotations to augment document retrieval for distillation .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
information extraction annotations
approach
information extraction annotations
document retrieval

triple_list: [["information extraction annotations", "Used_for", "approach"], ["information extraction annotations", "Used_for", "document retrieval"]]
pred: [('information extraction annotations', 'Used_for', 'approach'), ('information extraction annotations', 'Used_for', 'document retrieval')]
trues: [['information extraction annotations', 'Used_for', 'document retrieval for distillation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.30952380952380953, 0.2848904267589389, 0.2966966966966967]
text: The psycholinguistic literature provides evidence for syntactic priming , i.e. , the tendency to repeat structures .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
psycholinguistic literature
syntactic priming

triple_list: [["psycholinguistic literature", "Used_for", "syntactic priming"]]
pred: [('psycholinguistic literature', 'Used_for', 'syntactic priming')]
trues: [['psycholinguistic literature', 'Used_for', 'syntactic priming']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.31038798498122655, 0.2857142857142857, 0.2975404919016197]
text: In this paper we specialize the projective unifocal , bifo-cal , and trifocal tensors to the affine case , and show how the tensors obtained relate to the registered tensors encountered in previous work .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
projective unifocal , bifo-cal , and trifocal tensors
affine case

triple_list: [["projective unifocal , bifo-cal , and trifocal tensors", "Used_for", "affine case"]]
pred: [('projective unifocal , bifo-cal , and trifocal tensors', 'Used_for', 'affine case')]
trues: [['projective unifocal , bifo-cal , and trifocal tensors', 'Used_for', 'affine case']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.31125, 0.286536248561565, 0.2983822648292391]
text: We present a scanning method that recovers dense sub-pixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
scanning method
dense sub-pixel camera-projector correspondence
photometric calibration
scanning method

triple_list: [["scanning method", "Used_for", "dense sub-pixel camera-projector correspondence"], ["photometric calibration", "Used_for", "scanning method"]]
pred: [('scanning method', 'Used_for', 'dense sub-pixel camera-projector correspondence'), ('photometric calibration', 'Used_for', 'scanning method')]
trues: [['scanning method', 'Used_for', 'dense sub-pixel camera-projector correspondence']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3117206982543641, 0.28735632183908044, 0.2990430622009569]
text: The key idea of our approach is to use different view points for reasoning about contradictions and consistencies between multiple depth maps generated with the same stereo algorithm .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
view points
approach
view points
depth maps

triple_list: [["view points", "Used_for", "approach"], ["view points", "Used_for", "depth maps"]]
pred: [('view points', 'Used_for', 'approach'), ('view points', 'Used_for', 'depth maps')]
trues: [['view points', 'Used_for', 'approach']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.31218905472636815, 0.2881745120551091, 0.29970149253731343]
text: The automated segmentation of images into semantically meaningful parts requires shape information since low-level feature analysis alone often fails to reach this goal .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
shape information
segmentation of images
low-level feature analysis
segmentation of images

triple_list: [["shape information", "Used_for", "segmentation of images"], ["low-level feature analysis", "Used_for", "segmentation of images"]]
pred: [('shape information', 'Used_for', 'segmentation of images'), ('low-level feature analysis', 'Used_for', 'segmentation of images')]
trues: [['images', 'Used_for', 'automated segmentation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31141439205955335, 0.28784403669724773, 0.299165673420739]
text: It is particularly valuable to empirical MT research .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
It
empirical MT research

triple_list: [["It", "Used_for", "empirical MT research"]]
pred: [('It', 'Used_for', 'empirical MT research')]
trues: [['It', 'Used_for', 'empirical MT research']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.31226765799256506, 0.28865979381443296, 0.3]
text: The objects can be complex in that they may be composed of multiple layers with different refractive indices .
result: The candidate relations for this text are:
Part_of
The candidate entities for this text are:
layers
objects

triple_list: [["layers", "Part_of", "objects"]]
pred: [('layers', 'Part_of', 'objects')]
trues: [['multiple layers', 'Part_of', 'they'], ['refractive indices', 'Feature_of', 'multiple layers']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3118811881188119, 0.288, 0.2994652406417112]
text: Finally , a novel feature weighting and selection function is presented , which yields superior feature vectors and better word similarity performance .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
feature weighting and selection function
feature vectors
feature weighting and selection function
word similarity

triple_list: [["feature weighting and selection function", "Used_for", "feature vectors"], ["feature weighting and selection function", "Used_for", "word similarity"]]
pred: [('feature weighting and selection function', 'Used_for', 'feature vectors'), ('feature weighting and selection function', 'Used_for', 'word similarity')]
trues: [['feature weighting and selection function', 'Used_for', 'feature vectors'], ['feature weighting and selection function', 'Used_for', 'word similarity'], ['feature vectors', 'Conjunction', 'word similarity']]
metrics_sample: [1.0, 0.6666666666666666, 0.8]
metrics_current: [0.3135802469135803, 0.28929384965831434, 0.3009478672985782]
text: In this model , on-line handwriting is considered as a modulation of a simple cycloidal pen motion , described by two coupled oscillations with a constant linear drift along the line of the writing .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
on-line handwriting
modulation

triple_list: [["on-line handwriting", "Used_for", "modulation"]]
pred: [('on-line handwriting', 'Used_for', 'modulation')]
trues: [['model', 'Used_for', 'on-line handwriting'], ['on-line handwriting', 'Part_of', 'cycloidal pen motion']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31319358816276205, 0.28863636363636364, 0.3004139562389119]
text: Coedition of a natural language text and its representation in some interlingual form seems the best and simplest way to share text revision across languages .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
interlingual form
text revision

triple_list: [["interlingual form", "Used_for", "text revision"]]
pred: [('interlingual form', 'Used_for', 'text revision')]
trues: [['Coedition', 'Used_for', 'text revision'], ['natural language text', 'Used_for', 'Coedition']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.312807881773399, 0.28798185941043086, 0.2998819362455726]
text: Structural or numerical constraints can then be added locally to the reconstruction process through a constrained optimization scheme .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
constrained optimization scheme
reconstruction process

triple_list: [["constrained optimization scheme", "Used_for", "reconstruction process"]]
pred: [('constrained optimization scheme', 'Used_for', 'reconstruction process')]
trues: [['Structural or numerical constraints', 'Used_for', 'reconstruction process'], ['constrained optimization scheme', 'Used_for', 'Structural or numerical constraints']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3124231242312423, 0.2873303167420814, 0.2993517972893341]
text: Our goal is to learn a Mahalanobis distance by minimizing a loss defined on the weighted sum of the precision at different ranks .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
weighted sum of the precision at different ranks
Mahalanobis distance

triple_list: [["weighted sum of the precision at different ranks", "Used_for", "Mahalanobis distance"]]
pred: [('weighted sum of the precision at different ranks', 'Used_for', 'Mahalanobis distance')]
trues: [['loss', 'Used_for', 'Mahalanobis distance'], ['weighted sum', 'Feature_of', 'precision']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31203931203931207, 0.2866817155756208, 0.2988235294117647]
text: In spite of over two decades of intense research , illumination and pose invariance remain prohibitively challenging aspects of face recognition for most practical applications .
result: The candidate relations for this text are:
Feature_of
The candidate entities for this text are:
illumination and pose invariance
face recognition

triple_list: [["illumination and pose invariance", "Feature_of", "face recognition"]]
pred: [('illumination and pose invariance', 'Feature_of', 'face recognition')]
trues: [['illumination', 'Conjunction', 'pose invariance'], ['illumination', 'Part_of', 'face recognition'], ['pose invariance', 'Part_of', 'face recognition']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3116564417177914, 0.2857142857142857, 0.2981220657276995]
text: This phenomenon causes many image processing techniques to fail as they assume the presence of only one layer at each examined site e.g. motion estimation and object recognition .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
image processing techniques
layer
image processing techniques
motion estimation
motion estimation
object recognition
object recognition
image processing techniques
object recognition
motion estimation

triple_list: [["image processing techniques", "Used_for", "layer"], ["image processing techniques", "Used_for", "motion estimation"], ["motion estimation", "Conjunction", "object recognition"], ["object recognition", "Used_for", "image processing techniques"], ["object recognition", "Used_for", "motion estimation"]]
pred: [('image processing techniques', 'Used_for', 'layer'), ('image processing techniques', 'Used_for', 'motion estimation'), ('motion estimation', 'Conjunction', 'object recognition'), ('object recognition', 'Used_for', 'image processing techniques'), ('object recognition', 'Used_for', 'motion estimation')]
trues: [['motion estimation', 'Conjunction', 'object recognition']]
metrics_sample: [0.2, 1.0, 0.33333333333333337]
metrics_current: [0.31097560975609756, 0.28651685393258425, 0.2982456140350877]
text: This paper describes FERRET , an interactive question-answering ( Q/A ) system designed to address the challenges of integrating automatic Q/A applications into real-world environments .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
interactive question-answering ( Q/A ) system
integrating automatic Q/A applications
real-world environments
integrating automatic Q/A applications

triple_list: [["interactive question-answering ( Q/A ) system", "Used_for", "integrating automatic Q/A applications"], ["real-world environments", "Used_for", "integrating automatic Q/A applications"]]
pred: [('interactive question-answering ( Q/A ) system', 'Used_for', 'integrating automatic Q/A applications'), ('real-world environments', 'Used_for', 'integrating automatic Q/A applications')]
trues: [['FERRET', 'Hyponym_of', 'interactive question-answering ( Q/A ) system'], ['FERRET', 'Used_for', 'integrating automatic Q/A applications into real-world environments']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3102189781021898, 0.2858744394618834, 0.2975495915985998]
text: Our technique is based on an improved , dynamic-programming , stereo algorithm for efficient novel-view generation .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
dynamic-programming , stereo algorithm
technique
dynamic-programming , stereo algorithm
novel-view generation

triple_list: [["dynamic-programming , stereo algorithm", "Used_for", "technique"], ["dynamic-programming , stereo algorithm", "Used_for", "novel-view generation"]]
pred: [('dynamic-programming , stereo algorithm', 'Used_for', 'technique'), ('dynamic-programming , stereo algorithm', 'Used_for', 'novel-view generation')]
trues: [['technique', 'Used_for', 'novel-view generation'], ['dynamic-programming , stereo algorithm', 'Used_for', 'technique']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3106796116504854, 0.28635346756152125, 0.2980209545983702]
text: The goal of this work is the enrichment of human-machine interactions in a natural language environment .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
natural language environment
human-machine interactions

triple_list: [["natural language environment", "Used_for", "human-machine interactions"]]
pred: [('natural language environment', 'Used_for', 'human-machine interactions')]
trues: [['natural language environment', 'Feature_of', 'human-machine interactions']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3103030303030303, 0.2860335195530726, 0.29767441860465116]
text: Topic signatures can be useful in a number of Natural Language Processing ( NLP ) applications , such as Word Sense Disambiguation ( WSD ) and Text Summarisation .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
Topic signatures
Natural Language Processing ( NLP ) applications
Word Sense Disambiguation ( WSD )
Natural Language Processing ( NLP ) applications
Word Sense Disambiguation ( WSD )
Text Summarisation
Text Summarisation
Natural Language Processing ( NLP ) applications
Text Summarisation
Natural Language Processing ( NLP ) applications

triple_list: [["Topic signatures", "Used_for", "Natural Language Processing ( NLP ) applications"], ["Word Sense Disambiguation ( WSD )", "Used_for", "Natural Language Processing ( NLP ) applications"], ["Word Sense Disambiguation ( WSD )", "Conjunction", "Text Summarisation"], ["Text Summarisation", "Used_for", "Natural Language Processing ( NLP ) applications"], ["Text Summarisation", "Used_for", "Natural Language Processing ( NLP ) applications"]]
pred: [('Topic signatures', 'Used_for', 'Natural Language Processing ( NLP ) applications'), ('Word Sense Disambiguation ( WSD )', 'Used_for', 'Natural Language Processing ( NLP ) applications'), ('Word Sense Disambiguation ( WSD )', 'Conjunction', 'Text Summarisation'), ('Text Summarisation', 'Used_for', 'Natural Language Processing ( NLP ) applications'), ('Text Summarisation', 'Used_for', 'Natural Language Processing ( NLP ) applications')]
trues: [['Topic signatures', 'Used_for', 'Natural Language Processing ( NLP ) applications'], ['Topic signatures', 'Used_for', 'Word Sense Disambiguation ( WSD )'], ['Topic signatures', 'Used_for', 'Text Summarisation'], ['Word Sense Disambiguation ( WSD )', 'Hyponym_of', 'Natural Language Processing ( NLP ) applications'], ['Word Sense Disambiguation ( WSD )', 'Conjunction', 'Text Summarisation'], ['Text Summarisation', 'Hyponym_of', 'Natural Language Processing ( NLP ) applications']]
metrics_sample: [0.5, 0.3333333333333333, 0.4]
metrics_current: [0.3112183353437877, 0.28634850166481685, 0.29826589595375724]
text: A purely functional implementation of LR-parsers is given , together with a simple correctness proof .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
functional implementation
LR-parsers

triple_list: [["functional implementation", "Used_for", "LR-parsers"]]
pred: [('functional implementation', 'Used_for', 'LR-parsers')]
trues: [['correctness proof', 'Conjunction', 'LR-parsers']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3108433734939759, 0.2860310421286031, 0.2979214780600462]
text: In this paper , we introduce an annotation scheme for scientific articles which can be used to build such a resource in a consistent way .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
annotation scheme
resource

triple_list: [["annotation scheme", "Used_for", "resource"]]
pred: [('annotation scheme', 'Used_for', 'resource')]
trues: [['annotation scheme', 'Used_for', 'scientific articles'], ['annotation scheme', 'Used_for', 'resource']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.31167268351383876, 0.28650442477876104, 0.2985590778097983]
text: An extension of our method to space-time interest point detection for action classification is also presented .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
extension
method
space-time interest point detection
action classification

triple_list: [["extension", "Used_for", "method"], ["space-time interest point detection", "Used_for", "action classification"]]
pred: [('extension', 'Used_for', 'method'), ('space-time interest point detection', 'Used_for', 'action classification')]
trues: [['method', 'Used_for', 'space-time interest point detection'], ['space-time interest point detection', 'Used_for', 'action classification']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.31212484993997597, 0.2869757174392936, 0.2990224266820012]
text: First , we investigate how well the addressee of a dialogue act can be predicted based on gaze , utterance and conversational context features .
result: The candidate relations for this text are:
Used_for
Used_for
Conjunction
Used_for
Used_for
The candidate entities for this text are:
gaze
dialogue act
gaze
utterance
utterance
dialogue act
utterance
conversational context features
conversational context features
dialogue act

triple_list: [["gaze", "Used_for", "dialogue act"], ["gaze", "Used_for", "utterance"], ["utterance", "Conjunction", "conversational context features"], ["utterance", "Used_for", "dialogue act"], ["conversational context features", "Used_for", "dialogue act"]]
pred: [('gaze', 'Used_for', 'dialogue act'), ('gaze', 'Used_for', 'utterance'), ('utterance', 'Conjunction', 'conversational context features'), ('utterance', 'Used_for', 'dialogue act'), ('conversational context features', 'Used_for', 'dialogue act')]
trues: [['gaze', 'Used_for', 'addressee of a dialogue act'], ['gaze', 'Conjunction', 'utterance'], ['utterance', 'Used_for', 'addressee of a dialogue act'], ['utterance', 'Conjunction', 'conversational context features'], ['conversational context features', 'Used_for', 'addressee of a dialogue act']]
metrics_sample: [0.2, 0.2, 0.20000000000000004]
metrics_current: [0.31145584725536996, 0.2864983534577388, 0.29845626072041176]
text: The experimental results show that the proposed histogram-based interest point detectors perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of repeatability and distinctiveness .
result: The candidate relations for this text are:
Used_for
Used_for
Evaluate_for
Evaluate_for
Conjunction
The candidate entities for this text are:
histogram-based interest point detectors
matching textured scenes under blur and illumination changes
histogram-based interest point detectors
tasks
repeatability
histogram-based interest point detectors
distinctiveness
histogram-based interest point detectors
blur
illumination changes

triple_list: [["histogram-based interest point detectors", "Used_for", "matching textured scenes under blur and illumination changes"], ["histogram-based interest point detectors", "Used_for", "tasks"], ["repeatability", "Evaluate_for", "histogram-based interest point detectors"], ["distinctiveness", "Evaluate_for", "histogram-based interest point detectors"], ["blur", "Conjunction", "illumination changes"]]
pred: [('histogram-based interest point detectors', 'Used_for', 'matching textured scenes under blur and illumination changes'), ('histogram-based interest point detectors', 'Used_for', 'tasks'), ('repeatability', 'Evaluate_for', 'histogram-based interest point detectors'), ('distinctiveness', 'Evaluate_for', 'histogram-based interest point detectors'), ('blur', 'Conjunction', 'illumination changes')]
trues: [['histogram-based interest point detectors', 'Used_for', 'matching textured scenes'], ['repeatability', 'Evaluate_for', 'histogram-based interest point detectors'], ['repeatability', 'Conjunction', 'distinctiveness'], ['distinctiveness', 'Evaluate_for', 'histogram-based interest point detectors']]
metrics_sample: [0.4, 0.5, 0.4444444444444445]
metrics_current: [0.31198102016607354, 0.28743169398907104, 0.2992036405005688]
text: This paper examines the benefits of system combination for unsupervised WSD .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
system combination
unsupervised WSD

triple_list: [["system combination", "Used_for", "unsupervised WSD"]]
pred: [('system combination', 'Used_for', 'unsupervised WSD')]
trues: [['system combination', 'Used_for', 'unsupervised WSD']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.3127962085308057, 0.28820960698689957, 0.30000000000000004]
text: To a large extent , these statistics reflect semantic constraints and thus are used to disambiguate anaphora references and syntactic ambiguities .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
Conjunction
The candidate entities for this text are:
statistics
semantic constraints
statistics
anaphora references
statistics
syntactic ambiguities
anaphora references
syntactic ambiguities

triple_list: [["statistics", "Used_for", "semantic constraints"], ["statistics", "Used_for", "anaphora references"], ["statistics", "Used_for", "syntactic ambiguities"], ["anaphora references", "Conjunction", "syntactic ambiguities"]]
pred: [('statistics', 'Used_for', 'semantic constraints'), ('statistics', 'Used_for', 'anaphora references'), ('statistics', 'Used_for', 'syntactic ambiguities'), ('anaphora references', 'Conjunction', 'syntactic ambiguities')]
trues: [['semantic constraints', 'Used_for', 'anaphora references'], ['semantic constraints', 'Used_for', 'syntactic ambiguities'], ['anaphora references', 'Conjunction', 'syntactic ambiguities']]
metrics_sample: [0.25, 0.3333333333333333, 0.28571428571428575]
metrics_current: [0.3125, 0.2883569096844396, 0.29994340690435767]
text: By employing a joint classification and regression optimization objective , this network is capable of automatically localizing the start and end points of actions more accurately .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
joint classification and regression optimization objective
network
network
localizing the start and end points of actions

triple_list: [["joint classification and regression optimization objective", "Used_for", "network"], ["network", "Used_for", "localizing the start and end points of actions"]]
pred: [('joint classification and regression optimization objective', 'Used_for', 'network'), ('network', 'Used_for', 'localizing the start and end points of actions')]
trues: [['joint classification and regression optimization objective', 'Used_for', 'network']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3129411764705882, 0.2891304347826087, 0.3005649717514124]
text: Experimental results on our dataset and the public G3D dataset both demonstrate very promising performance of our scheme .
result: The candidate relations for this text are:
Evaluate_for
Evaluate_for
The candidate entities for this text are:
dataset
scheme
G3D dataset
scheme

triple_list: [["dataset", "Evaluate_for", "scheme"], ["G3D dataset", "Evaluate_for", "scheme"]]
pred: [('dataset', 'Evaluate_for', 'scheme'), ('G3D dataset', 'Evaluate_for', 'scheme')]
trues: [['dataset', 'Conjunction', 'G3D dataset']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31220657276995306, 0.2888165038002172, 0.3000564015792442]
text: Specifically , by leveraging the merits of the deep Long Short-Term Memory ( LSTM ) subnetwork , the proposed model automatically captures the complex long-range temporal dynamics , which naturally avoids the typical sliding window design and thus ensures high computational efficiency .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
deep Long Short-Term Memory ( LSTM ) subnetwork
model
sliding window design
model

triple_list: [["deep Long Short-Term Memory ( LSTM ) subnetwork", "Used_for", "model"], ["sliding window design", "Used_for", "model"]]
pred: [('deep Long Short-Term Memory ( LSTM ) subnetwork', 'Used_for', 'model'), ('sliding window design', 'Used_for', 'model')]
trues: [['deep Long Short-Term Memory ( LSTM ) subnetwork', 'Used_for', 'model'], ['long-range temporal dynamics', 'Feature_of', 'model']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.3126463700234192, 0.28927410617551463, 0.3005064715813169]
text: Training instances are generated from experience and observation , and a variant of GOLEM is used to learn action models from these instances .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
instances
action models
GOLEM
action models

triple_list: [["instances", "Used_for", "action models"], ["GOLEM", "Used_for", "action models"]]
pred: [('instances', 'Used_for', 'action models'), ('GOLEM', 'Used_for', 'action models')]
trues: [['GOLEM', 'Used_for', 'action models']]
metrics_sample: [0.5, 1.0, 0.6666666666666666]
metrics_current: [0.3130841121495327, 0.29004329004329005, 0.30112359550561796]
text: While current approaches are developed by only considering the low rank plus sparse structure , in many applications , side information of row and/or column entities may also be given , and it is still unclear to what extent could such information help robust PCA .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
low rank plus sparse structure
approaches

triple_list: [["low rank plus sparse structure", "Used_for", "approaches"]]
pred: [('low rank plus sparse structure', 'Used_for', 'approaches')]
trues: [['low rank plus sparse structure', 'Used_for', 'approaches'], ['information', 'Used_for', 'robust PCA']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.31388564760793464, 0.29049676025917925, 0.30173864273696016]
text: The integrated learning system has been experimentally validated in simulated construction and ooce domains .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
integrated learning system
simulated construction and ooce domains

triple_list: [["integrated learning system", "Used_for", "simulated construction and ooce domains"]]
pred: [('integrated learning system', 'Used_for', 'simulated construction and ooce domains')]
trues: [['simulated construction', 'Evaluate_for', 'integrated learning system'], ['simulated construction', 'Conjunction', 'ooce domains'], ['ooce domains', 'Evaluate_for', 'integrated learning system']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3135198135198135, 0.28955866523143164, 0.3010632344711808]
text: This paper presents a machine learning approach to bare slice disambiguation in dialogue .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
machine learning approach
bare slice disambiguation in dialogue

triple_list: [["machine learning approach", "Used_for", "bare slice disambiguation in dialogue"]]
pred: [('machine learning approach', 'Used_for', 'bare slice disambiguation in dialogue')]
trues: [['machine learning approach', 'Used_for', 'bare slice disambiguation'], ['dialogue', 'Used_for', 'bare slice disambiguation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3131548311990687, 0.2889366272824919, 0.30055865921787706]
text: This paper presents an unsupervised learning approach to disambiguate various relations between named entities by use of various lexical and syntactic features from the contexts .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
unsupervised learning approach
disambiguate various relations between named entities
lexical and syntactic features
unsupervised learning approach

triple_list: [["unsupervised learning approach", "Used_for", "disambiguate various relations between named entities"], ["lexical and syntactic features", "Used_for", "unsupervised learning approach"]]
pred: [('unsupervised learning approach', 'Used_for', 'disambiguate various relations between named entities'), ('lexical and syntactic features', 'Used_for', 'unsupervised learning approach')]
trues: [['unsupervised learning approach', 'Used_for', 'relations between named entities'], ['lexical and syntactic features', 'Used_for', 'unsupervised learning approach']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.313588850174216, 0.28938906752411575, 0.3010033444816053]
text: This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
statistical approach
translation errors

triple_list: [["statistical approach", "Used_for", "translation errors"]]
pred: [('statistical approach', 'Used_for', 'translation errors')]
trues: [['loss functions', 'Evaluate_for', 'translation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31322505800464034, 0.2890792291220557, 0.30066815144766146]
text: Another problem with determiners is their inherent ambiguity .
result: The candidate relations for this text are:
Feature_of
The candidate entities for this text are:
ambiguity
determiners

triple_list: [["ambiguity", "Feature_of", "determiners"]]
pred: [('ambiguity', 'Feature_of', 'determiners')]
trues: [['ambiguity', 'Feature_of', 'determiners']]
metrics_sample: [1.0, 1.0, 1.0]
metrics_current: [0.31402085747392816, 0.2898395721925134, 0.30144605116796436]
text: One remarkable feature of this model is that it has allowed us to derive an efficient algorithm based on powerful statistical methods for estimating the Fujisaki-model parameters from raw F0 contours .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
algorithm
Fujisaki-model parameters
statistical methods
algorithm
raw F0 contours
Fujisaki-model parameters

triple_list: [["algorithm", "Used_for", "Fujisaki-model parameters"], ["statistical methods", "Used_for", "algorithm"], ["raw F0 contours", "Used_for", "Fujisaki-model parameters"]]
pred: [('algorithm', 'Used_for', 'Fujisaki-model parameters'), ('statistical methods', 'Used_for', 'algorithm'), ('raw F0 contours', 'Used_for', 'Fujisaki-model parameters')]
trues: [['remarkable feature', 'Feature_of', 'model'], ['algorithm', 'Used_for', 'Fujisaki-model parameters'], ['statistical methods', 'Used_for', 'algorithm'], ['raw F0 contours', 'Used_for', 'Fujisaki-model parameters']]
metrics_sample: [1.0, 0.75, 0.8571428571428571]
metrics_current: [0.3163972286374134, 0.29179978700745474, 0.303601108033241]
text: Building on previous work at Carnegie-Mellon University e.g. [ 4 , 5 , 8 ] , Plume 's approach to parsing is based on semantic caseframe instantiation .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
semantic caseframe instantiation
parsing
Plume
semantic caseframe instantiation

triple_list: [["semantic caseframe instantiation", "Used_for", "parsing"], ["Plume", "Used_for", "semantic caseframe instantiation"]]
pred: [('semantic caseframe instantiation', 'Used_for', 'parsing'), ('Plume', 'Used_for', 'semantic caseframe instantiation')]
trues: [["Plume 's approach", 'Used_for', 'parsing'], ['semantic caseframe instantiation', 'Used_for', "Plume 's approach"]]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.315668202764977, 0.2911795961742827, 0.3029297954671089]
text: A separation method is proposed that is nearly statistically efficient ( approaching the corresponding Cramr-Rao lower bound ) , if the separated signals obey the assumed model .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
separation method
separated signals

triple_list: [["separation method", "Used_for", "separated signals"]]
pred: [('separation method', 'Used_for', 'separated signals')]
trues: [['Cramr-Rao lower bound )', 'Feature_of', 'separation method']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31530494821634064, 0.2908704883227176, 0.3025952512424075]
text: In the second half of the paper , we report a laboratory study using the Wizard of Oz technique to identify NL requirements for carrying out this task .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
Wizard of Oz technique
NL requirements

triple_list: [["Wizard of Oz technique", "Used_for", "NL requirements"]]
pred: [('Wizard of Oz technique', 'Used_for', 'NL requirements')]
trues: [['Wizard of Oz technique', 'Used_for', 'NL requirements'], ['Wizard of Oz technique', 'Used_for', 'task']]
metrics_sample: [1.0, 0.5, 0.6666666666666666]
metrics_current: [0.3160919540229885, 0.2913135593220339, 0.30319735391400215]
text: Furthermore , we propose the use of standard parser evaluation methods for automatically evaluating the summarization quality of sentence condensation systems .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
parser evaluation methods
summarization quality
parser evaluation methods
sentence condensation systems

triple_list: [["parser evaluation methods", "Used_for", "summarization quality"], ["parser evaluation methods", "Used_for", "sentence condensation systems"]]
pred: [('parser evaluation methods', 'Used_for', 'summarization quality'), ('parser evaluation methods', 'Used_for', 'sentence condensation systems')]
trues: [['parser evaluation methods', 'Evaluate_for', 'summarization quality'], ['summarization quality', 'Evaluate_for', 'sentence condensation systems']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31536697247706424, 0.29069767441860467, 0.30253025302530256]
text: Simulated and experimental results show that our method recovers scene geometry with high subpixel precision , and that it can handle many challenges of active reconstruction systems .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
method
scene geometry

triple_list: [["method", "Used_for", "scene geometry"]]
pred: [('method', 'Used_for', 'scene geometry')]
trues: [['method', 'Used_for', 'scene geometry'], ['subpixel precision', 'Feature_of', 'scene geometry'], ['it', 'Used_for', 'active reconstruction systems']]
metrics_sample: [1.0, 0.3333333333333333, 0.5]
metrics_current: [0.3161512027491409, 0.29083245521601686, 0.30296377607025243]
text: We then turn to a discussion comparing the linguistic expressiveness of the two formalisms .
result: The candidate relations for this text are:
Compare
The candidate entities for this text are:
linguistic expressiveness
formalisms

triple_list: [["linguistic expressiveness", "Compare", "formalisms"]]
pred: [('linguistic expressiveness', 'Compare', 'formalisms')]
trues: [['linguistic expressiveness', 'Feature_of', 'formalisms']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3157894736842105, 0.2905263157894737, 0.3026315789473684]
text: In particular , our guarantee suggests that a substantial amount of low rank matrices , which can not be recovered by standard robust PCA , become re-coverable by our proposed method .
result: The candidate relations for this text are:
Used_for
The candidate entities for this text are:
robust PCA
low rank matrices

triple_list: [["robust PCA", "Used_for", "low rank matrices"]]
pred: [('robust PCA', 'Used_for', 'low rank matrices')]
trues: [['method', 'Used_for', 'low rank matrices']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31542857142857145, 0.2902208201892745, 0.30230010952902525]
text: At the core of the externally digital architecture is a high-density , low-power analog array performing binary-binary partial matrix-vector multiplication .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
high-density , low-power analog array
externally digital architecture
binary-binary partial matrix-vector multiplication
high-density , low-power analog array

triple_list: [["high-density , low-power analog array", "Used_for", "externally digital architecture"], ["binary-binary partial matrix-vector multiplication", "Used_for", "high-density , low-power analog array"]]
pred: [('high-density , low-power analog array', 'Used_for', 'externally digital architecture'), ('binary-binary partial matrix-vector multiplication', 'Used_for', 'high-density , low-power analog array')]
trues: [['high-density , low-power analog array', 'Part_of', 'externally digital architecture'], ['binary-binary partial matrix-vector multiplication', 'Used_for', 'high-density , low-power analog array']]
metrics_sample: [0.5, 0.5, 0.5]
metrics_current: [0.31584948688711517, 0.2906610703043022, 0.3027322404371585]
text: In this work , we study how Convolutional Neural Networks ( CNN ) architectures can be adapted to the task of simultaneous object recognition and pose estimation .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
Convolutional Neural Networks ( CNN ) architectures
task
Convolutional Neural Networks ( CNN ) architectures
simultaneous object recognition
Convolutional Neural Networks ( CNN ) architectures
pose estimation

triple_list: [["Convolutional Neural Networks ( CNN ) architectures", "Used_for", "task"], ["Convolutional Neural Networks ( CNN ) architectures", "Used_for", "simultaneous object recognition"], ["Convolutional Neural Networks ( CNN ) architectures", "Used_for", "pose estimation"]]
pred: [('Convolutional Neural Networks ( CNN ) architectures', 'Used_for', 'task'), ('Convolutional Neural Networks ( CNN ) architectures', 'Used_for', 'simultaneous object recognition'), ('Convolutional Neural Networks ( CNN ) architectures', 'Used_for', 'pose estimation')]
trues: [['Convolutional Neural Networks ( CNN ) architectures', 'Used_for', 'object recognition'], ['Convolutional Neural Networks ( CNN ) architectures', 'Used_for', 'pose estimation'], ['object recognition', 'Conjunction', 'pose estimation']]
metrics_sample: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]
metrics_current: [0.3159090909090909, 0.2907949790794979, 0.3028322440087146]
text: On the other hand , the manifold learning methods suggest to use the Euclidean distance in the local range , combining with the graphical relationship between samples , for approximating the geodesic distance .
result: The candidate relations for this text are:
Used_for
Used_for
Used_for
The candidate entities for this text are:
Euclidean distance
manifold learning methods
Euclidean distance
local range
graphical relationship between samples
geodesic distance

triple_list: [["Euclidean distance", "Used_for", "manifold learning methods"], ["Euclidean distance", "Used_for", "local range"], ["graphical relationship between samples", "Used_for", "geodesic distance"]]
pred: [('Euclidean distance', 'Used_for', 'manifold learning methods'), ('Euclidean distance', 'Used_for', 'local range'), ('graphical relationship between samples', 'Used_for', 'geodesic distance')]
trues: [['Euclidean distance', 'Used_for', 'manifold learning methods'], ['Euclidean distance', 'Conjunction', 'graphical relationship'], ['Euclidean distance', 'Used_for', 'geodesic distance'], ['local range', 'Feature_of', 'Euclidean distance'], ['graphical relationship', 'Used_for', 'geodesic distance']]
metrics_sample: [0.3333333333333333, 0.2, 0.25]
metrics_current: [0.3159682899207248, 0.2903225806451613, 0.30260303687635576]
text: Given the video streams acquired by two cameras placed on either side of a computer monitor , the proposed algorithm synthesises images from a virtual camera in arbitrary position ( typically located within the monitor ) to facilitate eye contact .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
video streams
algorithm
images
virtual camera

triple_list: [["video streams", "Used_for", "algorithm"], ["images", "Used_for", "virtual camera"]]
pred: [('video streams', 'Used_for', 'algorithm'), ('images', 'Used_for', 'virtual camera')]
trues: [['cameras', 'Used_for', 'video streams'], ['algorithm', 'Used_for', 'eye contact'], ['virtual camera', 'Used_for', 'images'], ['arbitrary position', 'Feature_of', 'virtual camera']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.3152542372881356, 0.28911917098445594, 0.30162162162162165]
text: In general , our CRF model yields a lower error rate than the HMM and Max-ent models on the NIST sentence boundary detection task in speech , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .
result: The candidate relations for this text are:
Evaluate_for
Compare
Evaluate_for
Evaluate_for
Conjunction
Evaluate_for
The candidate entities for this text are:
NIST sentence boundary detection task in speech
CRF model
CRF model
HMM and Max-ent models
HMM and Max-ent models
CRF model
HMM and Max-ent models
Max-ent models
three-way voting
CRF model

triple_list: [["NIST sentence boundary detection task in speech", "Evaluate_for", "CRF model"], ["CRF model", "Compare", "HMM and Max-ent models"], ["HMM and Max-ent models", "Evaluate_for", "CRF model"], ["HMM and Max-ent models", "Evaluate_for", "Max-ent models"], ["three-way voting", "Conjunction", "CRF model"]]
pred: [('NIST sentence boundary detection task in speech', 'Evaluate_for', 'CRF model'), ('CRF model', 'Compare', 'HMM and Max-ent models'), ('HMM and Max-ent models', 'Evaluate_for', 'CRF model'), ('HMM and Max-ent models', 'Evaluate_for', 'Max-ent models'), ('three-way voting', 'Conjunction', 'CRF model')]
trues: [['CRF model', 'Compare', 'HMM and Max-ent models'], ['error rate', 'Evaluate_for', 'CRF model'], ['error rate', 'Evaluate_for', 'HMM and Max-ent models'], ['NIST sentence boundary detection task', 'Evaluate_for', 'CRF model'], ['NIST sentence boundary detection task', 'Evaluate_for', 'HMM and Max-ent models'], ['speech', 'Feature_of', 'NIST sentence boundary detection task'], ['classifiers', 'Used_for', 'three-way voting']]
metrics_sample: [0.2, 0.14285714285714285, 0.16666666666666666]
metrics_current: [0.3146067415730337, 0.2880658436213992, 0.3007518796992481]
text: The resulting logical expression is then transformed by a planning algorithm into efficient Prolog , cf. query optimisation in a relational database .
result: The candidate relations for this text are:
Used_for
Used_for
The candidate entities for this text are:
logical expression
planning algorithm
planning algorithm
Prolog

triple_list: [["logical expression", "Used_for", "planning algorithm"], ["planning algorithm", "Used_for", "Prolog"]]
pred: [('logical expression', 'Used_for', 'planning algorithm'), ('planning algorithm', 'Used_for', 'Prolog')]
trues: [['planning algorithm', 'Used_for', 'logical expression'], ['relational database', 'Used_for', 'query optimisation']]
metrics_sample: [0.0, 0.0, 0]
metrics_current: [0.31390134529147984, 0.2874743326488706, 0.30010718113612]