[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_code_rationale_steps=200_icl=3_mod=q-v", "Precision": 0.5437665782493368, "Recall": 0.5036855036855037, "F1_Score": 0.5229591836734694, "rel_type_metrics": {"Organization_based_in": {"precision": 0.5357142857142857, "recall": 0.3125, "f1": 0.39473684210526316}, "Live_in": {"precision": 0.6153846153846154, "recall": 0.4897959183673469, "f1": 0.5454545454545454}, "Kill": {"precision": 0.8666666666666667, "recall": 0.8297872340425532, "f1": 0.8478260869565217}, "Located_in": {"precision": 0.4, "recall": 0.4222222222222222, "f1": 0.41081081081081083}, "Work_for": {"precision": 0.49019607843137253, "recall": 0.6578947368421053, "f1": 0.5617977528089888}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-42-16", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_code_rationale_steps=200_icl=3_mod=q-v", "Precision": 0.5682382133995038, "Recall": 0.5626535626535627, "F1_Score": 0.5654320987654322, "rel_type_metrics": {"Work_for": {"precision": 0.5454545454545454, "recall": 0.631578947368421, "f1": 0.5853658536585366}, "Live_in": {"precision": 0.6041666666666666, "recall": 0.5918367346938775, "f1": 0.5979381443298969}, "Organization_based_in": {"precision": 0.4827586206896552, "recall": 0.4375, "f1": 0.4590163934426229}, "Kill": {"precision": 0.8695652173913043, "recall": 0.851063829787234, "f1": 0.8602150537634409}, "Located_in": {"precision": 0.4823529411764706, "recall": 0.45555555555555555, "f1": 0.4685714285714286}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-45-01", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_code_rationale_steps=200_icl=3_mod=q-v", "Precision": 0.5699745547073791, "Recall": 0.5503685503685504, "F1_Score": 0.56, "rel_type_metrics": {"Located_in": {"precision": 0.5061728395061729, "recall": 0.45555555555555555, "f1": 0.4795321637426901}, "Organization_based_in": {"precision": 0.5172413793103449, "recall": 0.46875, "f1": 0.4918032786885246}, "Live_in": {"precision": 0.5483870967741935, "recall": 0.5204081632653061, "f1": 0.5340314136125655}, "Kill": {"precision": 0.8913043478260869, "recall": 0.8723404255319149, "f1": 0.8817204301075269}, "Work_for": {"precision": 0.5411764705882353, "recall": 0.6052631578947368, "f1": 0.5714285714285714}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-17-47-45", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}]