[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_base_steps=200_icl=3_mod=k-v", "Precision": 0.6, "Recall": 0.6633906633906634, "F1_Score": 0.6301050175029171, "rel_type_metrics": {"Live_in": {"precision": 0.5412844036697247, "recall": 0.6020408163265306, "f1": 0.5700483091787439}, "Work_for": {"precision": 0.5617977528089888, "recall": 0.6578947368421053, "f1": 0.6060606060606061}, "Kill": {"precision": 0.84, "recall": 0.8936170212765957, "f1": 0.8659793814432989}, "Organization_based_in": {"precision": 0.5754716981132075, "recall": 0.6354166666666666, "f1": 0.6039603960396039}, "Located_in": {"precision": 0.6170212765957447, "recall": 0.6444444444444445, "f1": 0.6304347826086957}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-18-56-03", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_base_steps=200_icl=3_mod=k-v", "Precision": 0.6264501160092807, "Recall": 0.6633906633906634, "F1_Score": 0.6443914081145585, "rel_type_metrics": {"Work_for": {"precision": 0.6265060240963856, "recall": 0.6842105263157895, "f1": 0.6540880503144654}, "Located_in": {"precision": 0.6588235294117647, "recall": 0.6222222222222222, "f1": 0.64}, "Live_in": {"precision": 0.5454545454545454, "recall": 0.6122448979591837, "f1": 0.5769230769230769}, "Kill": {"precision": 0.8775510204081632, "recall": 0.9148936170212766, "f1": 0.8958333333333333}, "Organization_based_in": {"precision": 0.59, "recall": 0.6145833333333334, "f1": 0.6020408163265306}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-18-58-18", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_base_steps=200_icl=3_mod=k-v", "Precision": 0.6061946902654868, "Recall": 0.6732186732186732, "F1_Score": 0.6379511059371362, "rel_type_metrics": {"Organization_based_in": {"precision": 0.5384615384615384, "recall": 0.65625, "f1": 0.5915492957746479}, "Work_for": {"precision": 0.5909090909090909, "recall": 0.6842105263157895, "f1": 0.6341463414634148}, "Located_in": {"precision": 0.651685393258427, "recall": 0.6444444444444445, "f1": 0.6480446927374302}, "Kill": {"precision": 0.639344262295082, "recall": 0.8297872340425532, "f1": 0.7222222222222223}, "Live_in": {"precision": 0.6526315789473685, "recall": 0.6326530612244898, "f1": 0.6424870466321244}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-03-31-19-00-48", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}]