[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.43776824034334766, "Recall": 0.5012285012285013, "F1_Score": 0.46735395189003437, "rel_type_metrics": {"Live_in": {"precision": 0.5454545454545454, "recall": 0.6122448979591837, "f1": 0.5769230769230769}, "Work_for": {"precision": 0.6086956521739131, "recall": 0.5526315789473685, "f1": 0.5793103448275861}, "Organization_based_in": {"precision": 0.22448979591836735, "recall": 0.22916666666666666, "f1": 0.22680412371134018}, "Kill": {"precision": 0.42857142857142855, "recall": 0.6382978723404256, "f1": 0.5128205128205128}, "Located_in": {"precision": 0.423728813559322, "recall": 0.5555555555555556, "f1": 0.4807692307692307}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-26-20-26-41", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.5049261083743842, "Recall": 0.5036855036855037, "F1_Score": 0.5043050430504306, "rel_type_metrics": {"Live_in": {"precision": 0.6039603960396039, "recall": 0.6224489795918368, "f1": 0.6130653266331658}, "Located_in": {"precision": 0.6417910447761194, "recall": 0.4777777777777778, "f1": 0.5477707006369428}, "Work_for": {"precision": 0.625, "recall": 0.5921052631578947, "f1": 0.6081081081081081}, "Organization_based_in": {"precision": 0.4, "recall": 0.22916666666666666, "f1": 0.29139072847682124}, "Kill": {"precision": 0.3177570093457944, "recall": 0.723404255319149, "f1": 0.4415584415584416}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-13-53-05", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.5119363395225465, "Recall": 0.4742014742014742, "F1_Score": 0.4923469387755102, "rel_type_metrics": {"Live_in": {"precision": 0.625, "recall": 0.5612244897959183, "f1": 0.5913978494623656}, "Located_in": {"precision": 0.6551724137931034, "recall": 0.4222222222222222, "f1": 0.5135135135135135}, "Kill": {"precision": 0.32673267326732675, "recall": 0.7021276595744681, "f1": 0.44594594594594594}, "Work_for": {"precision": 0.6666666666666666, "recall": 0.6052631578947368, "f1": 0.6344827586206896}, "Organization_based_in": {"precision": 0.40384615384615385, "recall": 0.21875, "f1": 0.28378378378378377}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-00-37-36", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": true}]