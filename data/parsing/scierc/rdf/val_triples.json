[
    {
        "text": "This paper presents an algorithm for computing optical flow , shape , motion , lighting , and albedo from an image sequence of a rigidly-moving Lambertian object under distant illumination .",
        "triple_list": [
            [
                "algorithm",
                "Used_for",
                "computing optical flow , shape , motion , lighting , and albedo"
            ],
            [
                "image sequence",
                "Used_for",
                "algorithm"
            ],
            [
                "rigidly-moving Lambertian object",
                "Feature_of",
                "image sequence"
            ],
            [
                "distant illumination",
                "Feature_of",
                "rigidly-moving Lambertian object"
            ]
        ]
    },
    {
        "text": "The problem is formulated in a manner that subsumes structure from motion , multi-view stereo , and photo-metric stereo as special cases .",
        "triple_list": [
            [
                "motion",
                "Conjunction",
                "multi-view stereo"
            ],
            [
                "multi-view stereo",
                "Conjunction",
                "photo-metric stereo"
            ]
        ]
    },
    {
        "text": "The algorithm utilizes both spatial and temporal intensity variation as cues : the former constrains flow and the latter constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces .",
        "triple_list": [
            [
                "spatial and temporal intensity variation",
                "Used_for",
                "algorithm"
            ],
            [
                "former",
                "Hyponym_of",
                "cues"
            ],
            [
                "former",
                "Used_for",
                "flow"
            ],
            [
                "former",
                "Conjunction",
                "latter"
            ],
            [
                "latter",
                "Hyponym_of",
                "cues"
            ],
            [
                "latter",
                "Used_for",
                "surface orientation"
            ],
            [
                "cues",
                "Used_for",
                "dense reconstruction of both textured and texture-less surfaces"
            ]
        ]
    },
    {
        "text": "The algorithm works by iteratively estimating affine camera parameters , illumination , shape , and albedo in an alternating fashion .",
        "triple_list": [
            [
                "estimating affine camera parameters , illumination , shape , and albedo",
                "Used_for",
                "algorithm"
            ]
        ]
    },
    {
        "text": "An entity-oriented approach to restricted-domain parsing is proposed .",
        "triple_list": [
            [
                "entity-oriented approach",
                "Used_for",
                "restricted-domain parsing"
            ]
        ]
    },
    {
        "text": "Like semantic grammar , this allows easy exploitation of limited domain semantics .",
        "triple_list": [
            [
                "this",
                "Used_for",
                "limited domain semantics"
            ]
        ]
    },
    {
        "text": "In addition , it facilitates fragmentary recognition and the use of multiple parsing strategies , and so is particularly useful for robust recognition of extra-grammatical input .",
        "triple_list": [
            [
                "it",
                "Used_for",
                "fragmentary recognition"
            ],
            [
                "it",
                "Used_for",
                "multiple parsing strategies"
            ],
            [
                "multiple parsing strategies",
                "Used_for",
                "recognition of extra-grammatical input"
            ]
        ]
    },
    {
        "text": "Representative samples from an entity-oriented language definition are presented , along with a control structure for an entity-oriented parser , some parsing strategies that use the control structure , and worked examples of parses .",
        "triple_list": [
            [
                "control structure",
                "Used_for",
                "entity-oriented parser"
            ],
            [
                "control structure",
                "Used_for",
                "parsing strategies"
            ]
        ]
    },
    {
        "text": "A parser incorporating the control structure and the parsing strategies is currently under implementation .",
        "triple_list": [
            [
                "control structure",
                "Part_of",
                "parser"
            ]
        ]
    },
    {
        "text": "This paper summarizes the formalism of Category Cooccurrence Restrictions ( CCRs ) and describes two parsing algorithms that interpret it .",
        "triple_list": [
            [
                "parsing algorithms",
                "Used_for",
                "it"
            ]
        ]
    },
    {
        "text": "The use of CCRs leads to syntactic descriptions formulated entirely with restrictive statements .",
        "triple_list": [
            [
                "restrictive statements",
                "Feature_of",
                "syntactic descriptions"
            ]
        ]
    },
    {
        "text": "The paper shows how conventional algorithms for the analysis of context free languages can be adapted to the CCR formalism .",
        "triple_list": [
            [
                "algorithms",
                "Used_for",
                "CCR formalism"
            ],
            [
                "context free languages",
                "Used_for",
                "algorithms"
            ]
        ]
    },
    {
        "text": "Special attention is given to the part of the parser that checks the fulfillment of logical well-formedness conditions on trees .",
        "triple_list": [
            [
                "logical well-formedness conditions",
                "Feature_of",
                "trees"
            ]
        ]
    },
    {
        "text": "We present a text mining method for finding synonymous expressions based on the distributional hypothesis in a set of coherent corpora .",
        "triple_list": [
            [
                "text mining method",
                "Used_for",
                "synonymous expressions"
            ],
            [
                "distributional hypothesis",
                "Used_for",
                "text mining method"
            ]
        ]
    },
    {
        "text": "This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author 's text as a coherent corpus .",
        "triple_list": [
            [
                "accuracy",
                "Evaluate_for",
                "term aggregation system"
            ],
            [
                "term aggregation system",
                "Evaluate_for",
                "methodology"
            ]
        ]
    },
    {
        "text": "Our proposed method improves the accuracy of our term aggregation system , showing that our approach is successful .",
        "triple_list": [
            [
                "accuracy",
                "Evaluate_for",
                "term aggregation system"
            ],
            [
                "term aggregation system",
                "Evaluate_for",
                "method"
            ]
        ]
    },
    {
        "text": "In this work , we present a technique for robust estimation , which by explicitly incorporating the inherent uncertainty of the estimation procedure , results in a more efficient robust estimation algorithm .",
        "triple_list": [
            [
                "technique",
                "Used_for",
                "robust estimation"
            ],
            [
                "technique",
                "Used_for",
                "efficient robust estimation algorithm"
            ],
            [
                "inherent uncertainty of the estimation procedure",
                "Used_for",
                "technique"
            ]
        ]
    },
    {
        "text": "The combination of these two strategies results in a robust estimation procedure that provides a significant speed-up over existing RANSAC techniques , while requiring no prior information to guide the sampling process .",
        "triple_list": [
            [
                "strategies",
                "Used_for",
                "robust estimation procedure"
            ],
            [
                "RANSAC techniques",
                "Compare",
                "robust estimation procedure"
            ]
        ]
    },
    {
        "text": "In particular , our algorithm requires , on average , 3-10 times fewer samples than standard RANSAC , which is in close agreement with theoretical predictions .",
        "triple_list": [
            [
                "algorithm",
                "Compare",
                "RANSAC"
            ]
        ]
    },
    {
        "text": "The efficiency of the algorithm is demonstrated on a selection of geometric estimation problems .",
        "triple_list": [
            [
                "geometric estimation problems",
                "Evaluate_for",
                "algorithm"
            ]
        ]
    },
    {
        "text": "An attempt has been made to use an Augmented Transition Network as a procedural dialog model .",
        "triple_list": [
            [
                "Augmented Transition Network",
                "Hyponym_of",
                "dialog model"
            ]
        ]
    },
    {
        "text": "The development of such a model appears to be important in several respects : as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs .",
        "triple_list": [
            [
                "dialog schemata",
                "Used_for",
                "device"
            ],
            [
                "dialog schemata",
                "Used_for",
                "conversation analysis"
            ],
            [
                "models",
                "Used_for",
                "device"
            ],
            [
                "models",
                "Used_for",
                "verbal interaction"
            ],
            [
                "dialog schemata",
                "Conjunction",
                "verbal interaction"
            ]
        ]
    },
    {
        "text": "A standard ATN should be further developed in order to account for the verbal interactions of task-oriented dialogs .",
        "triple_list": [
            [
                "ATN",
                "Used_for",
                "verbal interactions"
            ],
            [
                "verbal interactions",
                "Feature_of",
                "task-oriented dialogs"
            ]
        ]
    },
    {
        "text": "We present a practically unsupervised learning method to produce single-snippet answers to definition questions in question answering systems that supplement Web search engines .",
        "triple_list": [
            [
                "unsupervised learning method",
                "Used_for",
                "single-snippet answers"
            ],
            [
                "question answering systems",
                "Used_for",
                "Web search engines"
            ]
        ]
    },
    {
        "text": "The method exploits on-line encyclopedias and dictionaries to generate automatically an arbitrarily large number of positive and negative definition examples , which are then used to train an svm to separate the two classes .",
        "triple_list": [
            [
                "method",
                "Used_for",
                "on-line encyclopedias and dictionaries"
            ],
            [
                "on-line encyclopedias and dictionaries",
                "Used_for",
                "positive and negative definition examples"
            ],
            [
                "positive and negative definition examples",
                "Used_for",
                "svm"
            ]
        ]
    },
    {
        "text": "We show experimentally that the proposed method is viable , that it outperforms the alternative of training the system on questions and news articles from trec , and that it helps the search engine handle definition questions significantly better .",
        "triple_list": [
            [
                "it",
                "Compare",
                "alternative"
            ],
            [
                "news articles",
                "Used_for",
                "system"
            ],
            [
                "news articles",
                "Part_of",
                "trec"
            ],
            [
                "it",
                "Used_for",
                "search engine"
            ]
        ]
    },
    {
        "text": "We revisit the classical decision-theoretic problem of weighted expert voting from a statistical learning perspective .",
        "triple_list": [
            [
                "statistical learning perspective",
                "Used_for",
                "classical decision-theoretic problem of weighted expert voting"
            ]
        ]
    },
    {
        "text": "In the case of known expert competence levels , we give sharp error estimates for the optimal rule .",
        "triple_list": [
            [
                "sharp error estimates",
                "Used_for",
                "optimal rule"
            ]
        ]
    },
    {
        "text": "We analyze a reweighted version of the Kikuchi approximation for estimating the log partition function of a product distribution defined over a region graph .",
        "triple_list": [
            [
                "reweighted version of the Kikuchi approximation",
                "Used_for",
                "log partition function of a product distribution"
            ],
            [
                "log partition function of a product distribution",
                "Feature_of",
                "region graph"
            ]
        ]
    },
    {
        "text": "We establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the Kikuchi expansion , and show that a reweighted version of the sum product algorithm applied to the Kikuchi region graph will produce global optima of the Kikuchi approximation whenever the algorithm converges .",
        "triple_list": [
            [
                "concavity",
                "Feature_of",
                "reweighted objective function"
            ],
            [
                "reweighted version of the sum product algorithm",
                "Used_for",
                "Kikuchi region graph"
            ],
            [
                "global optima",
                "Feature_of",
                "Kikuchi approximation"
            ]
        ]
    },
    {
        "text": "Finally , we provide an explicit characterization of the polytope of concavity in terms of the cycle structure of the region graph .",
        "triple_list": [
            [
                "cycle structure",
                "Feature_of",
                "region graph"
            ]
        ]
    },
    {
        "text": "We apply a decision tree based approach to pronoun resolution in spoken dialogue .",
        "triple_list": [
            [
                "decision tree based approach",
                "Used_for",
                "pronoun resolution"
            ],
            [
                "pronoun resolution",
                "Used_for",
                "spoken dialogue"
            ]
        ]
    },
    {
        "text": "Our system deals with pronouns with NP - and non-NP-antecedents .",
        "triple_list": [
            [
                "system",
                "Used_for",
                "pronouns"
            ],
            [
                "NP - and non-NP-antecedents",
                "Used_for",
                "pronouns"
            ]
        ]
    },
    {
        "text": "We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features .",
        "triple_list": [
            [
                "features",
                "Used_for",
                "pronoun resolution"
            ],
            [
                "pronoun resolution",
                "Used_for",
                "spoken dialogue"
            ]
        ]
    },
    {
        "text": "We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron 's ( 2002 ) manually tuned system .",
        "triple_list": [
            [
                "Switchboard dialogues",
                "Evaluate_for",
                "system"
            ],
            [
                "it",
                "Compare",
                "Byron 's ( 2002 ) manually tuned system"
            ]
        ]
    },
    {
        "text": "We present a new approach for building an efficient and robust classifier for the two class problem , that localizes objects that may appear in the image under different orien-tations .",
        "triple_list": [
            [
                "approach",
                "Used_for",
                "classifier"
            ],
            [
                "classifier",
                "Used_for",
                "class problem"
            ]
        ]
    },
    {
        "text": "In contrast to other works that address this problem using multiple classifiers , each one specialized for a specific orientation , we propose a simple two-step approach with an estimation stage and a classification stage .",
        "triple_list": [
            [
                "estimation stage",
                "Part_of",
                "approach"
            ],
            [
                "estimation stage",
                "Conjunction",
                "classification stage"
            ],
            [
                "classification stage",
                "Part_of",
                "approach"
            ]
        ]
    },
    {
        "text": "The estimator yields an initial set of potential object poses that are then validated by the classifier .",
        "triple_list": [
            [
                "classifier",
                "Used_for",
                "object poses"
            ]
        ]
    },
    {
        "text": "This methodology allows reducing the time complexity of the algorithm while classification results remain high .",
        "triple_list": [
            [
                "time complexity",
                "Evaluate_for",
                "algorithm"
            ]
        ]
    },
    {
        "text": "The classifier we use in both stages is based on a boosted combination of Random Ferns over local histograms of oriented gradients ( HOGs ) , which we compute during a pre-processing step .",
        "triple_list": [
            [
                "boosted combination of Random Ferns",
                "Used_for",
                "classifier"
            ],
            [
                "local histograms of oriented gradients ( HOGs )",
                "Feature_of",
                "boosted combination of Random Ferns"
            ],
            [
                "pre-processing step",
                "Used_for",
                "local histograms of oriented gradients ( HOGs )"
            ]
        ]
    },
    {
        "text": "Both the use of supervised learning and working on the gradient space makes our approach robust while being efficient at run-time .",
        "triple_list": [
            [
                "supervised learning",
                "Used_for",
                "approach"
            ],
            [
                "gradient space",
                "Used_for",
                "approach"
            ]
        ]
    },
    {
        "text": "We show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging conditions such as cluttered backgrounds , changing illumination conditions and partial occlusions .",
        "triple_list": [
            [
                "motorbikes under planar rotations",
                "Feature_of",
                "database"
            ],
            [
                "conditions",
                "Feature_of",
                "database"
            ],
            [
                "cluttered backgrounds",
                "Hyponym_of",
                "conditions"
            ],
            [
                "cluttered backgrounds",
                "Conjunction",
                "changing illumination conditions"
            ],
            [
                "changing illumination conditions",
                "Hyponym_of",
                "conditions"
            ],
            [
                "changing illumination conditions",
                "Conjunction",
                "partial occlusions"
            ],
            [
                "partial occlusions",
                "Hyponym_of",
                "conditions"
            ]
        ]
    },
    {
        "text": "A very simple improved duration model has reduced the error rate by about 10 % in both triphone and semiphone systems .",
        "triple_list": [
            [
                "duration model",
                "Used_for",
                "triphone and semiphone systems"
            ],
            [
                "error rate",
                "Evaluate_for",
                "triphone and semiphone systems"
            ]
        ]
    },
    {
        "text": "A new training strategy has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique .",
        "triple_list": [
            [
                "rapid adaptation technique",
                "Used_for",
                "training strategy"
            ]
        ]
    },
    {
        "text": "Finally , the recognizer has been modified to use bigram back-off language models .",
        "triple_list": [
            [
                "bigram back-off language models",
                "Used_for",
                "recognizer"
            ]
        ]
    },
    {
        "text": "The system was then transferred from the RM task to the ATIS CSR task and a limited number of development tests performed .",
        "triple_list": [
            [
                "system",
                "Used_for",
                "RM task"
            ],
            [
                "system",
                "Used_for",
                "ATIS CSR task"
            ],
            [
                "RM task",
                "Conjunction",
                "ATIS CSR task"
            ]
        ]
    },
    {
        "text": "A new approach for Interactive Machine Translation where the author interacts during the creation or the modification of the document is proposed .",
        "triple_list": [
            [
                "approach",
                "Used_for",
                "Interactive Machine Translation"
            ]
        ]
    },
    {
        "text": "This paper presents a new interactive disambiguation scheme based on the paraphrasing of a parser 's multiple output .",
        "triple_list": [
            [
                "paraphrasing",
                "Used_for",
                "interactive disambiguation scheme"
            ]
        ]
    },
    {
        "text": "We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation .",
        "triple_list": [
            [
                "approach",
                "Used_for",
                "statistical machine translation"
            ],
            [
                "syntactic information",
                "Part_of",
                "approach"
            ],
            [
                "syntactic information",
                "Conjunction",
                "phrasal translation"
            ],
            [
                "phrasal translation",
                "Part_of",
                "approach"
            ]
        ]
    },
    {
        "text": "This method requires a source-language dependency parser , target language word segmentation and an unsupervised word alignment component .",
        "triple_list": [
            [
                "source-language dependency parser",
                "Used_for",
                "method"
            ],
            [
                "source-language dependency parser",
                "Conjunction",
                "target language word segmentation"
            ],
            [
                "target language word segmentation",
                "Used_for",
                "method"
            ],
            [
                "target language word segmentation",
                "Conjunction",
                "unsupervised word alignment component"
            ],
            [
                "unsupervised word alignment component",
                "Used_for",
                "method"
            ]
        ]
    },
    {
        "text": "We describe an efficient decoder and show that using these tree-based models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser .",
        "triple_list": [
            [
                "tree-based models",
                "Conjunction",
                "SMT models"
            ],
            [
                "tree-based models",
                "Used_for",
                "approach"
            ],
            [
                "SMT models",
                "Used_for",
                "approach"
            ],
            [
                "phrasal SMT",
                "Conjunction",
                "linguistic generality"
            ],
            [
                "phrasal SMT",
                "Used_for",
                "parser"
            ],
            [
                "linguistic generality",
                "Feature_of",
                "parser"
            ]
        ]
    },
    {
        "text": "Video provides not only rich visual cues such as motion and appearance , but also much less explored long-range temporal interactions among objects .",
        "triple_list": [
            [
                "visual cues",
                "Feature_of",
                "Video"
            ],
            [
                "motion",
                "Hyponym_of",
                "visual cues"
            ],
            [
                "motion",
                "Conjunction",
                "appearance"
            ],
            [
                "appearance",
                "Hyponym_of",
                "visual cues"
            ]
        ]
    },
    {
        "text": "We aim to capture such interactions and to construct a powerful intermediate-level video representation for subsequent recognition .",
        "triple_list": [
            [
                "intermediate-level video representation",
                "Used_for",
                "recognition"
            ]
        ]
    },
    {
        "text": "First , we develop an efficient spatio-temporal video segmentation algorithm , which naturally incorporates long-range motion cues from the past and future frames in the form of clusters of point tracks with coherent motion .",
        "triple_list": [
            [
                "long-range motion cues",
                "Used_for",
                "spatio-temporal video segmentation algorithm"
            ],
            [
                "clusters of point tracks",
                "Used_for",
                "long-range motion cues"
            ]
        ]
    },
    {
        "text": "Second , we devise a new track clustering cost function that includes occlusion reasoning , in the form of depth ordering constraints , as well as motion similarity along the tracks .",
        "triple_list": [
            [
                "occlusion reasoning",
                "Part_of",
                "track clustering cost function"
            ],
            [
                "depth ordering constraints",
                "Feature_of",
                "occlusion reasoning"
            ],
            [
                "motion similarity",
                "Part_of",
                "track clustering cost function"
            ]
        ]
    },
    {
        "text": "We evaluate the proposed approach on a challenging set of video sequences of office scenes from feature length movies .",
        "triple_list": [
            [
                "video sequences of office scenes",
                "Evaluate_for",
                "approach"
            ]
        ]
    },
    {
        "text": "In this paper , we introduce KAZE features , a novel multiscale 2D feature detection and description algorithm in nonlinear scale spaces .",
        "triple_list": [
            [
                "KAZE features",
                "Hyponym_of",
                "multiscale 2D feature detection and description algorithm"
            ],
            [
                "nonlinear scale spaces",
                "Feature_of",
                "multiscale 2D feature detection and description algorithm"
            ]
        ]
    },
    {
        "text": "In contrast , we detect and describe 2D features in a nonlinear scale space by means of nonlinear diffusion filtering .",
        "triple_list": [
            [
                "nonlinear scale space",
                "Feature_of",
                "2D features"
            ],
            [
                "nonlinear diffusion filtering",
                "Used_for",
                "2D features"
            ]
        ]
    },
    {
        "text": "The nonlinear scale space is built using efficient Additive Operator Splitting ( AOS ) techniques and variable con-ductance diffusion .",
        "triple_list": [
            [
                "Additive Operator Splitting ( AOS ) techniques",
                "Used_for",
                "nonlinear scale space"
            ],
            [
                "Additive Operator Splitting ( AOS ) techniques",
                "Conjunction",
                "variable con-ductance diffusion"
            ],
            [
                "variable con-ductance diffusion",
                "Used_for",
                "nonlinear scale space"
            ]
        ]
    },
    {
        "text": "Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space , but comparable to SIFT , our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods .",
        "triple_list": [
            [
                "features",
                "Compare",
                "SURF"
            ],
            [
                "features",
                "Compare",
                "SIFT"
            ],
            [
                "results",
                "Compare",
                "state-of-the-art methods"
            ],
            [
                "detection",
                "Evaluate_for",
                "results"
            ],
            [
                "detection",
                "Conjunction",
                "description"
            ],
            [
                "detection",
                "Evaluate_for",
                "state-of-the-art methods"
            ],
            [
                "description",
                "Evaluate_for",
                "results"
            ],
            [
                "description",
                "Evaluate_for",
                "state-of-the-art methods"
            ]
        ]
    },
    {
        "text": "Creating summaries on lengthy Semantic Web documents for quick identification of the corresponding entity has been of great contemporary interest .",
        "triple_list": [
            [
                "Creating summaries",
                "Used_for",
                "identification of the corresponding entity"
            ],
            [
                "lengthy Semantic Web documents",
                "Used_for",
                "Creating summaries"
            ]
        ]
    },
    {
        "text": "Specifically , we highlight the importance of diversified ( faceted ) summaries by combining three dimensions : diversity , uniqueness , and popularity .",
        "triple_list": [
            [
                "diversity",
                "Feature_of",
                "diversified ( faceted ) summaries"
            ],
            [
                "diversity",
                "Conjunction",
                "uniqueness"
            ],
            [
                "uniqueness",
                "Feature_of",
                "diversified ( faceted ) summaries"
            ],
            [
                "uniqueness",
                "Conjunction",
                "popularity"
            ],
            [
                "popularity",
                "Feature_of",
                "diversified ( faceted ) summaries"
            ]
        ]
    },
    {
        "text": "Our novel diversity-aware entity summarization approach mimics human conceptual clustering techniques to group facts , and picks representative facts from each group to form concise ( i.e. , short ) and comprehensive ( i.e. , improved coverage through diversity ) summaries .",
        "triple_list": [
            [
                "human conceptual clustering techniques",
                "Used_for",
                "diversity-aware entity summarization approach"
            ]
        ]
    },
    {
        "text": "We evaluate our approach against the state-of-the-art techniques and show that our work improves both the quality and the efficiency of entity summarization .",
        "triple_list": [
            [
                "approach",
                "Used_for",
                "entity summarization"
            ],
            [
                "state-of-the-art techniques",
                "Compare",
                "approach"
            ],
            [
                "state-of-the-art techniques",
                "Used_for",
                "entity summarization"
            ],
            [
                "quality",
                "Evaluate_for",
                "entity summarization"
            ],
            [
                "efficiency",
                "Evaluate_for",
                "entity summarization"
            ]
        ]
    },
    {
        "text": "We present a framework for the fast computation of lexical affinity models .",
        "triple_list": [
            [
                "framework",
                "Used_for",
                "fast computation of lexical affinity models"
            ]
        ]
    },
    {
        "text": "The framework is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms , an independence model , and a parametric affinity model .",
        "triple_list": [
            [
                "algorithm",
                "Part_of",
                "framework"
            ],
            [
                "algorithm",
                "Used_for",
                "co-occurrence distribution"
            ],
            [
                "algorithm",
                "Conjunction",
                "independence model"
            ],
            [
                "independence model",
                "Part_of",
                "framework"
            ],
            [
                "independence model",
                "Conjunction",
                "parametric affinity model"
            ],
            [
                "parametric affinity model",
                "Part_of",
                "framework"
            ]
        ]
    },
    {
        "text": "In comparison with previous models , which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models , in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus .",
        "triple_list": [
            [
                "lexical affinity",
                "Used_for",
                "sequential models"
            ],
            [
                "models",
                "Compare",
                "models"
            ],
            [
                "models",
                "Used_for",
                "co-occurrence patterns"
            ]
        ]
    },
    {
        "text": "We apply it in combination with a terabyte corpus to answer natural language tests , achieving encouraging results .",
        "triple_list": [
            [
                "it",
                "Used_for",
                "natural language tests"
            ],
            [
                "terabyte corpus",
                "Evaluate_for",
                "it"
            ]
        ]
    },
    {
        "text": "This paper introduces a system for categorizing unknown words .",
        "triple_list": [
            [
                "system",
                "Used_for",
                "categorizing unknown words"
            ]
        ]
    },
    {
        "text": "The system is based on a multi-component architecture where each component is responsible for identifying one class of unknown words .",
        "triple_list": [
            [
                "multi-component architecture",
                "Used_for",
                "system"
            ],
            [
                "component",
                "Part_of",
                "multi-component architecture"
            ],
            [
                "component",
                "Used_for",
                "unknown words"
            ]
        ]
    },
    {
        "text": "The focus of this paper is the components that identify names and spelling errors .",
        "triple_list": [
            [
                "components",
                "Used_for",
                "names"
            ],
            [
                "components",
                "Used_for",
                "spelling errors"
            ],
            [
                "names",
                "Conjunction",
                "spelling errors"
            ]
        ]
    },
    {
        "text": "Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word .",
        "triple_list": [
            [
                "decision tree architecture",
                "Used_for",
                "component"
            ]
        ]
    },
    {
        "text": "The system is evaluated using data from live closed captions - a genre replete with a wide variety of unknown words .",
        "triple_list": [
            [
                "live closed captions",
                "Evaluate_for",
                "system"
            ]
        ]
    },
    {
        "text": "At MIT Lincoln Laboratory , we have been developing a Korean-to-English machine translation system CCLINC ( Common Coalition Language System at Lincoln Laboratory ) .",
        "triple_list": [
            [
                "CCLINC ( Common Coalition Language System at Lincoln Laboratory )",
                "Hyponym_of",
                "Korean-to-English machine translation system"
            ]
        ]
    },
    {
        "text": "The CCLINC Korean-to-English translation system consists of two core modules , language understanding and generation modules mediated by a language neutral meaning representation called a semantic frame .",
        "triple_list": [
            [
                "core modules",
                "Part_of",
                "CCLINC Korean-to-English translation system"
            ],
            [
                "language neutral meaning representation",
                "Used_for",
                "language understanding and generation modules"
            ],
            [
                "semantic frame",
                "Hyponym_of",
                "language neutral meaning representation"
            ]
        ]
    },
    {
        "text": "The key features of the system include : ( i ) Robust efficient parsing of Korean ( a verb final language with overt case markers , relatively free word order , and frequent omissions of arguments ) .",
        "triple_list": [
            [
                "Korean",
                "Hyponym_of",
                "verb final language"
            ],
            [
                "overt case markers",
                "Feature_of",
                "verb final language"
            ]
        ]
    },
    {
        "text": "( ii ) High quality translation via word sense disambiguation and accurate word order generation of the target language .",
        "triple_list": [
            [
                "word sense disambiguation",
                "Used_for",
                "translation"
            ],
            [
                "word sense disambiguation",
                "Conjunction",
                "word order generation"
            ],
            [
                "word order generation",
                "Used_for",
                "translation"
            ]
        ]
    },
    {
        "text": "Having been trained on Korean newspaper articles on missiles and chemical biological warfare , the system produces the translation output sufficient for content understanding of the original document .",
        "triple_list": [
            [
                "Korean newspaper articles",
                "Used_for",
                "system"
            ],
            [
                "missiles and chemical biological warfare",
                "Feature_of",
                "Korean newspaper articles"
            ]
        ]
    },
    {
        "text": "The JAVELIN system integrates a flexible , planning-based architecture with a variety of language processing modules to provide an open-domain question answering capability on free text .",
        "triple_list": [
            [
                "JAVELIN system",
                "Used_for",
                "open-domain question answering capability"
            ],
            [
                "planning-based architecture",
                "Part_of",
                "JAVELIN system"
            ],
            [
                "language processing modules",
                "Part_of",
                "JAVELIN system"
            ],
            [
                "language processing modules",
                "Conjunction",
                "planning-based architecture"
            ]
        ]
    },
    {
        "text": "We present the first application of the head-driven statistical parsing model of Collins ( 1999 ) as a simultaneous language model and parser for large-vocabulary speech recognition .",
        "triple_list": [
            [
                "head-driven statistical parsing model",
                "Used_for",
                "simultaneous language model"
            ],
            [
                "head-driven statistical parsing model",
                "Used_for",
                "parser"
            ],
            [
                "simultaneous language model",
                "Conjunction",
                "parser"
            ],
            [
                "simultaneous language model",
                "Used_for",
                "large-vocabulary speech recognition"
            ],
            [
                "parser",
                "Used_for",
                "large-vocabulary speech recognition"
            ]
        ]
    },
    {
        "text": "The model is adapted to an online left to right chart-parser for word lattices , integrating acoustic , n-gram , and parser probabilities .",
        "triple_list": [
            [
                "model",
                "Used_for",
                "online left to right chart-parser"
            ],
            [
                "online left to right chart-parser",
                "Used_for",
                "word lattices"
            ],
            [
                "acoustic , n-gram , and parser probabilities",
                "Part_of",
                "online left to right chart-parser"
            ]
        ]
    },
    {
        "text": "The parser uses structural and lexical dependencies not considered by n-gram models , conditioning recognition on more linguistically-grounded relationships .",
        "triple_list": [
            [
                "structural and lexical dependencies",
                "Used_for",
                "parser"
            ]
        ]
    },
    {
        "text": "Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding .",
        "triple_list": [
            [
                "Wall Street Journal treebank",
                "Conjunction",
                "lattice corpora"
            ],
            [
                "Wall Street Journal treebank",
                "Evaluate_for",
                "n-gram language model"
            ],
            [
                "lattice corpora",
                "Evaluate_for",
                "n-gram language model"
            ],
            [
                "word error rates",
                "Evaluate_for",
                "n-gram language model"
            ],
            [
                "structural information",
                "Used_for",
                "speech understanding"
            ]
        ]
    },
    {
        "text": "Image composition ( or mosaicing ) has attracted a growing attention in recent years as one of the main elements in video analysis and representation .",
        "triple_list": [
            [
                "Image composition ( or mosaicing )",
                "Part_of",
                "video analysis and representation"
            ]
        ]
    },
    {
        "text": "In this paper we deal with the problem of global alignment and super-resolution .",
        "triple_list": [
            [
                "global alignment",
                "Conjunction",
                "super-resolution"
            ]
        ]
    },
    {
        "text": "We also propose to evaluate the quality of the resulting mosaic by measuring the amount of blurring .",
        "triple_list": [
            [
                "amount of blurring",
                "Evaluate_for",
                "mosaic"
            ]
        ]
    },
    {
        "text": "Global registration is achieved by combining a graph-based technique -- that exploits the topological structure of the sequence induced by the spatial overlap -- with a bundle adjustment which uses only the homographies computed in the previous steps .",
        "triple_list": [
            [
                "graph-based technique",
                "Used_for",
                "Global registration"
            ],
            [
                "graph-based technique",
                "Used_for",
                "topological structure"
            ],
            [
                "graph-based technique",
                "Conjunction",
                "bundle adjustment"
            ],
            [
                "bundle adjustment",
                "Used_for",
                "Global registration"
            ],
            [
                "homographies",
                "Used_for",
                "bundle adjustment"
            ]
        ]
    },
    {
        "text": "Experimental comparison with other techniques shows the effectiveness of our approach .",
        "triple_list": [
            [
                "approach",
                "Compare",
                "techniques"
            ]
        ]
    },
    {
        "text": "The main of this project is computer-assisted acquisition and morpho-syntactic description of verb-noun collocations in Polish .",
        "triple_list": [
            [
                "Polish",
                "Used_for",
                "computer-assisted acquisition and morpho-syntactic description of verb-noun collocations"
            ]
        ]
    },
    {
        "text": "We present methodology and resources obtained in three main project phases which are : dictionary-based acquisition of collocation lexicon , feasibility study for corpus-based lexicon enlargement phase , corpus-based lexicon enlargement and collocation description .",
        "triple_list": [
            [
                "dictionary-based acquisition of collocation lexicon",
                "Hyponym_of",
                "phases"
            ],
            [
                "dictionary-based acquisition of collocation lexicon",
                "Conjunction",
                "feasibility study"
            ],
            [
                "feasibility study",
                "Hyponym_of",
                "phases"
            ],
            [
                "feasibility study",
                "Used_for",
                "corpus-based lexicon enlargement phase"
            ],
            [
                "corpus-based lexicon enlargement and collocation description",
                "Hyponym_of",
                "phases"
            ],
            [
                "corpus-based lexicon enlargement and collocation description",
                "Conjunction",
                "feasibility study"
            ]
        ]
    },
    {
        "text": "The presented here corpus-based approach permitted us to triple the size the verb-noun collocation dictionary for Polish .",
        "triple_list": [
            [
                "corpus-based approach",
                "Used_for",
                "verb-noun collocation dictionary"
            ],
            [
                "Polish",
                "Feature_of",
                "verb-noun collocation dictionary"
            ]
        ]
    },
    {
        "text": "Along with the increasing requirements , the hash-tag recommendation task for microblogs has been receiving considerable attention in recent years .",
        "triple_list": [
            [
                "hash-tag recommendation task",
                "Used_for",
                "microblogs"
            ]
        ]
    },
    {
        "text": "Motivated by the successful use of convolutional neural networks ( CNNs ) for many natural language processing tasks , in this paper , we adopt CNNs to perform the hashtag recommendation problem .",
        "triple_list": [
            [
                "convolutional neural networks ( CNNs )",
                "Used_for",
                "natural language processing tasks"
            ]
        ]
    },
    {
        "text": "To incorporate the trigger words whose effectiveness have been experimentally evaluated in several previous works , we propose a novel architecture with an attention mechanism .",
        "triple_list": [
            [
                "architecture",
                "Used_for",
                "trigger words"
            ],
            [
                "attention mechanism",
                "Feature_of",
                "architecture"
            ]
        ]
    },
    {
        "text": "The results of experiments on the data collected from a real world microblogging service demonstrated that the proposed model outperforms state-of-the-art methods .",
        "triple_list": [
            [
                "data",
                "Evaluate_for",
                "model"
            ],
            [
                "model",
                "Compare",
                "state-of-the-art methods"
            ]
        ]
    },
    {
        "text": "By incorporating trigger words into the consideration , the relative improvement of the proposed method over the state-of-the-art method is around 9.4 % in the F1-score .",
        "triple_list": [
            [
                "method",
                "Compare",
                "state-of-the-art method"
            ],
            [
                "F1-score",
                "Evaluate_for",
                "state-of-the-art method"
            ]
        ]
    },
    {
        "text": "In this paper , we improve an unsupervised learning method using the Expectation-Maximization ( EM ) algorithm proposed by Nigam et al. for text classification problems in order to apply it to word sense disambiguation ( WSD ) problems .",
        "triple_list": [
            [
                "Expectation-Maximization ( EM ) algorithm",
                "Used_for",
                "unsupervised learning method"
            ],
            [
                "Expectation-Maximization ( EM ) algorithm",
                "Used_for",
                "text classification problems"
            ],
            [
                "it",
                "Used_for",
                "word sense disambiguation ( WSD ) problems"
            ]
        ]
    },
    {
        "text": "In experiments , we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2 .",
        "triple_list": [
            [
                "Japanese Dictionary Task",
                "Feature_of",
                "SENSEVAL2"
            ]
        ]
    },
    {
        "text": "Furthermore , our methods were confirmed to be effective also for verb WSD problems .",
        "triple_list": [
            [
                "methods",
                "Used_for",
                "verb WSD problems"
            ]
        ]
    },
    {
        "text": "Dividing sentences in chunks of words is a useful preprocessing step for parsing , information extraction and information retrieval .",
        "triple_list": [
            [
                "Dividing sentences in chunks of words",
                "Used_for",
                "parsing"
            ],
            [
                "Dividing sentences in chunks of words",
                "Used_for",
                "information extraction"
            ],
            [
                "Dividing sentences in chunks of words",
                "Used_for",
                "information retrieval"
            ],
            [
                "parsing",
                "Conjunction",
                "information extraction"
            ],
            [
                "information extraction",
                "Conjunction",
                "information retrieval"
            ]
        ]
    },
    {
        "text": "( Ramshaw and Marcus , 1995 ) have introduced a `` convenient '' data representation for chunking by converting it to a tagging task .",
        "triple_list": [
            [
                "data representation",
                "Used_for",
                "chunking"
            ]
        ]
    },
    {
        "text": "In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks .",
        "triple_list": [
            [
                "data representations",
                "Used_for",
                "recognizing noun phrase chunks"
            ]
        ]
    },
    {
        "text": "However , equipped with the most suitable data representation , our memory-based learning chunker was able to improve the best published chunking results for a standard data set .",
        "triple_list": [
            [
                "data representation",
                "Used_for",
                "memory-based learning chunker"
            ],
            [
                "data set",
                "Evaluate_for",
                "memory-based learning chunker"
            ]
        ]
    },
    {
        "text": "We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms , trained on a corpus of 1 million question/answer pairs collected from the Web .",
        "triple_list": [
            [
                "system",
                "Used_for",
                "FAQ-like questions and answers"
            ],
            [
                "noisy-channel architecture",
                "Used_for",
                "system"
            ],
            [
                "noisy-channel architecture",
                "Used_for",
                "language model"
            ],
            [
                "noisy-channel architecture",
                "Used_for",
                "transformation model"
            ]
        ]
    },
    {
        "text": "In this paper we evaluate four objective measures of speech with regards to intelligibility prediction of synthesized speech in diverse noisy situations .",
        "triple_list": [
            [
                "measures of speech",
                "Evaluate_for",
                "intelligibility prediction"
            ],
            [
                "synthesized speech",
                "Used_for",
                "intelligibility prediction"
            ],
            [
                "diverse noisy situations",
                "Feature_of",
                "synthesized speech"
            ]
        ]
    },
    {
        "text": "We evaluated three intel-ligibility measures , the Dau measure , the glimpse proportion and the Speech Intelligibility Index ( SII ) and a quality measure , the Perceptual Evaluation of Speech Quality ( PESQ ) .",
        "triple_list": [
            [
                "intel-ligibility measures",
                "Conjunction",
                "quality measure"
            ],
            [
                "Dau measure",
                "Hyponym_of",
                "intel-ligibility measures"
            ],
            [
                "Dau measure",
                "Conjunction",
                "glimpse proportion"
            ],
            [
                "glimpse proportion",
                "Hyponym_of",
                "intel-ligibility measures"
            ],
            [
                "glimpse proportion",
                "Conjunction",
                "Speech Intelligibility Index ( SII )"
            ],
            [
                "Speech Intelligibility Index ( SII )",
                "Hyponym_of",
                "intel-ligibility measures"
            ],
            [
                "Perceptual Evaluation of Speech Quality ( PESQ )",
                "Hyponym_of",
                "quality measure"
            ]
        ]
    },
    {
        "text": "For the generation of synthesized speech we used a state of the art HMM-based speech synthesis system .",
        "triple_list": [
            [
                "HMM-based speech synthesis system",
                "Used_for",
                "generation of synthesized speech"
            ]
        ]
    },
    {
        "text": "The noisy conditions comprised four additive noises .",
        "triple_list": [
            [
                "additive noises",
                "Part_of",
                "noisy conditions"
            ]
        ]
    },
    {
        "text": "The measures were compared with subjective intelligibility scores obtained in listening tests .",
        "triple_list": [
            [
                "measures",
                "Compare",
                "subjective intelligibility scores"
            ]
        ]
    },
    {
        "text": "The results show the Dau and the glimpse measures to be the best predictors of intelligibility , with correlations of around 0.83 to subjective scores .",
        "triple_list": [
            [
                "Dau",
                "Conjunction",
                "glimpse measures"
            ],
            [
                "Dau",
                "Hyponym_of",
                "predictors of intelligibility"
            ],
            [
                "Dau",
                "Compare",
                "subjective scores"
            ],
            [
                "glimpse measures",
                "Hyponym_of",
                "predictors of intelligibility"
            ],
            [
                "glimpse measures",
                "Compare",
                "subjective scores"
            ],
            [
                "correlations",
                "Evaluate_for",
                "Dau"
            ],
            [
                "correlations",
                "Evaluate_for",
                "glimpse measures"
            ]
        ]
    },
    {
        "text": "All measures gave less accurate predictions of intelligibility for synthetic speech than have previously been found for natural speech ; in particular the SII measure .",
        "triple_list": [
            [
                "measures",
                "Evaluate_for",
                "predictions of intelligibility"
            ],
            [
                "synthetic speech",
                "Used_for",
                "predictions of intelligibility"
            ],
            [
                "synthetic speech",
                "Compare",
                "natural speech"
            ],
            [
                "SII measure",
                "Hyponym_of",
                "measures"
            ]
        ]
    },
    {
        "text": "In additional experiments , we processed the synthesized speech by an ideal binary mask before adding noise .",
        "triple_list": [
            [
                "ideal binary mask",
                "Used_for",
                "synthesized speech"
            ]
        ]
    },
    {
        "text": "The Glimpse measure gave the most accurate intelligibility predictions in this situation .",
        "triple_list": [
            [
                "Glimpse measure",
                "Used_for",
                "intelligibility predictions"
            ]
        ]
    },
    {
        "text": "A '' graphics for vision '' approach is proposed to address the problem of reconstruction from a large and imperfect data set : reconstruction on demand by tensor voting , or ROD-TV .",
        "triple_list": [
            [
                "'' graphics for vision '' approach",
                "Used_for",
                "reconstruction"
            ],
            [
                "large and imperfect data set",
                "Used_for",
                "reconstruction"
            ],
            [
                "tensor voting",
                "Used_for",
                "reconstruction"
            ],
            [
                "tensor voting",
                "Conjunction",
                "ROD-TV"
            ],
            [
                "ROD-TV",
                "Used_for",
                "reconstruction"
            ]
        ]
    },
    {
        "text": "ROD-TV simultaneously delivers good efficiency and robust-ness , by adapting to a continuum of primitive connectivity , view dependence , and levels of detail ( LOD ) .",
        "triple_list": [
            [
                "efficiency",
                "Evaluate_for",
                "ROD-TV"
            ],
            [
                "robust-ness",
                "Evaluate_for",
                "ROD-TV"
            ],
            [
                "robust-ness",
                "Conjunction",
                "efficiency"
            ],
            [
                "view dependence",
                "Conjunction",
                "primitive connectivity"
            ],
            [
                "levels of detail ( LOD )",
                "Conjunction",
                "view dependence"
            ]
        ]
    },
    {
        "text": "Locally inferred surface elements are robust to noise and better capture local shapes .",
        "triple_list": [
            [
                "Locally inferred surface elements",
                "Used_for",
                "local shapes"
            ]
        ]
    },
    {
        "text": "By inferring per-vertex normals at sub-voxel precision on the fly , we can achieve interpolative shading .",
        "triple_list": [
            [
                "per-vertex normals",
                "Used_for",
                "interpolative shading"
            ],
            [
                "sub-voxel precision",
                "Feature_of",
                "per-vertex normals"
            ]
        ]
    },
    {
        "text": "By relaxing the mesh connectivity requirement , we extend ROD-TV and propose a simple but effective multiscale feature extraction algorithm .",
        "triple_list": [
            [
                "mesh connectivity requirement",
                "Used_for",
                "multiscale feature extraction algorithm"
            ],
            [
                "ROD-TV",
                "Used_for",
                "multiscale feature extraction algorithm"
            ]
        ]
    },
    {
        "text": "ROD-TV consists of a hierarchical data structure that encodes different levels of detail .",
        "triple_list": [
            [
                "hierarchical data structure",
                "Part_of",
                "ROD-TV"
            ]
        ]
    },
    {
        "text": "The local reconstruction algorithm is tensor voting .",
        "triple_list": [
            [
                "tensor voting",
                "Hyponym_of",
                "local reconstruction algorithm"
            ]
        ]
    },
    {
        "text": "It is applied on demand to the visible subset of data at a desired level of detail , by traversing the data hierarchy and collecting tensorial support in a neighborhood .",
        "triple_list": [
            [
                "traversing the data hierarchy",
                "Used_for",
                "It"
            ],
            [
                "traversing the data hierarchy",
                "Conjunction",
                "collecting tensorial support"
            ],
            [
                "collecting tensorial support",
                "Used_for",
                "It"
            ]
        ]
    },
    {
        "text": "Both rhetorical structure and punctuation have been helpful in discourse processing .",
        "triple_list": [
            [
                "rhetorical structure",
                "Conjunction",
                "punctuation"
            ],
            [
                "rhetorical structure",
                "Used_for",
                "discourse processing"
            ],
            [
                "punctuation",
                "Used_for",
                "discourse processing"
            ]
        ]
    },
    {
        "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts : Colon , Dash , Ellipsis , Exclamation Mark , Question Mark , and Semicolon .",
        "triple_list": [
            [
                "Chinese punctuation marks",
                "Part_of",
                "news commentary texts"
            ],
            [
                "Colon",
                "Hyponym_of",
                "Chinese punctuation marks"
            ],
            [
                "Colon",
                "Conjunction",
                "Dash"
            ],
            [
                "Dash",
                "Hyponym_of",
                "Chinese punctuation marks"
            ],
            [
                "Dash",
                "Conjunction",
                "Ellipsis"
            ],
            [
                "Ellipsis",
                "Hyponym_of",
                "Chinese punctuation marks"
            ],
            [
                "Ellipsis",
                "Conjunction",
                "Exclamation Mark"
            ],
            [
                "Exclamation Mark",
                "Hyponym_of",
                "Chinese punctuation marks"
            ],
            [
                "Exclamation Mark",
                "Conjunction",
                "Question Mark"
            ],
            [
                "Question Mark",
                "Hyponym_of",
                "Chinese punctuation marks"
            ],
            [
                "Question Mark",
                "Conjunction",
                "Semicolon"
            ],
            [
                "Semicolon",
                "Hyponym_of",
                "Chinese punctuation marks"
            ]
        ]
    },
    {
        "text": "The rhetorical patterns of these marks are compared against patterns around cue phrases in general .",
        "triple_list": [
            [
                "rhetorical patterns",
                "Feature_of",
                "marks"
            ],
            [
                "rhetorical patterns",
                "Compare",
                "patterns around cue phrases"
            ]
        ]
    },
    {
        "text": "Results show that these Chinese punctuation marks , though fewer in number than cue phrases , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in Chinese texts .",
        "triple_list": [
            [
                "Chinese punctuation marks",
                "Compare",
                "cue phrases"
            ],
            [
                "Chinese punctuation marks",
                "Used_for",
                "indicators of nuclearity"
            ],
            [
                "Chinese texts",
                "Feature_of",
                "indicators of nuclearity"
            ]
        ]
    },
    {
        "text": "The features based on Markov random field ( MRF ) models are usually sensitive to the rotation of image textures .",
        "triple_list": [
            [
                "Markov random field ( MRF ) models",
                "Used_for",
                "features"
            ]
        ]
    },
    {
        "text": "This paper develops an anisotropic circular Gaussian MRF ( ACGMRF ) model for modelling rotated image textures and retrieving rotation-invariant texture features .",
        "triple_list": [
            [
                "anisotropic circular Gaussian MRF ( ACGMRF ) model",
                "Used_for",
                "modelling rotated image textures"
            ],
            [
                "anisotropic circular Gaussian MRF ( ACGMRF ) model",
                "Used_for",
                "retrieving rotation-invariant texture features"
            ],
            [
                "modelling rotated image textures",
                "Conjunction",
                "retrieving rotation-invariant texture features"
            ]
        ]
    },
    {
        "text": "To overcome the singularity problem of the least squares estimate ( LSE ) method , an approximate least squares estimate ( ALSE ) method is proposed to estimate the parameters of the ACGMRF model .",
        "triple_list": [
            [
                "singularity problem",
                "Feature_of",
                "least squares estimate ( LSE ) method"
            ],
            [
                "approximate least squares estimate ( ALSE ) method",
                "Used_for",
                "parameters of the ACGMRF model"
            ]
        ]
    },
    {
        "text": "The rotation-invariant features can be obtained from the parameters of the ACGMRF model by the one-dimensional ( 1-D ) discrete Fourier transform ( DFT ) .",
        "triple_list": [
            [
                "parameters of the ACGMRF model",
                "Used_for",
                "rotation-invariant features"
            ],
            [
                "one-dimensional ( 1-D ) discrete Fourier transform ( DFT )",
                "Used_for",
                "rotation-invariant features"
            ]
        ]
    },
    {
        "text": "Significantly improved accuracy can be achieved by applying the rotation-invariant features to classify SAR ( synthetic aperture radar ) sea ice and Brodatz imagery .",
        "triple_list": [
            [
                "rotation-invariant features",
                "Used_for",
                "SAR ( synthetic aperture radar"
            ]
        ]
    },
    {
        "text": "Despite much recent progress on accurate semantic role labeling , previous work has largely used independent classifiers , possibly combined with separate label sequence models via Viterbi decoding .",
        "triple_list": [
            [
                "independent classifiers",
                "Used_for",
                "semantic role labeling"
            ],
            [
                "independent classifiers",
                "Conjunction",
                "label sequence models"
            ],
            [
                "Viterbi decoding",
                "Used_for",
                "label sequence models"
            ]
        ]
    },
    {
        "text": "We show how to build a joint model of argument frames , incorporating novel features that model these interactions into discriminative log-linear models .",
        "triple_list": [
            [
                "features",
                "Part_of",
                "discriminative log-linear models"
            ]
        ]
    },
    {
        "text": "This system achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank .",
        "triple_list": [
            [
                "error reduction",
                "Evaluate_for",
                "system"
            ],
            [
                "error reduction",
                "Evaluate_for",
                "independent classifier"
            ],
            [
                "independent classifier",
                "Compare",
                "system"
            ],
            [
                "gold-standard parse trees",
                "Evaluate_for",
                "system"
            ],
            [
                "gold-standard parse trees",
                "Evaluate_for",
                "independent classifier"
            ],
            [
                "gold-standard parse trees",
                "Part_of",
                "PropBank"
            ]
        ]
    },
    {
        "text": "In order to deal with ambiguity , the MORphological PArser MORPA is provided with a probabilistic context-free grammar ( PCFG ) , i.e. it combines a `` conventional '' context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse .",
        "triple_list": [
            [
                "MORphological PArser MORPA",
                "Used_for",
                "ambiguity"
            ],
            [
                "probabilistic context-free grammar ( PCFG )",
                "Used_for",
                "MORphological PArser MORPA"
            ],
            [
                "`` conventional '' context-free morphological grammar",
                "Used_for",
                "it"
            ],
            [
                "`` conventional '' context-free morphological grammar",
                "Used_for",
                "ungrammatical segmentations"
            ],
            [
                "probability-based scoring function",
                "Used_for",
                "it"
            ],
            [
                "probability-based scoring function",
                "Conjunction",
                "`` conventional '' context-free morphological grammar"
            ],
            [
                "probability-based scoring function",
                "Used_for",
                "parse"
            ]
        ]
    },
    {
        "text": "Test performance data will show that a PCFG yields good results in morphological parsing .",
        "triple_list": [
            [
                "PCFG",
                "Used_for",
                "morphological parsing"
            ]
        ]
    },
    {
        "text": "MORPA is a fully implemented parser developed for use in a text-to-speech conversion system .",
        "triple_list": [
            [
                "MORPA",
                "Hyponym_of",
                "parser"
            ],
            [
                "MORPA",
                "Used_for",
                "text-to-speech conversion system"
            ],
            [
                "parser",
                "Used_for",
                "text-to-speech conversion system"
            ]
        ]
    },
    {
        "text": "This paper describes the framework of a Korean phonological knowledge base system using the unification-based grammar formalism : Korean Phonology Structure Grammar ( KPSG ) .",
        "triple_list": [
            [
                "unification-based grammar formalism",
                "Used_for",
                "Korean phonological knowledge base system"
            ],
            [
                "Korean Phonology Structure Grammar ( KPSG )",
                "Hyponym_of",
                "unification-based grammar formalism"
            ]
        ]
    },
    {
        "text": "The approach of KPSG provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system .",
        "triple_list": [
            [
                "approach",
                "Used_for",
                "KPSG"
            ],
            [
                "KPSG",
                "Used_for",
                "phonological system"
            ]
        ]
    },
    {
        "text": "We show that the proposed approach is more describable than other approaches such as those employing a traditional generative phonological approach .",
        "triple_list": [
            [
                "approach",
                "Compare",
                "approaches"
            ],
            [
                "generative phonological approach",
                "Used_for",
                "those"
            ]
        ]
    },
    {
        "text": "In this paper , we study the design of core-selecting payment rules for such domains .",
        "triple_list": [
            [
                "design of core-selecting payment rules",
                "Used_for",
                "domains"
            ]
        ]
    },
    {
        "text": "We design two core-selecting rules that always satisfy IR in expectation .",
        "triple_list": [
            [
                "core-selecting rules",
                "Used_for",
                "IR"
            ]
        ]
    },
    {
        "text": "To study the performance of our rules we perform a computational Bayes-Nash equilibrium analysis .",
        "triple_list": [
            [
                "computational Bayes-Nash equilibrium analysis",
                "Used_for",
                "rules"
            ]
        ]
    },
    {
        "text": "We show that , in equilibrium , our new rules have better incentives , higher efficiency , and a lower rate of ex-post IR violations than standard core-selecting rules .",
        "triple_list": [
            [
                "rules",
                "Compare",
                "core-selecting rules"
            ],
            [
                "rate of ex-post IR violations",
                "Evaluate_for",
                "rules"
            ],
            [
                "rate of ex-post IR violations",
                "Evaluate_for",
                "core-selecting rules"
            ]
        ]
    },
    {
        "text": "In this paper , we will describe a search tool for a huge set of ngrams .",
        "triple_list": [
            [
                "search tool",
                "Used_for",
                "ngrams"
            ]
        ]
    },
    {
        "text": "This system can be a very useful tool for linguistic knowledge discovery and other NLP tasks .",
        "triple_list": [
            [
                "tool",
                "Used_for",
                "linguistic knowledge discovery"
            ],
            [
                "tool",
                "Used_for",
                "NLP tasks"
            ],
            [
                "linguistic knowledge discovery",
                "Conjunction",
                "NLP tasks"
            ]
        ]
    },
    {
        "text": "This paper explores the role of user modeling in such systems .",
        "triple_list": [
            [
                "user modeling",
                "Part_of",
                "systems"
            ]
        ]
    },
    {
        "text": "Since acquiring the knowledge for a user model is a fundamental problem in user modeling , a section is devoted to this topic .",
        "triple_list": [
            [
                "user model",
                "Used_for",
                "user modeling"
            ]
        ]
    },
    {
        "text": "Next , the benefits and costs of implementing a user modeling component for a system are weighed in light of several aspects of the interaction requirements that may be imposed by the system .",
        "triple_list": [
            [
                "user modeling component",
                "Part_of",
                "system"
            ]
        ]
    },
    {
        "text": "Information extraction techniques automatically create structured databases from unstructured data sources , such as the Web or newswire documents .",
        "triple_list": [
            [
                "Information extraction techniques",
                "Used_for",
                "structured databases"
            ],
            [
                "unstructured data sources",
                "Used_for",
                "Information extraction techniques"
            ],
            [
                "Web",
                "Hyponym_of",
                "unstructured data sources"
            ],
            [
                "Web",
                "Conjunction",
                "newswire documents"
            ],
            [
                "newswire documents",
                "Hyponym_of",
                "unstructured data sources"
            ]
        ]
    },
    {
        "text": "Despite the successes of these systems , accuracy will always be imperfect .",
        "triple_list": [
            [
                "accuracy",
                "Evaluate_for",
                "systems"
            ]
        ]
    },
    {
        "text": "The information extraction system we evaluate is based on a linear-chain conditional random field ( CRF ) , a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary , overlapping features of the input in a Markov model .",
        "triple_list": [
            [
                "linear-chain conditional random field ( CRF )",
                "Used_for",
                "information extraction system"
            ],
            [
                "linear-chain conditional random field ( CRF )",
                "Hyponym_of",
                "probabilistic model"
            ],
            [
                "probabilistic model",
                "Used_for",
                "information extraction tasks"
            ],
            [
                "probabilistic model",
                "Used_for",
                "arbitrary , overlapping features"
            ],
            [
                "arbitrary , overlapping features",
                "Feature_of",
                "input"
            ],
            [
                "arbitrary , overlapping features",
                "Part_of",
                "Markov model"
            ]
        ]
    },
    {
        "text": "We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records , obtaining an average precision of 98 % for retrieving correct fields and 87 % for multi-field records .",
        "triple_list": [
            [
                "extracted fields",
                "Conjunction",
                "multi-field records"
            ],
            [
                "average precision",
                "Evaluate_for",
                "techniques"
            ]
        ]
    },
    {
        "text": "In this paper , we use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries .",
        "triple_list": [
            [
                "information redundancy in multilingual input",
                "Used_for",
                "machine translation"
            ],
            [
                "information redundancy in multilingual input",
                "Used_for",
                "multilingual summaries"
            ]
        ]
    },
    {
        "text": "We demonstrate how errors in the machine translations of the input Arabic documents can be corrected by identifying and generating from such redundancy , focusing on noun phrases .",
        "triple_list": [
            [
                "Arabic documents",
                "Used_for",
                "machine translations"
            ]
        ]
    },
    {
        "text": "In this paper , we propose a new approach to generate oriented object proposals ( OOPs ) to reduce the detection error caused by various orientations of the object .",
        "triple_list": [
            [
                "approach",
                "Used_for",
                "oriented object proposals ( OOPs )"
            ],
            [
                "detection error",
                "Evaluate_for",
                "oriented object proposals ( OOPs )"
            ]
        ]
    },
    {
        "text": "To this end , we propose to efficiently locate object regions according to pixelwise object probability , rather than measuring the objectness from a set of sampled windows .",
        "triple_list": [
            [
                "pixelwise object probability",
                "Used_for",
                "object regions"
            ],
            [
                "pixelwise object probability",
                "Compare",
                "objectness"
            ]
        ]
    },
    {
        "text": "We formulate the proposal generation problem as a generative proba-bilistic model such that object proposals of different shapes ( i.e. , sizes and orientations ) can be produced by locating the local maximum likelihoods .",
        "triple_list": [
            [
                "generative proba-bilistic model",
                "Used_for",
                "proposal generation problem"
            ],
            [
                "shapes",
                "Feature_of",
                "object proposals"
            ],
            [
                "sizes",
                "Hyponym_of",
                "shapes"
            ],
            [
                "sizes",
                "Conjunction",
                "orientations"
            ],
            [
                "orientations",
                "Hyponym_of",
                "shapes"
            ],
            [
                "local maximum likelihoods",
                "Used_for",
                "object proposals"
            ]
        ]
    },
    {
        "text": "First , it helps the object detector handle objects of different orientations .",
        "triple_list": [
            [
                "object detector",
                "Used_for",
                "orientations"
            ]
        ]
    },
    {
        "text": "Third , it avoids massive window sampling , and thereby reducing the number of proposals while maintaining a high recall .",
        "triple_list": [
            [
                "it",
                "Used_for",
                "number of proposals"
            ],
            [
                "recall",
                "Evaluate_for",
                "it"
            ]
        ]
    },
    {
        "text": "Experiments on the PASCAL VOC 2007 dataset show that the proposed OOP outperforms the state-of-the-art fast methods .",
        "triple_list": [
            [
                "PASCAL VOC 2007 dataset",
                "Evaluate_for",
                "OOP"
            ],
            [
                "OOP",
                "Compare",
                "state-of-the-art fast methods"
            ]
        ]
    },
    {
        "text": "Further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art proposal generation methods in either object rotation scenarios or general scenarios .",
        "triple_list": [
            [
                "rotation invariant property",
                "Used_for",
                "class-specific object detector"
            ],
            [
                "class-specific object detector",
                "Compare",
                "proposal generation methods"
            ],
            [
                "object rotation scenarios",
                "Evaluate_for",
                "class-specific object detector"
            ],
            [
                "object rotation scenarios",
                "Evaluate_for",
                "proposal generation methods"
            ],
            [
                "object rotation scenarios",
                "Conjunction",
                "general scenarios"
            ],
            [
                "general scenarios",
                "Evaluate_for",
                "class-specific object detector"
            ],
            [
                "general scenarios",
                "Evaluate_for",
                "proposal generation methods"
            ]
        ]
    },
    {
        "text": "This paper describes three relatively domain-independent capabilities recently added to the Paramax spoken language understanding system : non-monotonic reasoning , implicit reference resolution , and database query paraphrase .",
        "triple_list": [
            [
                "domain-independent capabilities",
                "Part_of",
                "Paramax spoken language understanding system"
            ],
            [
                "non-monotonic reasoning",
                "Hyponym_of",
                "domain-independent capabilities"
            ],
            [
                "implicit reference resolution",
                "Hyponym_of",
                "domain-independent capabilities"
            ],
            [
                "database query paraphrase",
                "Hyponym_of",
                "domain-independent capabilities"
            ]
        ]
    },
    {
        "text": "Finally , we briefly describe an experiment which we have done in extending the n-best speech/language integration architecture to improving OCR accuracy .",
        "triple_list": [
            [
                "OCR accuracy",
                "Evaluate_for",
                "n-best speech/language integration architecture"
            ]
        ]
    },
    {
        "text": "We investigate the problem of fine-grained sketch-based image retrieval ( SBIR ) , where free-hand human sketches are used as queries to perform instance-level retrieval of images .",
        "triple_list": [
            [
                "free-hand human sketches",
                "Used_for",
                "instance-level retrieval of images"
            ]
        ]
    },
    {
        "text": "This is an extremely challenging task because ( i ) visual comparisons not only need to be fine-grained but also executed cross-domain , ( ii ) free-hand ( finger ) sketches are highly abstract , making fine-grained matching harder , and most importantly ( iii ) annotated cross-domain sketch-photo datasets required for training are scarce , challenging many state-of-the-art machine learning techniques .",
        "triple_list": [
            [
                "annotated cross-domain sketch-photo datasets",
                "Used_for",
                "machine learning techniques"
            ]
        ]
    },
    {
        "text": "We then develop a deep triplet-ranking model for instance-level SBIR with a novel data augmentation and staged pre-training strategy to alleviate the issue of insufficient fine-grained training data .",
        "triple_list": [
            [
                "deep triplet-ranking model",
                "Used_for",
                "instance-level SBIR"
            ],
            [
                "deep triplet-ranking model",
                "Used_for",
                "insufficient fine-grained training data"
            ],
            [
                "data augmentation",
                "Used_for",
                "deep triplet-ranking model"
            ],
            [
                "data augmentation",
                "Conjunction",
                "staged pre-training strategy"
            ],
            [
                "staged pre-training strategy",
                "Used_for",
                "deep triplet-ranking model"
            ]
        ]
    },
    {
        "text": "Extensive experiments are carried out to contribute a variety of insights into the challenges of data sufficiency and over-fitting avoidance when training deep networks for fine-grained cross-domain ranking tasks .",
        "triple_list": [
            [
                "data sufficiency",
                "Conjunction",
                "over-fitting avoidance"
            ],
            [
                "deep networks",
                "Used_for",
                "fine-grained cross-domain ranking tasks"
            ]
        ]
    },
    {
        "text": "In this paper we target at generating generic action proposals in unconstrained videos .",
        "triple_list": [
            [
                "unconstrained videos",
                "Used_for",
                "generic action proposals"
            ]
        ]
    },
    {
        "text": "Each action proposal corresponds to a temporal series of spatial bounding boxes , i.e. , a spatio-temporal video tube , which has a good potential to locate one human action .",
        "triple_list": [
            [
                "spatio-temporal video tube",
                "Hyponym_of",
                "temporal series of spatial bounding boxes"
            ],
            [
                "spatio-temporal video tube",
                "Used_for",
                "human action"
            ]
        ]
    },
    {
        "text": "Assuming each action is performed by a human with meaningful motion , both appearance and motion cues are utilized to measure the ac-tionness of the video tubes .",
        "triple_list": [
            [
                "appearance and motion cues",
                "Used_for",
                "ac-tionness"
            ],
            [
                "ac-tionness",
                "Evaluate_for",
                "video tubes"
            ]
        ]
    },
    {
        "text": "After picking those spatiotem-poral paths of high actionness scores , our action proposal generation is formulated as a maximum set coverage problem , where greedy search is performed to select a set of action proposals that can maximize the overall actionness score .",
        "triple_list": [
            [
                "maximum set coverage problem",
                "Used_for",
                "action proposal generation"
            ],
            [
                "greedy search",
                "Used_for",
                "action proposals"
            ],
            [
                "actionness score",
                "Evaluate_for",
                "action proposals"
            ]
        ]
    },
    {
        "text": "Compared with existing action proposal approaches , our action proposals do not rely on video segmentation and can be generated in nearly real-time .",
        "triple_list": [
            [
                "action proposal approaches",
                "Compare",
                "action proposals"
            ]
        ]
    },
    {
        "text": "Experimental results on two challenging datasets , MSRII and UCF 101 , validate the superior performance of our action proposals as well as competitive results on action detection and search .",
        "triple_list": [
            [
                "datasets",
                "Evaluate_for",
                "action proposals"
            ],
            [
                "MSRII",
                "Hyponym_of",
                "datasets"
            ],
            [
                "MSRII",
                "Conjunction",
                "UCF 101"
            ],
            [
                "UCF 101",
                "Hyponym_of",
                "datasets"
            ],
            [
                "action detection and search",
                "Evaluate_for",
                "action proposals"
            ]
        ]
    },
    {
        "text": "This paper reports recent research into methods for creating natural language text .",
        "triple_list": [
            [
                "methods",
                "Used_for",
                "creating natural language text"
            ]
        ]
    },
    {
        "text": "KDS ( Knowledge Delivery System ) , which embodies this paradigm , has distinct parts devoted to creation of the propositional units , to organization of the text , to prevention of excess redundancy , to creation of combinations of units , to evaluation of these combinations as potential sentences , to selection of the best among competing combinations , and to creation of the final text .",
        "triple_list": [
            [
                "paradigm",
                "Part_of",
                "KDS ( Knowledge Delivery System )"
            ]
        ]
    },
    {
        "text": "The Fragment-and-Compose paradigm and the computational methods of KDS are described .",
        "triple_list": [
            [
                "computational methods",
                "Used_for",
                "KDS"
            ]
        ]
    },
    {
        "text": "This paper explores the issue of using different co-occurrence similarities between terms for separating query terms that are useful for retrieval from those that are harmful .",
        "triple_list": [
            [
                "co-occurrence similarities",
                "Used_for",
                "query terms"
            ],
            [
                "query terms",
                "Used_for",
                "retrieval"
            ],
            [
                "those",
                "Compare",
                "query terms"
            ]
        ]
    },
    {
        "text": "The hypothesis under examination is that useful terms tend to be more similar to each other than to other query terms .",
        "triple_list": [
            [
                "useful terms",
                "Compare",
                "query terms"
            ]
        ]
    },
    {
        "text": "Preliminary experiments with similarities computed using first-order and second-order co-occurrence seem to confirm the hypothesis .",
        "triple_list": [
            [
                "first-order and second-order co-occurrence",
                "Used_for",
                "similarities"
            ]
        ]
    },
    {
        "text": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several , previously proposed phrase-based translation models .",
        "triple_list": [
            [
                "phrase-based translation model",
                "Conjunction",
                "decoding algorithm"
            ]
        ]
    },
    {
        "text": "Within our framework , we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models .",
        "triple_list": [
            [
                "phrase-based models",
                "Compare",
                "word-based models"
            ]
        ]
    },
    {
        "text": "Our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means : heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations .",
        "triple_list": [
            [
                "heuristic learning of phrase translations",
                "Hyponym_of",
                "means"
            ],
            [
                "word-based alignments",
                "Used_for",
                "heuristic learning of phrase translations"
            ],
            [
                "lexical weighting of phrase translations",
                "Hyponym_of",
                "means"
            ]
        ]
    },
    {
        "text": "Traditional methods for color constancy can improve surface re-flectance estimates from such uncalibrated images , but their output depends significantly on the background scene .",
        "triple_list": [
            [
                "methods",
                "Used_for",
                "color constancy"
            ],
            [
                "methods",
                "Used_for",
                "surface re-flectance estimates"
            ],
            [
                "uncalibrated images",
                "Used_for",
                "surface re-flectance estimates"
            ]
        ]
    },
    {
        "text": "We introduce the multi-view color constancy problem , and present a method to recover estimates of underlying surface re-flectance based on joint estimation of these surface properties and the illuminants present in multiple images .",
        "triple_list": [
            [
                "method",
                "Used_for",
                "estimates of underlying surface re-flectance"
            ]
        ]
    },
    {
        "text": "The method can exploit image correspondences obtained by various alignment techniques , and we show examples based on matching local region features .",
        "triple_list": [
            [
                "method",
                "Used_for",
                "image correspondences"
            ],
            [
                "alignment techniques",
                "Used_for",
                "image correspondences"
            ]
        ]
    },
    {
        "text": "Our results show that multi-view constraints can significantly improve estimates of both scene illuminants and object color ( surface reflectance ) when compared to a baseline single-view method .",
        "triple_list": [
            [
                "multi-view constraints",
                "Used_for",
                "estimates of both scene illuminants and object color ( surface reflectance )"
            ],
            [
                "baseline single-view method",
                "Compare",
                "multi-view constraints"
            ]
        ]
    },
    {
        "text": "Our contributions include a concise , modular architecture with reversible processes of understanding and generation , an information-state model of reference , and flexible links between semantics and collaborative problem solving .",
        "triple_list": [
            [
                "concise , modular architecture",
                "Used_for",
                "understanding"
            ],
            [
                "concise , modular architecture",
                "Used_for",
                "generation"
            ],
            [
                "understanding",
                "Conjunction",
                "generation"
            ]
        ]
    }
]