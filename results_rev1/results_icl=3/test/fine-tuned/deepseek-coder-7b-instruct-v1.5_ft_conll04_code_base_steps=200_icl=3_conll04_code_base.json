[{"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.5626423690205011, "Recall": 0.6068796068796068, "F1_Score": 0.5839243498817966, "rel_type_metrics": {"Live_in": {"precision": 0.427536231884058, "recall": 0.6020408163265306, "f1": 0.5}, "Located_in": {"precision": 0.5306122448979592, "recall": 0.5777777777777777, "f1": 0.5531914893617021}, "Work_for": {"precision": 0.6103896103896104, "recall": 0.618421052631579, "f1": 0.6143790849673204}, "Kill": {"precision": 0.8541666666666666, "recall": 0.8723404255319149, "f1": 0.8631578947368421}, "Organization_based_in": {"precision": 0.6153846153846154, "recall": 0.5, "f1": 0.5517241379310345}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-27-02-43-00", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.5829383886255924, "Recall": 0.6044226044226044, "F1_Score": 0.5934861278648974, "rel_type_metrics": {"Kill": {"precision": 0.8333333333333334, "recall": 0.851063829787234, "f1": 0.8421052631578947}, "Located_in": {"precision": 0.5681818181818182, "recall": 0.5555555555555556, "f1": 0.5617977528089888}, "Organization_based_in": {"precision": 0.7384615384615385, "recall": 0.5, "f1": 0.5962732919254659}, "Work_for": {"precision": 0.4083333333333333, "recall": 0.6447368421052632, "f1": 0.5}, "Live_in": {"precision": 0.5841584158415841, "recall": 0.6020408163265306, "f1": 0.5929648241206029}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-22-06-12", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.5667447306791569, "Recall": 0.5945945945945946, "F1_Score": 0.5803357314148682, "rel_type_metrics": {"Kill": {"precision": 0.8333333333333334, "recall": 0.851063829787234, "f1": 0.8421052631578947}, "Organization_based_in": {"precision": 0.4731182795698925, "recall": 0.4583333333333333, "f1": 0.46560846560846564}, "Work_for": {"precision": 0.5263157894736842, "recall": 0.5263157894736842, "f1": 0.5263157894736842}, "Live_in": {"precision": 0.5833333333333334, "recall": 0.6428571428571429, "f1": 0.6116504854368933}, "Located_in": {"precision": 0.5392156862745098, "recall": 0.6111111111111112, "f1": 0.5729166666666667}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-12-35-58", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": true}]