[{"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.4897959183673469, "Recall": 0.2171945701357466, "F1_Score": 0.3009404388714733, "rel_type_metrics": {"Adverse_effect": {"precision": 0.5133689839572193, "recall": 0.2171945701357466, "f1": 0.30524642289348175}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2025-01-11-14-42-57", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.5221445221445221, "Recall": 0.5067873303167421, "F1_Score": 0.5143513203214696, "rel_type_metrics": {"Adverse_effect": {"precision": 0.5270588235294118, "recall": 0.5067873303167421, "f1": 0.516724336793541}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2025-01-11-15-04-32", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.6145833333333334, "Recall": 0.667420814479638, "F1_Score": 0.6399132321041214, "rel_type_metrics": {"Adverse_effect": {"precision": 0.6171548117154811, "recall": 0.667420814479638, "f1": 0.6413043478260869}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2025-01-11-15-21-17", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": false, "chat_model": true}]