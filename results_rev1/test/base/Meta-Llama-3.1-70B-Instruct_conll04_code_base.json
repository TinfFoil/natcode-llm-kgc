[{"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.23537414965986395, "Recall": 0.4250614250614251, "F1_Score": 0.30297723292469353, "rel_type_metrics": {"Work_for": {"precision": 0.2578947368421053, "recall": 0.6447368421052632, "f1": 0.3684210526315789}, "Organization_based_in": {"precision": 0.3125, "recall": 0.20833333333333334, "f1": 0.25}, "Live_in": {"precision": 0.297029702970297, "recall": 0.30612244897959184, "f1": 0.30150753768844224}, "Kill": {"precision": 0.4603174603174603, "recall": 0.6170212765957447, "f1": 0.5272727272727272}, "Located_in": {"precision": 0.14423076923076922, "recall": 0.5, "f1": 0.22388059701492538}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-12-03-23-23", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.1482059282371295, "Recall": 0.2334152334152334, "F1_Score": 0.18129770992366412, "rel_type_metrics": {"Located_in": {"precision": 0.07756232686980609, "recall": 0.3111111111111111, "f1": 0.12416851441241684}, "Live_in": {"precision": 0.23170731707317074, "recall": 0.19387755102040816, "f1": 0.21111111111111114}, "Organization_based_in": {"precision": 0.07692307692307693, "recall": 0.010416666666666666, "f1": 0.018348623853211007}, "Kill": {"precision": 0.3333333333333333, "recall": 0.2765957446808511, "f1": 0.3023255813953488}, "Work_for": {"precision": 0.2786885245901639, "recall": 0.4473684210526316, "f1": 0.3434343434343435}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-12-04-12-47", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.19141914191419143, "Recall": 0.28501228501228504, "F1_Score": 0.2290227048371175, "rel_type_metrics": {"Organization_based_in": {"precision": 0.25, "recall": 0.020833333333333332, "f1": 0.038461538461538464}, "Work_for": {"precision": 0.2672413793103448, "recall": 0.40789473684210525, "f1": 0.3229166666666667}, "Live_in": {"precision": 0.26436781609195403, "recall": 0.23469387755102042, "f1": 0.24864864864864863}, "Kill": {"precision": 0.4153846153846154, "recall": 0.574468085106383, "f1": 0.48214285714285715}, "Located_in": {"precision": 0.102803738317757, "recall": 0.36666666666666664, "f1": 0.16058394160583941}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-12-05-13-07", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}]