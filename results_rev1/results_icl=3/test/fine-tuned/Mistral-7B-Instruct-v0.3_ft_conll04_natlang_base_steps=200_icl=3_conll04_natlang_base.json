[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.6209150326797386, "Recall": 0.7002457002457002, "F1_Score": 0.6581986143187066, "rel_type_metrics": {"Work_for": {"precision": 0.4811320754716981, "recall": 0.6710526315789473, "f1": 0.5604395604395604}, "Located_in": {"precision": 0.7142857142857143, "recall": 0.6666666666666666, "f1": 0.689655172413793}, "Live_in": {"precision": 0.5691056910569106, "recall": 0.7142857142857143, "f1": 0.6334841628959277}, "Kill": {"precision": 0.8461538461538461, "recall": 0.9361702127659575, "f1": 0.8888888888888888}, "Organization_based_in": {"precision": 0.6382978723404256, "recall": 0.625, "f1": 0.631578947368421}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-27-00-28-12", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.6420323325635104, "Recall": 0.683046683046683, "F1_Score": 0.661904761904762, "rel_type_metrics": {"Kill": {"precision": 0.8431372549019608, "recall": 0.9148936170212766, "f1": 0.8775510204081632}, "Work_for": {"precision": 0.4722222222222222, "recall": 0.6710526315789473, "f1": 0.5543478260869564}, "Live_in": {"precision": 0.6339285714285714, "recall": 0.7244897959183674, "f1": 0.6761904761904762}, "Organization_based_in": {"precision": 0.691358024691358, "recall": 0.5833333333333334, "f1": 0.6327683615819208}, "Located_in": {"precision": 0.7037037037037037, "recall": 0.6333333333333333, "f1": 0.6666666666666667}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-19-19-18", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.6353211009174312, "Recall": 0.6805896805896806, "F1_Score": 0.6571767497034401, "rel_type_metrics": {"Organization_based_in": {"precision": 0.4596774193548387, "recall": 0.59375, "f1": 0.5181818181818182}, "Kill": {"precision": 0.8431372549019608, "recall": 0.9148936170212766, "f1": 0.8775510204081632}, "Live_in": {"precision": 0.5964912280701754, "recall": 0.6938775510204082, "f1": 0.6415094339622641}, "Located_in": {"precision": 0.7307692307692307, "recall": 0.6333333333333333, "f1": 0.6785714285714285}, "Work_for": {"precision": 0.7536231884057971, "recall": 0.6842105263157895, "f1": 0.7172413793103448}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-09-02-05", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}]