[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=6", "Precision": 0.6633906633906634, "Recall": 0.6633906633906634, "F1_Score": 0.6633906633906634, "rel_type_metrics": {"Kill": {"precision": 0.8723404255319149, "recall": 0.8723404255319149, "f1": 0.8723404255319149}, "Located_in": {"precision": 0.6145833333333334, "recall": 0.6555555555555556, "f1": 0.6344086021505377}, "Live_in": {"precision": 0.7, "recall": 0.7142857142857143, "f1": 0.7070707070707072}, "Work_for": {"precision": 0.5679012345679012, "recall": 0.6052631578947368, "f1": 0.5859872611464968}, "Organization_based_in": {"precision": 0.6506024096385542, "recall": 0.5625, "f1": 0.6033519553072625}}, "n_icl_samples": 6, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-33-45", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=6", "Precision": 0.6594202898550725, "Recall": 0.6707616707616708, "F1_Score": 0.6650426309378807, "rel_type_metrics": {"Live_in": {"precision": 0.5877192982456141, "recall": 0.6836734693877551, "f1": 0.6320754716981133}, "Organization_based_in": {"precision": 0.6222222222222222, "recall": 0.5833333333333334, "f1": 0.6021505376344085}, "Located_in": {"precision": 0.6224489795918368, "recall": 0.6777777777777778, "f1": 0.6489361702127661}, "Work_for": {"precision": 0.746031746031746, "recall": 0.618421052631579, "f1": 0.6762589928057554}, "Kill": {"precision": 0.8571428571428571, "recall": 0.8936170212765957, "f1": 0.875}}, "n_icl_samples": 6, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-36-39", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=6", "Precision": 0.6674876847290641, "Recall": 0.6658476658476659, "F1_Score": 0.6666666666666667, "rel_type_metrics": {"Organization_based_in": {"precision": 0.6842105263157895, "recall": 0.5416666666666666, "f1": 0.6046511627906976}, "Live_in": {"precision": 0.693069306930693, "recall": 0.7142857142857143, "f1": 0.7035175879396985}, "Work_for": {"precision": 0.5866666666666667, "recall": 0.5789473684210527, "f1": 0.5827814569536424}, "Located_in": {"precision": 0.5981308411214953, "recall": 0.7111111111111111, "f1": 0.6497461928934011}, "Kill": {"precision": 0.8723404255319149, "recall": 0.8723404255319149, "f1": 0.8723404255319149}}, "n_icl_samples": 6, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-39-28", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}]