[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=6", "Precision": 0.6748166259168704, "Recall": 0.6781326781326781, "F1_Score": 0.6764705882352942, "rel_type_metrics": {"Located_in": {"precision": 0.6559139784946236, "recall": 0.6777777777777778, "f1": 0.6666666666666666}, "Live_in": {"precision": 0.6, "recall": 0.673469387755102, "f1": 0.6346153846153846}, "Organization_based_in": {"precision": 0.6590909090909091, "recall": 0.6041666666666666, "f1": 0.6304347826086956}, "Work_for": {"precision": 0.7272727272727273, "recall": 0.631578947368421, "f1": 0.676056338028169}, "Kill": {"precision": 0.8431372549019608, "recall": 0.9148936170212766, "f1": 0.8775510204081632}}, "n_icl_samples": 6, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-03-18", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=6", "Precision": 0.6878048780487804, "Recall": 0.6928746928746928, "F1_Score": 0.6903304773561811, "rel_type_metrics": {"Organization_based_in": {"precision": 0.7126436781609196, "recall": 0.6458333333333334, "f1": 0.6775956284153006}, "Work_for": {"precision": 0.7384615384615385, "recall": 0.631578947368421, "f1": 0.6808510638297872}, "Kill": {"precision": 0.8936170212765957, "recall": 0.8936170212765957, "f1": 0.8936170212765957}, "Live_in": {"precision": 0.6448598130841121, "recall": 0.7040816326530612, "f1": 0.673170731707317}, "Located_in": {"precision": 0.5980392156862745, "recall": 0.6777777777777778, "f1": 0.6354166666666667}}, "n_icl_samples": 6, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-05-16", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=6", "Precision": 0.6955445544554455, "Recall": 0.6904176904176904, "F1_Score": 0.6929716399506781, "rel_type_metrics": {"Kill": {"precision": 0.9148936170212766, "recall": 0.9148936170212766, "f1": 0.9148936170212766}, "Located_in": {"precision": 0.6888888888888889, "recall": 0.6888888888888889, "f1": 0.6888888888888889}, "Live_in": {"precision": 0.6634615384615384, "recall": 0.7040816326530612, "f1": 0.6831683168316831}, "Organization_based_in": {"precision": 0.6224489795918368, "recall": 0.6354166666666666, "f1": 0.6288659793814433}, "Work_for": {"precision": 0.7301587301587301, "recall": 0.6052631578947368, "f1": 0.6618705035971223}}, "n_icl_samples": 6, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-07-16", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}]