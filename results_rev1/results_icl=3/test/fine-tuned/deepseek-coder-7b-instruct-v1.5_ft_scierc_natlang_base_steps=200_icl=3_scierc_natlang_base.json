[{"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_scierc_natlang_base_steps=200_icl=3", "Precision": 0.3333333333333333, "Recall": 0.34188911704312114, "F1_Score": 0.33755701976685254, "rel_type_metrics": {"Feature_of": {"precision": 0.0136986301369863, "recall": 0.01694915254237288, "f1": 0.01515151515151515}, "Evaluate_for": {"precision": 0.3058823529411765, "recall": 0.2857142857142857, "f1": 0.29545454545454547}, "Part_of": {"precision": 0.4166666666666667, "recall": 0.23809523809523808, "f1": 0.30303030303030304}, "Conjunction": {"precision": 0.4394904458598726, "recall": 0.5609756097560976, "f1": 0.4928571428571428}, "Used_for": {"precision": 0.33213644524236985, "recall": 0.34709193245778613, "f1": 0.3394495412844037}, "Hyponym_of": {"precision": 0.42105263157894735, "recall": 0.3582089552238806, "f1": 0.3870967741935484}, "Compare": {"precision": 0.38235294117647056, "recall": 0.34210526315789475, "f1": 0.3611111111111111}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2024-12-27-01-15-05", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_scierc_natlang_base_steps=200_icl=3", "Precision": 0.34018499486125386, "Recall": 0.3398357289527721, "F1_Score": 0.3400102722136621, "rel_type_metrics": {"Used_for": {"precision": 0.32809773123909247, "recall": 0.3527204502814259, "f1": 0.3399638336347197}, "Part_of": {"precision": 0.3170731707317073, "recall": 0.20634920634920634, "f1": 0.25}, "Evaluate_for": {"precision": 0.33783783783783783, "recall": 0.27472527472527475, "f1": 0.30303030303030304}, "Hyponym_of": {"precision": 0.4444444444444444, "recall": 0.3582089552238806, "f1": 0.396694214876033}, "Feature_of": {"precision": 0.01818181818181818, "recall": 0.01694915254237288, "f1": 0.017543859649122806}, "Conjunction": {"precision": 0.48226950354609927, "recall": 0.5528455284552846, "f1": 0.5151515151515151}, "Compare": {"precision": 0.36363636363636365, "recall": 0.3157894736842105, "f1": 0.3380281690140845}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2024-12-28-20-09-44", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_scierc_natlang_base_steps=200_icl=3", "Precision": 0.3417593528816987, "Recall": 0.3470225872689938, "F1_Score": 0.3443708609271523, "rel_type_metrics": {"Conjunction": {"precision": 0.4825174825174825, "recall": 0.5609756097560976, "f1": 0.518796992481203}, "Hyponym_of": {"precision": 0.43636363636363634, "recall": 0.3582089552238806, "f1": 0.3934426229508196}, "Used_for": {"precision": 0.33568904593639576, "recall": 0.35647279549718575, "f1": 0.34576888080072793}, "Evaluate_for": {"precision": 0.28125, "recall": 0.2967032967032967, "f1": 0.28877005347593576}, "Part_of": {"precision": 0.3, "recall": 0.19047619047619047, "f1": 0.23300970873786406}, "Feature_of": {"precision": 0.05357142857142857, "recall": 0.05084745762711865, "f1": 0.052173913043478265}, "Compare": {"precision": 0.3939393939393939, "recall": 0.34210526315789475, "f1": 0.3661971830985915}}, "n_icl_samples": 3, "n_samples_test": 397, "dataset": "scierc", "date": "2024-12-29-10-02-46", "schema_path": "./data/codekgc-data/scierc/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}]