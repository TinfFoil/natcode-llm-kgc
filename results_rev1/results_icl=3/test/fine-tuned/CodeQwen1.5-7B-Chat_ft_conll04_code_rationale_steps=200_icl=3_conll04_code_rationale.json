[{"Model": "./models/CodeQwen1.5-7B-Chat_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.4713541666666667, "Recall": 0.44471744471744473, "F1_Score": 0.45764854614412137, "rel_type_metrics": {"Work_for": {"precision": 0.5222222222222223, "recall": 0.618421052631579, "f1": 0.5662650602409638}, "Live_in": {"precision": 0.6226415094339622, "recall": 0.336734693877551, "f1": 0.4370860927152318}, "Organization_based_in": {"precision": 0.37662337662337664, "recall": 0.3020833333333333, "f1": 0.33526011560693647}, "Located_in": {"precision": 0.32456140350877194, "recall": 0.4111111111111111, "f1": 0.3627450980392157}, "Kill": {"precision": 0.7142857142857143, "recall": 0.7446808510638298, "f1": 0.7291666666666666}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-16-55-31", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/CodeQwen1.5-7B-Chat_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.41596638655462187, "Recall": 0.4864864864864865, "F1_Score": 0.44847112117780297, "rel_type_metrics": {"Organization_based_in": {"precision": 0.39090909090909093, "recall": 0.4479166666666667, "f1": 0.41747572815533984}, "Located_in": {"precision": 0.2727272727272727, "recall": 0.4666666666666667, "f1": 0.3442622950819672}, "Work_for": {"precision": 0.3939393939393939, "recall": 0.5131578947368421, "f1": 0.44571428571428573}, "Live_in": {"precision": 0.6190476190476191, "recall": 0.3979591836734694, "f1": 0.48447204968944096}, "Kill": {"precision": 0.7142857142857143, "recall": 0.7446808510638298, "f1": 0.7291666666666666}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-04-52-15", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/CodeQwen1.5-7B-Chat_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.47549019607843135, "Recall": 0.47665847665847666, "F1_Score": 0.47607361963190187, "rel_type_metrics": {"Live_in": {"precision": 0.65, "recall": 0.3979591836734694, "f1": 0.49367088607594933}, "Located_in": {"precision": 0.30973451327433627, "recall": 0.3888888888888889, "f1": 0.3448275862068965}, "Work_for": {"precision": 0.5168539325842697, "recall": 0.6052631578947368, "f1": 0.5575757575757576}, "Kill": {"precision": 0.7254901960784313, "recall": 0.7872340425531915, "f1": 0.7551020408163265}, "Organization_based_in": {"precision": 0.3894736842105263, "recall": 0.3854166666666667, "f1": 0.387434554973822}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-04-54-24", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}]