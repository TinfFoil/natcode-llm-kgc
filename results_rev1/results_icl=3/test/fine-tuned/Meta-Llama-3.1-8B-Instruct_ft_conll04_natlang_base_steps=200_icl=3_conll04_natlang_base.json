[{"Model": "./models/Meta-Llama-3.1-8B-Instruct_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.52, "Recall": 0.5749385749385749, "F1_Score": 0.5460910151691948, "rel_type_metrics": {"Work_for": {"precision": 0.6901408450704225, "recall": 0.6447368421052632, "f1": 0.6666666666666666}, "Located_in": {"precision": 0.3402777777777778, "recall": 0.5444444444444444, "f1": 0.4188034188034188}, "Organization_based_in": {"precision": 0.6710526315789473, "recall": 0.53125, "f1": 0.5930232558139534}, "Kill": {"precision": 0.7843137254901961, "recall": 0.851063829787234, "f1": 0.8163265306122448}, "Live_in": {"precision": 0.4166666666666667, "recall": 0.45918367346938777, "f1": 0.4368932038834952}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-27-01-27-21", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-8B-Instruct_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.5054466230936819, "Recall": 0.5700245700245701, "F1_Score": 0.5357967667436491, "rel_type_metrics": {"Work_for": {"precision": 0.6911764705882353, "recall": 0.618421052631579, "f1": 0.6527777777777778}, "Located_in": {"precision": 0.41739130434782606, "recall": 0.5333333333333333, "f1": 0.4682926829268293}, "Live_in": {"precision": 0.42342342342342343, "recall": 0.47959183673469385, "f1": 0.4497607655502392}, "Organization_based_in": {"precision": 0.4260869565217391, "recall": 0.5104166666666666, "f1": 0.46445497630331756}, "Kill": {"precision": 0.82, "recall": 0.8723404255319149, "f1": 0.8453608247422681}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-20-23-13", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-8B-Instruct_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.5563380281690141, "Recall": 0.5823095823095823, "F1_Score": 0.5690276110444179, "rel_type_metrics": {"Kill": {"precision": 0.8269230769230769, "recall": 0.9148936170212766, "f1": 0.8686868686868686}, "Located_in": {"precision": 0.5581395348837209, "recall": 0.5333333333333333, "f1": 0.5454545454545454}, "Live_in": {"precision": 0.32867132867132864, "recall": 0.47959183673469385, "f1": 0.39004149377593356}, "Organization_based_in": {"precision": 0.7432432432432432, "recall": 0.5729166666666666, "f1": 0.6470588235294117}, "Work_for": {"precision": 0.6197183098591549, "recall": 0.5789473684210527, "f1": 0.5986394557823128}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-10-15-57", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}]