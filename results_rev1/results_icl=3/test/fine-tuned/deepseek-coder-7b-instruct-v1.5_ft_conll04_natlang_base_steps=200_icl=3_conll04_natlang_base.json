[{"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.5298165137614679, "Recall": 0.5675675675675675, "F1_Score": 0.5480427046263345, "rel_type_metrics": {"Located_in": {"precision": 0.5050505050505051, "recall": 0.5555555555555556, "f1": 0.5291005291005292}, "Live_in": {"precision": 0.38345864661654133, "recall": 0.5204081632653061, "f1": 0.4415584415584416}, "Organization_based_in": {"precision": 0.6075949367088608, "recall": 0.5, "f1": 0.5485714285714285}, "Work_for": {"precision": 0.5512820512820513, "recall": 0.5657894736842105, "f1": 0.5584415584415585}, "Kill": {"precision": 0.8297872340425532, "recall": 0.8297872340425532, "f1": 0.8297872340425532}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-27-01-13-47", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.5365296803652968, "Recall": 0.5773955773955773, "F1_Score": 0.5562130177514794, "rel_type_metrics": {"Organization_based_in": {"precision": 0.5280898876404494, "recall": 0.4895833333333333, "f1": 0.5081081081081081}, "Kill": {"precision": 0.8297872340425532, "recall": 0.8297872340425532, "f1": 0.8297872340425532}, "Work_for": {"precision": 0.573170731707317, "recall": 0.618421052631579, "f1": 0.5949367088607596}, "Located_in": {"precision": 0.46296296296296297, "recall": 0.5555555555555556, "f1": 0.505050505050505}, "Live_in": {"precision": 0.4642857142857143, "recall": 0.5306122448979592, "f1": 0.49523809523809526}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-20-08-24", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "./models/deepseek-coder-7b-instruct-v1.5_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.515695067264574, "Recall": 0.5651105651105651, "F1_Score": 0.5392731535756154, "rel_type_metrics": {"Work_for": {"precision": 0.4056603773584906, "recall": 0.5657894736842105, "f1": 0.47252747252747257}, "Located_in": {"precision": 0.4895833333333333, "recall": 0.5222222222222223, "f1": 0.5053763440860215}, "Organization_based_in": {"precision": 0.5568181818181818, "recall": 0.5104166666666666, "f1": 0.532608695652174}, "Kill": {"precision": 0.8125, "recall": 0.8297872340425532, "f1": 0.8210526315789474}, "Live_in": {"precision": 0.48148148148148145, "recall": 0.5306122448979592, "f1": 0.5048543689320388}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-10-01-27", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": true}]