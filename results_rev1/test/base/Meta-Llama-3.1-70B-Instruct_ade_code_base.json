[{"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.1590909090909091, "Recall": 0.01583710407239819, "F1_Score": 0.028806584362139915, "rel_type_metrics": {"Adverse_effect": {"precision": 0.28, "recall": 0.01583710407239819, "f1": 0.029978586723768737}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2025-01-12-01-57-02", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.4512820512820513, "Recall": 0.19909502262443438, "F1_Score": 0.27629513343799056, "rel_type_metrics": {"Adverse_effect": {"precision": 0.5269461077844312, "recall": 0.19909502262443438, "f1": 0.28899835796387524}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2025-01-12-02-20-53", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-70B-Instruct", "Precision": 0.3888888888888889, "Recall": 0.03167420814479638, "F1_Score": 0.058577405857740586, "rel_type_metrics": {"Adverse_effect": {"precision": 0.4375, "recall": 0.03167420814479638, "f1": 0.05907172995780591}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2025-01-12-02-45-03", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}]