[{"Model": "mistralai/Mistral-7B-Instruct-v0.3", "Precision": 0.18838028169014084, "Recall": 0.2420814479638009, "F1_Score": 0.2118811881188119, "rel_type_metrics": {"Adverse_effect": {"precision": 0.2772020725388601, "recall": 0.2420814479638009, "f1": 0.2584541062801932}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-29-00-28-31", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "mistralai/Mistral-7B-Instruct-v0.3", "Precision": 0.0777027027027027, "Recall": 0.10407239819004525, "F1_Score": 0.08897485493230174, "rel_type_metrics": {"Adverse_effect": {"precision": 0.215962441314554, "recall": 0.10407239819004525, "f1": 0.14045801526717558}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-29-02-55-11", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": true, "chat_model": true}, {"Model": "mistralai/Mistral-7B-Instruct-v0.3", "Precision": 0.18761726078799248, "Recall": 0.22624434389140272, "F1_Score": 0.20512820512820512, "rel_type_metrics": {"Adverse_effect": {"precision": 0.31446540880503143, "recall": 0.22624434389140272, "f1": 0.26315789473684204}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-29-02-56-56", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": true, "natlang": true, "chat_model": true}]