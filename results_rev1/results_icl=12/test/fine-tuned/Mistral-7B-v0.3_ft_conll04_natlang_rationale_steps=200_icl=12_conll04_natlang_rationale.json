[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=12", "Precision": 0.6684073107049608, "Recall": 0.628992628992629, "F1_Score": 0.6481012658227847, "rel_type_metrics": {"Work_for": {"precision": 0.6716417910447762, "recall": 0.5921052631578947, "f1": 0.6293706293706294}, "Located_in": {"precision": 0.6428571428571429, "recall": 0.6, "f1": 0.6206896551724138}, "Live_in": {"precision": 0.6923076923076923, "recall": 0.6428571428571429, "f1": 0.6666666666666666}, "Organization_based_in": {"precision": 0.6067415730337079, "recall": 0.5625, "f1": 0.5837837837837837}, "Kill": {"precision": 0.7843137254901961, "recall": 0.851063829787234, "f1": 0.8163265306122448}}, "n_icl_samples": 12, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-18-13-09", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=12", "Precision": 0.6648936170212766, "Recall": 0.6142506142506142, "F1_Score": 0.6385696040868454, "rel_type_metrics": {"Kill": {"precision": 0.7843137254901961, "recall": 0.851063829787234, "f1": 0.8163265306122448}, "Located_in": {"precision": 0.5510204081632653, "recall": 0.6, "f1": 0.5744680851063829}, "Work_for": {"precision": 0.7377049180327869, "recall": 0.5921052631578947, "f1": 0.656934306569343}, "Organization_based_in": {"precision": 0.5875, "recall": 0.4895833333333333, "f1": 0.5340909090909091}, "Live_in": {"precision": 0.7529411764705882, "recall": 0.6530612244897959, "f1": 0.6994535519125683}}, "n_icl_samples": 12, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-18-17-36", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_rationale_steps=200_icl=12", "Precision": 0.7018970189701897, "Recall": 0.6363636363636364, "F1_Score": 0.6675257731958762, "rel_type_metrics": {"Work_for": {"precision": 0.7580645161290323, "recall": 0.618421052631579, "f1": 0.6811594202898551}, "Live_in": {"precision": 0.6739130434782609, "recall": 0.6326530612244898, "f1": 0.6526315789473685}, "Located_in": {"precision": 0.6352941176470588, "recall": 0.6, "f1": 0.6171428571428571}, "Kill": {"precision": 0.82, "recall": 0.8723404255319149, "f1": 0.8453608247422681}, "Organization_based_in": {"precision": 0.6875, "recall": 0.5729166666666666, "f1": 0.625}}, "n_icl_samples": 12, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-18-21-52", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}]