[{"Model": "./models/Meta-Llama-3.1-8B-Instruct_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.4834710743801653, "Recall": 0.5749385749385749, "F1_Score": 0.5252525252525252, "rel_type_metrics": {"Located_in": {"precision": 0.5054945054945055, "recall": 0.5111111111111111, "f1": 0.5082872928176795}, "Organization_based_in": {"precision": 0.31547619047619047, "recall": 0.5520833333333334, "f1": 0.4015151515151515}, "Kill": {"precision": 0.8541666666666666, "recall": 0.8723404255319149, "f1": 0.8631578947368421}, "Live_in": {"precision": 0.44036697247706424, "recall": 0.4897959183673469, "f1": 0.463768115942029}, "Work_for": {"precision": 0.6764705882352942, "recall": 0.6052631578947368, "f1": 0.6388888888888888}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-27-03-09-37", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-8B-Instruct_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.46705426356589147, "Recall": 0.5921375921375921, "F1_Score": 0.5222101841820151, "rel_type_metrics": {"Work_for": {"precision": 0.3673469387755102, "recall": 0.7105263157894737, "f1": 0.4843049327354261}, "Live_in": {"precision": 0.3157894736842105, "recall": 0.42857142857142855, "f1": 0.36363636363636365}, "Organization_based_in": {"precision": 0.5913978494623656, "recall": 0.5729166666666666, "f1": 0.5820105820105821}, "Kill": {"precision": 0.84, "recall": 0.8936170212765957, "f1": 0.8659793814432989}, "Located_in": {"precision": 0.5161290322580645, "recall": 0.5333333333333333, "f1": 0.5245901639344263}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-22-33-44", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "./models/Meta-Llama-3.1-8B-Instruct_ft_conll04_code_base_steps=200_icl=3", "Precision": 0.5135746606334841, "Recall": 0.5577395577395577, "F1_Score": 0.5347467608951707, "rel_type_metrics": {"Located_in": {"precision": 0.5487804878048781, "recall": 0.5, "f1": 0.5232558139534884}, "Kill": {"precision": 0.3620689655172414, "recall": 0.8936170212765957, "f1": 0.5153374233128835}, "Work_for": {"precision": 0.6470588235294118, "recall": 0.5789473684210527, "f1": 0.6111111111111113}, "Organization_based_in": {"precision": 0.6309523809523809, "recall": 0.5520833333333334, "f1": 0.5888888888888888}, "Live_in": {"precision": 0.4673913043478261, "recall": 0.4387755102040816, "f1": 0.45263157894736844}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-13-05-56", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": true}]