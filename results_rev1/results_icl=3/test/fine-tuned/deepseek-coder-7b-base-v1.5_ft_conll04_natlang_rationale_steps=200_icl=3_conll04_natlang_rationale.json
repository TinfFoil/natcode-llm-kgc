[{"Model": "./models/deepseek-coder-7b-base-v1.5_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.6077922077922078, "Recall": 0.5749385749385749, "F1_Score": 0.5909090909090909, "rel_type_metrics": {"Organization_based_in": {"precision": 0.8135593220338984, "recall": 0.5, "f1": 0.6193548387096774}, "Live_in": {"precision": 0.6842105263157895, "recall": 0.5306122448979592, "f1": 0.5977011494252874}, "Work_for": {"precision": 0.6081081081081081, "recall": 0.5921052631578947, "f1": 0.6}, "Located_in": {"precision": 0.6153846153846154, "recall": 0.5333333333333333, "f1": 0.5714285714285715}, "Kill": {"precision": 0.41836734693877553, "recall": 0.8723404255319149, "f1": 0.5655172413793104}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-26-20-45-56", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/deepseek-coder-7b-base-v1.5_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.5738498789346247, "Recall": 0.5823095823095823, "F1_Score": 0.578048780487805, "rel_type_metrics": {"Organization_based_in": {"precision": 0.8064516129032258, "recall": 0.5208333333333334, "f1": 0.6329113924050633}, "Kill": {"precision": 0.803921568627451, "recall": 0.8723404255319149, "f1": 0.8367346938775511}, "Located_in": {"precision": 0.35294117647058826, "recall": 0.5333333333333333, "f1": 0.4247787610619469}, "Work_for": {"precision": 0.5844155844155844, "recall": 0.5921052631578947, "f1": 0.5882352941176471}, "Live_in": {"precision": 0.6091954022988506, "recall": 0.5408163265306123, "f1": 0.572972972972973}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-14-12-57", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}, {"Model": "./models/deepseek-coder-7b-base-v1.5_ft_conll04_natlang_rationale_steps=200_icl=3", "Precision": 0.573170731707317, "Recall": 0.5773955773955773, "F1_Score": 0.5752753977968176, "rel_type_metrics": {"Located_in": {"precision": 0.5604395604395604, "recall": 0.5666666666666667, "f1": 0.56353591160221}, "Organization_based_in": {"precision": 0.8070175438596491, "recall": 0.4791666666666667, "f1": 0.6013071895424837}, "Work_for": {"precision": 0.6142857142857143, "recall": 0.5657894736842105, "f1": 0.5890410958904109}, "Live_in": {"precision": 0.3776223776223776, "recall": 0.5510204081632653, "f1": 0.44813278008298757}, "Kill": {"precision": 0.8367346938775511, "recall": 0.8723404255319149, "f1": 0.8541666666666667}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-00-57-57", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": true, "chat_model": false}]