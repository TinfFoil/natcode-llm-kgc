[{"Model": "./models/Meta-Llama-3.1-8B_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.5342465753424658, "Recall": 0.5749385749385749, "F1_Score": 0.553846153846154, "rel_type_metrics": {"Live_in": {"precision": 0.30714285714285716, "recall": 0.4387755102040816, "f1": 0.3613445378151261}, "Work_for": {"precision": 0.6575342465753424, "recall": 0.631578947368421, "f1": 0.6442953020134228}, "Located_in": {"precision": 0.5652173913043478, "recall": 0.5777777777777777, "f1": 0.5714285714285713}, "Kill": {"precision": 0.7692307692307693, "recall": 0.851063829787234, "f1": 0.8080808080808081}, "Organization_based_in": {"precision": 0.6296296296296297, "recall": 0.53125, "f1": 0.5762711864406779}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-26-22-46-23", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}, {"Model": "./models/Meta-Llama-3.1-8B_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.498812351543943, "Recall": 0.515970515970516, "F1_Score": 0.5072463768115942, "rel_type_metrics": {"Live_in": {"precision": 0.26282051282051283, "recall": 0.41836734693877553, "f1": 0.3228346456692913}, "Kill": {"precision": 0.7843137254901961, "recall": 0.851063829787234, "f1": 0.8163265306122448}, "Organization_based_in": {"precision": 0.71875, "recall": 0.4791666666666667, "f1": 0.575}, "Work_for": {"precision": 0.65, "recall": 0.5131578947368421, "f1": 0.573529411764706}, "Located_in": {"precision": 0.4888888888888889, "recall": 0.4888888888888889, "f1": 0.4888888888888889}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-16-41-58", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}, {"Model": "./models/Meta-Llama-3.1-8B_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.48552338530066813, "Recall": 0.5356265356265356, "F1_Score": 0.5093457943925234, "rel_type_metrics": {"Located_in": {"precision": 0.4897959183673469, "recall": 0.5333333333333333, "f1": 0.5106382978723404}, "Organization_based_in": {"precision": 0.6714285714285714, "recall": 0.4895833333333333, "f1": 0.5662650602409638}, "Live_in": {"precision": 0.4090909090909091, "recall": 0.3673469387755102, "f1": 0.3870967741935484}, "Kill": {"precision": 0.35344827586206895, "recall": 0.8723404255319149, "f1": 0.5030674846625767}, "Work_for": {"precision": 0.5974025974025974, "recall": 0.6052631578947368, "f1": 0.6013071895424836}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-04-31-26", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}]