[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=12", "Precision": 0.7229551451187335, "Recall": 0.6732186732186732, "F1_Score": 0.6972010178117048, "rel_type_metrics": {"Work_for": {"precision": 0.7058823529411765, "recall": 0.631578947368421, "f1": 0.6666666666666667}, "Located_in": {"precision": 0.7922077922077922, "recall": 0.6777777777777778, "f1": 0.7305389221556887}, "Kill": {"precision": 0.7547169811320755, "recall": 0.851063829787234, "f1": 0.8}, "Organization_based_in": {"precision": 0.6666666666666666, "recall": 0.5833333333333334, "f1": 0.6222222222222222}, "Live_in": {"precision": 0.711340206185567, "recall": 0.7040816326530612, "f1": 0.7076923076923078}}, "n_icl_samples": 12, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-32-16", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=12", "Precision": 0.7075718015665796, "Recall": 0.6658476658476659, "F1_Score": 0.6860759493670886, "rel_type_metrics": {"Live_in": {"precision": 0.673469387755102, "recall": 0.673469387755102, "f1": 0.673469387755102}, "Located_in": {"precision": 0.775, "recall": 0.6888888888888889, "f1": 0.7294117647058822}, "Kill": {"precision": 0.6779661016949152, "recall": 0.851063829787234, "f1": 0.7547169811320754}, "Organization_based_in": {"precision": 0.6962025316455697, "recall": 0.5729166666666666, "f1": 0.6285714285714286}, "Work_for": {"precision": 0.7164179104477612, "recall": 0.631578947368421, "f1": 0.6713286713286712}}, "n_icl_samples": 12, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-34-17", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_natlang_base_steps=200_icl=12", "Precision": 0.7150395778364116, "Recall": 0.6658476658476659, "F1_Score": 0.6895674300254454, "rel_type_metrics": {"Work_for": {"precision": 0.6911764705882353, "recall": 0.618421052631579, "f1": 0.6527777777777778}, "Organization_based_in": {"precision": 0.6666666666666666, "recall": 0.5833333333333334, "f1": 0.6222222222222222}, "Live_in": {"precision": 0.6979166666666666, "recall": 0.6836734693877551, "f1": 0.6907216494845362}, "Located_in": {"precision": 0.7922077922077922, "recall": 0.6777777777777778, "f1": 0.7305389221556887}, "Kill": {"precision": 0.7407407407407407, "recall": 0.851063829787234, "f1": 0.7920792079207921}}, "n_icl_samples": 12, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-36-15", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}]