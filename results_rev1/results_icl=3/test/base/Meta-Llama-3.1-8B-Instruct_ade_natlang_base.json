[{"Model": "unsloth/Meta-Llama-3.1-8B-Instruct", "Precision": 0.18757062146892656, "Recall": 0.3755656108597285, "F1_Score": 0.25018839487565936, "rel_type_metrics": {"Adverse_effect": {"precision": 0.4450402144772118, "recall": 0.3755656108597285, "f1": 0.4073619631901841}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-27-01-34-11", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-8B-Instruct", "Precision": 0.16944444444444445, "Recall": 0.27601809954751133, "F1_Score": 0.20998278829604133, "rel_type_metrics": {"Adverse_effect": {"precision": 0.5258620689655172, "recall": 0.27601809954751133, "f1": 0.3620178041543027}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-28-20-39-27", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": true, "chat_model": true}, {"Model": "unsloth/Meta-Llama-3.1-8B-Instruct", "Precision": 0.1474056603773585, "Recall": 0.2828054298642534, "F1_Score": 0.19379844961240314, "rel_type_metrics": {"Adverse_effect": {"precision": 0.5656108597285068, "recall": 0.2828054298642534, "f1": 0.3770739064856712}}, "n_icl_samples": 3, "n_samples_test": 300, "dataset": "ade", "date": "2024-12-29-10-45-09", "schema_path": "./data/codekgc-data/ade/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": true, "chat_model": true}]