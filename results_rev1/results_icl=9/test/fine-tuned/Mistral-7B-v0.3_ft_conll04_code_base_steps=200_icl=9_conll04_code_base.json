[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=9", "Precision": 0.6885644768856448, "Recall": 0.6953316953316954, "F1_Score": 0.6919315403422983, "rel_type_metrics": {"Live_in": {"precision": 0.711340206185567, "recall": 0.7040816326530612, "f1": 0.7076923076923078}, "Work_for": {"precision": 0.7428571428571429, "recall": 0.6842105263157895, "f1": 0.7123287671232877}, "Kill": {"precision": 0.6307692307692307, "recall": 0.8723404255319149, "f1": 0.7321428571428572}, "Located_in": {"precision": 0.6630434782608695, "recall": 0.6777777777777778, "f1": 0.6703296703296704}, "Organization_based_in": {"precision": 0.6896551724137931, "recall": 0.625, "f1": 0.6557377049180328}}, "n_icl_samples": 9, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-48-05", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=9", "Precision": 0.6932668329177057, "Recall": 0.683046683046683, "F1_Score": 0.6881188118811882, "rel_type_metrics": {"Kill": {"precision": 0.84, "recall": 0.8936170212765957, "f1": 0.8659793814432989}, "Located_in": {"precision": 0.6559139784946236, "recall": 0.6777777777777778, "f1": 0.6666666666666666}, "Live_in": {"precision": 0.5964912280701754, "recall": 0.6938775510204082, "f1": 0.6415094339622641}, "Organization_based_in": {"precision": 0.75, "recall": 0.625, "f1": 0.6818181818181818}, "Work_for": {"precision": 0.734375, "recall": 0.618421052631579, "f1": 0.6714285714285715}}, "n_icl_samples": 9, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-51-05", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_base_steps=200_icl=9", "Precision": 0.6724565756823822, "Recall": 0.6658476658476659, "F1_Score": 0.669135802469136, "rel_type_metrics": {"Organization_based_in": {"precision": 0.7236842105263158, "recall": 0.5729166666666666, "f1": 0.6395348837209303}, "Located_in": {"precision": 0.5779816513761468, "recall": 0.7, "f1": 0.6331658291457285}, "Work_for": {"precision": 0.7076923076923077, "recall": 0.6052631578947368, "f1": 0.6524822695035462}, "Kill": {"precision": 0.8541666666666666, "recall": 0.8723404255319149, "f1": 0.8631578947368421}, "Live_in": {"precision": 0.6285714285714286, "recall": 0.673469387755102, "f1": 0.6502463054187192}}, "n_icl_samples": 9, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-22-54-32", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": false, "chat_model": false}]