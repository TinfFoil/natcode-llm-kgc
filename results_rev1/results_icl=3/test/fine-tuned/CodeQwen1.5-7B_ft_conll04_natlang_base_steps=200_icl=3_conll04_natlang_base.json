[{"Model": "./models/CodeQwen1.5-7B_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.549889135254989, "Recall": 0.6093366093366094, "F1_Score": 0.5780885780885782, "rel_type_metrics": {"Work_for": {"precision": 0.5747126436781609, "recall": 0.6578947368421053, "f1": 0.6134969325153374}, "Kill": {"precision": 0.7755102040816326, "recall": 0.8085106382978723, "f1": 0.7916666666666665}, "Located_in": {"precision": 0.5232558139534884, "recall": 0.5, "f1": 0.5113636363636364}, "Live_in": {"precision": 0.6138613861386139, "recall": 0.6326530612244898, "f1": 0.6231155778894472}, "Organization_based_in": {"precision": 0.4140625, "recall": 0.5520833333333334, "f1": 0.4732142857142857}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-20-01-14", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/CodeQwen1.5-7B_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.5662100456621004, "Recall": 0.6093366093366094, "F1_Score": 0.5869822485207101, "rel_type_metrics": {"Located_in": {"precision": 0.49411764705882355, "recall": 0.4666666666666667, "f1": 0.48}, "Live_in": {"precision": 0.47619047619047616, "recall": 0.6122448979591837, "f1": 0.5357142857142858}, "Kill": {"precision": 0.8085106382978723, "recall": 0.8085106382978723, "f1": 0.8085106382978723}, "Organization_based_in": {"precision": 0.5978260869565217, "recall": 0.5729166666666666, "f1": 0.5851063829787233}, "Work_for": {"precision": 0.6022727272727273, "recall": 0.6973684210526315, "f1": 0.6463414634146342}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-09-46-53", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}, {"Model": "./models/CodeQwen1.5-7B_ft_conll04_natlang_base_steps=200_icl=3", "Precision": 0.5570175438596491, "Recall": 0.6240786240786241, "F1_Score": 0.5886442641946698, "rel_type_metrics": {"Live_in": {"precision": 0.5727272727272728, "recall": 0.6428571428571429, "f1": 0.6057692307692308}, "Kill": {"precision": 0.7916666666666666, "recall": 0.8085106382978723, "f1": 0.7999999999999999}, "Organization_based_in": {"precision": 0.5346534653465347, "recall": 0.5625, "f1": 0.5482233502538072}, "Located_in": {"precision": 0.532608695652174, "recall": 0.5444444444444444, "f1": 0.5384615384615384}, "Work_for": {"precision": 0.47619047619047616, "recall": 0.6578947368421053, "f1": 0.5524861878453038}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-09-50-27", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": false, "natlang": true, "chat_model": false}]