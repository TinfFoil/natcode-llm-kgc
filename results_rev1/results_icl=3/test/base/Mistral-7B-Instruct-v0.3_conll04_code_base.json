[{"Model": "mistralai/Mistral-7B-Instruct-v0.3", "Precision": 0.09961685823754789, "Recall": 0.12776412776412777, "F1_Score": 0.11194833153928954, "rel_type_metrics": {"Organization_based_in": {"precision": 0.13636363636363635, "recall": 0.0625, "f1": 0.08571428571428572}, "Kill": {"precision": 0.16981132075471697, "recall": 0.19148936170212766, "f1": 0.18}, "Work_for": {"precision": 0.20588235294117646, "recall": 0.18421052631578946, "f1": 0.19444444444444445}, "Live_in": {"precision": 0.26666666666666666, "recall": 0.04081632653061224, "f1": 0.07079646017699115}, "Located_in": {"precision": 0.10270270270270271, "recall": 0.2111111111111111, "f1": 0.13818181818181818}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-14-37-36", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "mistralai/Mistral-7B-Instruct-v0.3", "Precision": 0.12378640776699029, "Recall": 0.12530712530712532, "F1_Score": 0.12454212454212454, "rel_type_metrics": {"Live_in": {"precision": 0.08333333333333333, "recall": 0.08163265306122448, "f1": 0.08247422680412371}, "Work_for": {"precision": 0.1896551724137931, "recall": 0.14473684210526316, "f1": 0.16417910447761194}, "Kill": {"precision": 0.5357142857142857, "recall": 0.3191489361702128, "f1": 0.4}, "Organization_based_in": {"precision": 0.16666666666666666, "recall": 0.010416666666666666, "f1": 0.0196078431372549}, "Located_in": {"precision": 0.16842105263157894, "recall": 0.17777777777777778, "f1": 0.17297297297297298}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-14-41-19", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}, {"Model": "mistralai/Mistral-7B-Instruct-v0.3", "Precision": 0.13720316622691292, "Recall": 0.12776412776412777, "F1_Score": 0.13231552162849872, "rel_type_metrics": {"Live_in": {"precision": 0.17647058823529413, "recall": 0.030612244897959183, "f1": 0.05217391304347826}, "Kill": {"precision": 0.5555555555555556, "recall": 0.10638297872340426, "f1": 0.17857142857142855}, "Located_in": {"precision": 0.20588235294117646, "recall": 0.15555555555555556, "f1": 0.17721518987341772}, "Organization_based_in": {"precision": 0.19101123595505617, "recall": 0.17708333333333334, "f1": 0.18378378378378377}, "Work_for": {"precision": 0.13829787234042554, "recall": 0.17105263157894737, "f1": 0.15294117647058825}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-14-46-17", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": false, "rationale": false, "natlang": false, "chat_model": true}]