[{"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_rationale_steps=200_icl=6", "Precision": 0.6631853785900783, "Recall": 0.6240786240786241, "F1_Score": 0.6430379746835443, "rel_type_metrics": {"Live_in": {"precision": 0.5909090909090909, "recall": 0.6632653061224489, "f1": 0.6249999999999999}, "Kill": {"precision": 0.875, "recall": 0.8936170212765957, "f1": 0.8842105263157894}, "Work_for": {"precision": 0.6666666666666666, "recall": 0.5263157894736842, "f1": 0.5882352941176471}, "Organization_based_in": {"precision": 0.6022727272727273, "recall": 0.5520833333333334, "f1": 0.5760869565217391}, "Located_in": {"precision": 0.7012987012987013, "recall": 0.6, "f1": 0.6467065868263472}}, "n_icl_samples": 6, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-20-49-23", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_rationale_steps=200_icl=6", "Precision": 0.6571428571428571, "Recall": 0.6216216216216216, "F1_Score": 0.6388888888888888, "rel_type_metrics": {"Kill": {"precision": 0.7, "recall": 0.8936170212765957, "f1": 0.7850467289719626}, "Live_in": {"precision": 0.6632653061224489, "recall": 0.6632653061224489, "f1": 0.6632653061224489}, "Located_in": {"precision": 0.68, "recall": 0.5666666666666667, "f1": 0.6181818181818183}, "Organization_based_in": {"precision": 0.5978260869565217, "recall": 0.5729166666666666, "f1": 0.5851063829787233}, "Work_for": {"precision": 0.6779661016949152, "recall": 0.5263157894736842, "f1": 0.5925925925925927}}, "n_icl_samples": 6, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-20-56-49", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}, {"Model": "./models/Mistral-7B-v0.3_ft_conll04_code_rationale_steps=200_icl=6", "Precision": 0.6632124352331606, "Recall": 0.628992628992629, "F1_Score": 0.6456494325346784, "rel_type_metrics": {"Located_in": {"precision": 0.627906976744186, "recall": 0.6, "f1": 0.6136363636363636}, "Organization_based_in": {"precision": 0.6, "recall": 0.5625, "f1": 0.5806451612903225}, "Kill": {"precision": 0.875, "recall": 0.8936170212765957, "f1": 0.8842105263157894}, "Live_in": {"precision": 0.6565656565656566, "recall": 0.6632653061224489, "f1": 0.6598984771573604}, "Work_for": {"precision": 0.6507936507936508, "recall": 0.5394736842105263, "f1": 0.5899280575539567}}, "n_icl_samples": 6, "n_samples_test": 288, "dataset": "conll04", "date": "2025-01-18-21-04-04", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": false}]