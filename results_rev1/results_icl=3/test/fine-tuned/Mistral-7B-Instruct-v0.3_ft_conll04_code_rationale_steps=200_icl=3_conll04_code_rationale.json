[{"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.5774278215223098, "Recall": 0.5405405405405406, "F1_Score": 0.5583756345177665, "rel_type_metrics": {"Organization_based_in": {"precision": 0.6842105263157895, "recall": 0.40625, "f1": 0.5098039215686275}, "Work_for": {"precision": 0.6615384615384615, "recall": 0.5657894736842105, "f1": 0.6099290780141844}, "Kill": {"precision": 0.41237113402061853, "recall": 0.851063829787234, "f1": 0.5555555555555556}, "Live_in": {"precision": 0.6666666666666666, "recall": 0.5510204081632653, "f1": 0.6033519553072626}, "Located_in": {"precision": 0.55, "recall": 0.4888888888888889, "f1": 0.5176470588235293}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-26-21-35-16", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.5692307692307692, "Recall": 0.5454545454545454, "F1_Score": 0.5570890840652446, "rel_type_metrics": {"Organization_based_in": {"precision": 0.6268656716417911, "recall": 0.4375, "f1": 0.5153374233128833}, "Live_in": {"precision": 0.44696969696969696, "recall": 0.6020408163265306, "f1": 0.5130434782608696}, "Located_in": {"precision": 0.5308641975308642, "recall": 0.4777777777777778, "f1": 0.5029239766081872}, "Kill": {"precision": 0.8666666666666667, "recall": 0.8297872340425532, "f1": 0.8478260869565217}, "Work_for": {"precision": 0.6190476190476191, "recall": 0.5131578947368421, "f1": 0.5611510791366907}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-28-15-33-37", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}, {"Model": "./models/Mistral-7B-Instruct-v0.3_ft_conll04_code_rationale_steps=200_icl=3", "Precision": 0.6264044943820225, "Recall": 0.547911547911548, "F1_Score": 0.5845347313237221, "rel_type_metrics": {"Work_for": {"precision": 0.7222222222222222, "recall": 0.5131578947368421, "f1": 0.6000000000000001}, "Organization_based_in": {"precision": 0.6507936507936508, "recall": 0.4270833333333333, "f1": 0.5157232704402516}, "Kill": {"precision": 0.8478260869565217, "recall": 0.8297872340425532, "f1": 0.8387096774193549}, "Live_in": {"precision": 0.5213675213675214, "recall": 0.6224489795918368, "f1": 0.5674418604651164}, "Located_in": {"precision": 0.5733333333333334, "recall": 0.4777777777777778, "f1": 0.5212121212121212}}, "n_icl_samples": 3, "n_samples_test": 288, "dataset": "conll04", "date": "2024-12-29-03-16-02", "schema_path": "./data/codekgc-data/conll04/code_prompt", "split": "test", "fine-tuned": true, "rationale": true, "natlang": false, "chat_model": true}]